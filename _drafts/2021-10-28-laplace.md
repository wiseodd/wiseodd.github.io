---
layout:     post
title:      "Modern Arts of Laplace Approximations"
subtitle:   "Blah."
date:       2021-10-22 00:00
author:     "wiseodd"
header-img: "img/laplace_signature.svg"
category:   techblog
tags:       [math]
---

Let $f: X \times \Theta \to Y$ defined by $(x, \theta) \mapsto f_\theta(x)$ be a neural network, where $X \subseteq \R^n$, $\Theta \subseteq \R^d$, and $Y \subseteq \R^c$ be the input, parameter, and output spaces, respectively.
Given a dataset $\D := \\{ (x_i, y_i) : x_i \in X, y_i \in Y \\}_{i=1}^m$, we define the likelihood $p(\D \mid \theta) := \prod\_{i=1}^m p(y_i \mid f\_\theta(x_i))$.
Then, given a prior $p(\theta)$, we can obtain the posterior via an application of Bayes' rule: $p(\theta \mid \D) = 1/Z \,\, p(\D \mid \theta) p(\theta)$.
But, exact computation of $p(\theta \mid \D)$ is intractable in general due to the needs of computing the normalization constant

$$
    Z = \int_\Theta p(\D \mid \theta) p(\theta) \,d\theta
$$

We must then approximate $p(\theta \mid \D)$.
One simple way to do this is by simply finding one single likeliest point under the posterior, i.e. the mode of $p(\theta \mid \D)$.
This can be done via optimization, instead of integration:

$$
    \theta_\map := \argmax_\theta \sum_{i=1}^m \log p(y_i \mid f_\theta(x_i)) + \log p(\theta) =: \argmax_\theta \L(\theta; \D) .
$$

The estimate $\theta_\map$ is referred to as the **_maximum a posteriori (MAP) estimate_**.
However, the MAP estimate does not capture the uncertainty around $\theta$.
Thus, it often (and in some cases, e.g. [1], almost always) leads to overconfidence.

In the context of Bayesian neural networks, the Laplace approximation (LA) is a family of methods for obtaining a Gaussian approximate posterior distribution of networks' parameters.
The fact that it produces a Gaussian approximation is a step up from the MAP estimation: particularly, it conveys some notion of uncertainty in $\theta$.
LA stems from the early work of Pierre-Simon Laplace in 1774 [2] and it was first adapted for Bayesian neural networks (BNNs) by David MacKay in 1992 [3].
The method goes as follows.

Given the MAP estimate $\theta_\map$, let us Taylor-expand $\L$ around $\theta_\map$ up to the second order:

$$
    \L(\theta; \D) \approx \L(\theta_\map; \D) + \frac{1}{2} (\theta - \theta_\map)^\top \left(\nabla^2_\theta \L\vert_{\theta_\map}\right) (\theta - \theta_\map) .
$$

(Note that the gradient $\nabla_\theta \L$ is zero at $\theta_\map$ since $\theta_\map$ is a critical point of $\L$ and thus the first order term in the above is also zero.)
Now, recall that $\L$ is the log-numerator of the posterior $p(\theta \mid \D)$.
Thus, the r.h.s. of the above can be used to approximate the true numerator, by simply exponentiating it:

$$
\begin{align*}
    p(\D \mid \theta)p(\theta) &\approx \exp\left( \L(\theta_\map; \D) + \frac{1}{2} (\theta - \theta_\map)^\top \left(\nabla^2_\theta \L\vert_{\theta_\map}\right) (\theta - \theta_\map) \right) \\[5pt]
        %
        &= \exp(\L(\theta_\map; \D)) \exp\left(\frac{1}{2} (\theta - \theta_\map)^\top \left(\nabla^2_\theta \L\vert_{\theta_\map}\right) (\theta - \theta_\map) \right) .
\end{align*}
$$

For simplicity, let $\varSigma := -\left(\nabla^2_\theta \L\vert\_{\theta\_\map}\right)^{-1}$. Then, using this approximation, we can also obtain an approximation of $Z$:

$$
\begin{align*}
    Z &\approx \exp(\L(\theta_\map; \D)) \int_\theta  \exp\left(-\frac{1}{2} (\theta - \theta_\map)^\top \varSigma^{-1} (\theta - \theta_\map) \right) \,d\theta \\[5pt]
        %
        &= \exp(\L(\theta_\map; \D)) (2\pi)^{d/2} (\det \varSigma)^{1/2} ,
\end{align*}
$$

where the equality follows from the fact the integral above is the famous, tractable [Gaussian integral](https://en.wikipedia.org/wiki/Gaussian_integral).
Combining both approximations, we obtain

$$
\begin{align*}
    p(\theta \mid \D) &\approx \frac{1}{(2\pi)^{d/2} (\det \varSigma)^{1/2}} \exp\left(-\frac{1}{2} (\theta - \theta_\map)^\top \varSigma^{-1} (\theta - \theta_\map) \right) \\[5pt]
        %
        &= \N(\theta \mid \theta_\map, \varSigma) .
\end{align*}
$$

That is, we obtain a tractable, easy-to-work-with Gaussian approximation to the intractable posterior via a simple second-order Taylor expansion!
Moreover, this is not just any Gaussian approximation: Notice that this Gaussian is fully determined once we have the MAP estimate $\theta_\map$.
Considering that the MAP estimation is _the_ standard procedure for training NNs, the LA is nothing but a simple post-training step on top of it.
This means the LA, unlike other approximate inference methods, is a _post-hoc_ method that can be applied to virtually any pre-trained NN, without the need of re-training!

Given this approximation, we can then use it as a proxy to the true posterior.
For instance, we can use it to obtain the predictive distribution

$$
\begin{align*}
    p(y \mid x, \D) &\approx \int_\theta p(y \mid f_\theta(x)) \, \N(\theta \mid \theta_\map, \varSigma) \,d\theta \\
        %
        &\approx \frac{1}{s} \sum_{i=1}^s p(y \mid f_\theta(x)) \qquad \text{where} \enspace \theta_s \sim \N(\theta \mid \theta_\map, \varSigma) ,
\end{align*}
$$

which in general is less overconfident compared to the MAP-estimate-induced predictive distribution [3].

What we have seen is the most general framework of the LA.
One can make a specific design decision, such as by imposing a special structure to the Hessian $\nabla^2_\theta \L$, and thus the covariance $\varSigma$.


<h2 class="section-heading">The <span style="font-family: monospace; font-size: 15pt">laplace-torch</span> library</h2>

The simplicity of the LA is not without a drawback.
Recall that the parameter $\theta$ is in $\Theta \subseteq \R^d$.
In neural networks (NNs), $d$ is often in the order of millions or even billions.
Naively computing the Hessian $\nabla^2_\theta \L$ is thus often infeasible since it scales like $O(d^2)$.
Together with the fact that the LA is an old method (and thus not "trendy" in the (Bayesian) deep learning community), this might be the reason on why the LA is not as popular as other BNN posterior approximation methods such as variational Bayes (VB) and Markov Chain Monte Carlo (MCMC).

Motivated by this observation, in our NeurIPS 2021 paper titled ["Laplace Redux -- Effortless Bayesian Deep Learning"](https://arxiv.org/abs/2106.14806), we showcase that (i) the Hessian can be obtained cheaply, thanks to recent advances in second-order optimization, and (ii) even the simplest  LA can be competitive to more sophisticated VB and MCMC methods, while only being much cheaper than them.
Of course, numbers alone are not sufficient to promote the goodness LA.
So, in that paper, we also propose an extendible, easy-to-use software library for PyTorch called <span style="font-family: monospace; font-size: 12pt">laplace-torch</span>, which is available at <https://github.com/AlexImmer/Laplace>.


<h2 class="section-heading">Elements of Modern Laplace Approximations</h2>


![Laplace Flowchart]({{ site.baseurl }}/img/2021-10-28-laplace/flowchart.png){:width="100%"}


<h2 class="section-heading">Hyperparameter Tuning</h2>

Talk about marglik, crossval, LULA.

<h2 class="section-heading">Extensions</h2>

<h2 class="section-heading">Laplace vs. Competitors</h2>

<h2 class="section-heading">References</h2>

1.
