[["Map",1,2,7,8],"meta::meta",["Map",3,4,5,6],"astro-version","5.0.5","config-digest","8fddff858b97db98","post",["Map",9,10,24,25,35,36,49,50,61,62,72,73,84,85,95,96,107,108,118,119,129,130,140,141,151,152,162,163,173,174,186,187,197,198,208,209,220,221,231,232,243,244,254,255,265,266,276,277,287,288,298,299,309,310,320,321,331,332,343,344,355,356,366,367,377,378,388,389,400,401,413,414,424,425,435,436,446,447,457,458,468,469,479,480,490,491,502,503,513,514,524,525,536,537,547,548,558,559,569,570,580,581,592,593,605,606,616,617,628,629,639,640,650,651,661,662,672,673],"annealed-importance-sampling",{"id":9,"data":11,"body":19,"filePath":20,"digest":21,"legacyId":22,"deferredRender":23},{"title":12,"description":13,"publishDate":14,"draft":15,"tags":16},"Introduction to Annealed Importance Sampling","An introduction and implementation of Annealed Importance Sampling (AIS).",["Date","2017-12-23T12:00:00.000Z"],false,[17,18],"machine learning","bayesian","import BlogImage from \"@/components/BlogImage.astro\";\n\nSuppose we have this distribution:\n\n$$\n    p(x) = \\frac{1}{Z} f(x)\n$$\n\nwhere $ Z = \\sum_x f(x) $. In high dimension, this summation is intractable as there would be exponential number of terms. We are hopeless on computing $ Z $ and in turn we can't evaluate this distribution.\n\nNow, how do we compute an expectation w.r.t. to $p(x)$, i.e.:\n\n$$\n    \\mathbb{E}_{p(x)}[x] = \\sum_x x p(x)\n$$\n\nIt is impossible for us to do this as we don't know $ p(x) $. Our best hope is to approximate that. One of the popular way is to use importance sampling. However, importance sampling has a hyperparameter that is hard to adjust, i.e. the proposal distribution $ q(x) $. Importance sampling works well if we can provide $ q(x) $ that is a good approximation of $ p(x) $. It is problematic to find a good $ q(x) $, and this is one of the motivations behind Annealed Importance Sampling (AIS) [1].\n\n## Annealed Importance Sampling\n\nThe construction of AIS is as follows:\n\n1. Let $ p_0(x) = p(x) \\propto f_0(x) $ be our target distribution.\n2. Let $ p_n(x) = q(x) \\propto f_n(x) $ be our proposal distribution which only requirement is that we can sample independent point from it. It doesn't have to be close to $ p_0(x) $ thus the requirement is more relaxed than importance sampling.\n3. Define a sequence of intermediate distributions starting from $ p*n(x) $ to $ p_0(x) $ call it $ p_j(x) \\propto f_j(x) $. The requirement is that $ p_j(x) \\neq 0 $ whenever $p*{j-1}(x) \\neq 0$. That is, $ p*j(x) $ has to cover the support of $ p*{j-1}(x) $ so that we can take the ratio.\n4. Define local transition probabilities $ T_j(x, x') $.\n\nThen to sample from $ p_0(x) $, we need to:\n\n- Sample an independent point from $ x\\_{n-1} \\sim p_n(x) $.\n- Sample $ x*{n-2} $ from $ x*{n-1} $ by doing MCMC w.r.t. $ T\\_{n-1} $.\n- $ \\dots $\n- Sample $ x_1 $ from $ x_2 $ by doing MCMC w.r.t. $ T_2 $.\n- Sample $ x_0 $ from $ x_1 $ by doing MCMC w.r.t. $ T_1 $.\n\nIntuitively given two distributions, which might be disjoint in their support, we create intermediate distributions that are \"bridging\" from one to another. Then we do MCMC to move around these distributions and hope that we end up in our target distribution.\n\nAt this point, we have sequence of points $ x*{n-1}, x*{n-2}, \\dots, x_1, x_0 $. We can use them to compute the importance weight as follows:\n\n$$\n    w = \\frac{f*{n-1}(x*{n-1})}{f*n(x*{n-1})} \\frac{f*{n-2}(x*{n-2})}{f*{n-1}(x*{n-2})} \\dots \\frac{f_1(x_1)}{f_2(x_1)} \\frac{f_0(x_0)}{f_1(x_0)}\n$$\n\nNotice that $ w $ is telescoping, and without the intermediate distributions, it reduces to the usual weight used in importance sampling.\n\nWith this importance weight, then we can compute the expectation as in importance sampling:\n\n$$\n    \\mathbb{E}_{p(x)}[x] = \\frac{1}{\\sum_i^N w_i} \\sum_i^N x_i w_i\n$$\n\nwhere $ N $ is the number of samples.\n\n## Practicalities\n\nWe now have the full algorithm. However several things are missing, namely, the choice of $ f_j(x) $ and $ T_j(x, x') $.\n\nFor the intermediate distributions, we can set it as an annealing between to our target and proposal functions, i.e:\n\n$$\n    f_j(x) = f_0(x)^{\\beta_j} f_n(x)^{1-\\beta_j}\n$$\n\nwhere $ 1 = \\beta_0 > \\beta_1 > \\dots > \\beta_n = 0 $. For visual example, annealing between $ N(0, I) $ to $ N(5, I) $ with 10 intermediate distributions gives us:\n\n\u003CBlogImage\n  imagePath='/img/annealed-importance-sampling/intermediate_dists.png'\n  altText='Intermediate distributions.'\n/>\n\nFor the transition functions, we can use Metropolis-Hastings with acceptance probability:\n\n$$\n    A_j(x, x') = \\frac{f_j(x')}{f_j(x)}\n$$\n\nassuming we have symmetric proposal, e.g. $ N(0, I) $.\n\n## Implementation\n\nTo make it more concrete, we can look at the simple implementation of AIS. We first define our target function:\n\n```python\nimport numpy as np\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\n\ndef f_0(x):\n    \"\"\"\n    Target distribution: \\propto N(-5, 2)\n    \"\"\"\n    return np.exp(-(x+5)**2/2/2)\n```\n\nNext we define our proposal function and distribution, as we assume we can easily sample independent points from it:\n\n```python\ndef f_j(x, beta):\n    \"\"\"\n    Intermediate distribution: interpolation between f_0 and f_n\n    \"\"\"\n    return f_0(x)**beta * f_n(x)**(1-beta)\n\n# Proposal distribution: 1/Z * f_n\np_n = st.norm(0, 1)\n```\n\nLastly, we define our transition function:\n\n```python\ndef T(x, f, n_steps=10):\n    \"\"\"\n    Transition distribution: T(x'|x) using n-steps Metropolis sampler\n    \"\"\"\n    for t in range(n_steps): # Proposal\n        x_prime = x + np.random.randn()\n\n        # Acceptance prob\n        a = f(x_prime) / f(x)\n\n        if np.random.rand() \u003C a:\n            x = x_prime\n\n    return x\n```\n\nThen, we are ready to do the sampling:\n\n```python\nx = np.arange(-10, 5, 0.1)\n\nn_inter = 50 # num of intermediate dists\nbetas = np.linspace(0, 1, n_inter)\n\n# Sampling\nn_samples = 100\nsamples = np.zeros(n_samples)\nweights = np.zeros(n_samples)\n\nfor t in range(n_samples): # Sample initial point from q(x)\n    x = p_n.rvs()\n    w = 1\n\n    for n in range(1, len(betas)):\n        # Transition\n        x = T(x, lambda x: f_j(x, betas[n]), n_steps=5)\n\n        # Compute weight in log space (log-sum):\n        # w *= f_{n-1}(x_{n-1}) / f_n(x_{n-1})\n        w += np.log(f_j(x, betas[n])) - np.log(f_j(x, betas[n-1]))\n\n    samples[t] = x\n    weights[t] = np.exp(w)  # Transform back using exp\n```\n\nNotice, in the code above we do log-sum-exp trick to avoid underflow when computing $ w $.\n\nAfter the iteration finished, we have with ourselves our samples and their corresponding weights, from which we can compute the expectation as in importance sampling:\n\n```python\n# Compute expectation\na = 1/np.sum(weights) - np.sum(weights - samples)\n```\n\nIn this example, the result should be very close to the mean of our target Gaussian i.e. $ -5 $.\n\n## Discussion\n\nAIS is a very interesting and useful way to do importance sampling without having to sweat about the choice of proposal $ q(x) $. However, No Free Lunch theorem also applies to AIS: we still need to tune the hyperparameters such as the number of intermediate distributions and the number of MCMC step at each transition. This could potentially very expensive. Moreover, as in other sampling algorithms, it is inherently sequential and can't exploit fully the availability of GPU.\n\nNevertheless, AIS is powerful and has been used even nowadays. It is a popular choice of algorithms for approximating partition function in RBM [2]. Moreover it has been used for Deep Generative Models (GAN, VAE) [3].\n\n## References\n\n1. Neal, Radford M. \"Annealed importance sampling.\" Statistics and computing 11.2 (2001): 125-139.\n2. Salakhutdinov, Ruslan. \"Learning and evaluating Boltzmann machines.\" Tech. Rep., Technical Report UTML TR 2008-002, Department of Computer Science, University of Toronto (2008).\n   APA\n3. Wu, Yuhuai, et al. \"On the quantitative analysis of decoder-based generative models.\" arXiv preprint arXiv:1611.04273 (2016).","src/content/post/annealed-importance-sampling.mdx","2e862b4155bebedc","annealed-importance-sampling.mdx",true,"bayesian-regression",{"id":24,"data":26,"body":31,"filePath":32,"digest":33,"legacyId":34,"deferredRender":23},{"title":27,"description":28,"publishDate":29,"draft":15,"tags":30},"Linear Regression: A Bayesian Point of View","You know the drill, apply mean squared error, then descend those gradients. But, what is the intuition of that process in Bayesian PoV?",["Date","2017-01-05T07:41:00.000Z"],[17,18],"We all know the first model we learned when learning Machine Learning: Linear Regression. It is a simple, intuitive, and stimulating our mind to go deeper into Machine Learning hole.\n\nLinear Regression could be intuitively interpreted in several point of views, e.g. geometry and statistics (frequentist one!). Having frequentist statistics point of view, usually there should be the Bayesian counterpart. Hence, in this post, we would address the Bayesian point of view of Linear Regression.\n\n## Linear Regression: Refreshments\n\nRecall, in Linear Regression, we want to map our inputs into real numbers, i.e. $f: \\mathbb{R}^N \\to \\mathbb{R}$. For example, given some features, e.g. how many hour of studying, number of subject taken, and the IQ of a student, we want to predict his or her GPA.\n\nThere are several types of Linear Regression, depending on their cost function and the regularizer. In this post, we would focus on Linear Regression with $\\ell_2$ cost and $\\ell_2$ regularization. In statistics, this kind of regression is called Ridge Regression.\n\nFormally, the objective is as follows:\n\n$$\nL = \\frac{1}{2} \\Vert \\hat{y} - y \\Vert^2_2 + \\frac{\\lambda}{2} \\Vert W \\Vert^2_2\n$$\n\nwhere $\\hat{y}$ is the ground truth value, and $y$ is given by:\n\n$$\ny = W^Tx\n$$\n\nwhich is a linear combination of feature vector and weight matrix. The additional $\\frac{1}{2}$ in both terms is just for mathematical convenience when taking the derivative.\n\nThe idea is then to minimize this objective function with regard to $W$. That is, we want to find weight matrix $W$ that minimize the squared error.\n\nOf course we could ignore the regularization term. What we end up with then, is a vanilla Linear Regression:\n\n$$\nL = \\frac{1}{2} \\Vert \\hat{y} - y \\Vert^2_2\n$$\n\nMinimization this objective is the definition of Linear Least Square problem.\n\n## Frequentist view of Linear Regression\n\nWe could write the regression target of the above model as the predicted value plus some error:\n\n$$\n\\begin{align}\n\n\\hat{y} &= y + \\epsilon \\\\[10pt]\n        &= W^Tx + \\epsilon\n\n\\end{align}\n$$\n\nor equivalently, we could say that the error is:\n\n$$\n\\epsilon = \\hat{y} - y\n$$\n\nNow, let's say we model the regression target as a Gaussian random variable, i.e. $y \\sim N(\\mu, \\sigma^2)$, with $\\mu = y = W^Tx$, the prediction of our model. Formally:\n\n$$\nP(\\hat{y} \\vert x, W) = N(\\hat{y} \\vert W^Tx, \\sigma^2)\n$$\n\nThen, to find the optimum $W$, we could use Maximum Likelihood Estimation (MLE). As the above model is a likelihood, i.e. describing our data $y$ under parameter $W$, we will do MLE on that:\n\n$$\nW*{MLE} = \\mathop{\\rm arg\\,max}\\limits*{W} N(\\hat{y} \\vert W^Tx, \\sigma^2)\n$$\n\nThe PDF of Gaussian is given by:\n\n$$\nP(\\hat{y} \\vert x, W) = \\frac{1}{\\sqrt{2 \\sigma^2 \\pi}} \\, \\exp \\left( -\\frac{(\\hat{y} - W^Tx)^2}{2 \\sigma^2} \\right)\n$$\n\nAs we are doing maximization, we could ignore the normalizing constant of the likelihood. Hence:\n\n$$\nW*{MLE} = \\mathop{\\rm arg\\,max}\\limits*{W} \\, \\exp \\left( -\\frac{(\\hat{y} - W^Tx)^2}{2 \\sigma^2} \\right)\n$$\n\nAs always, it is easier to optimize the log likelihood:\n\n$$\n\\begin{align}\n\nW_{MLE} &= \\mathop{\\rm arg\\,max}\\limits_{W} \\, \\log \\left( \\exp \\left( -\\frac{(\\hat{y} - W^Tx)^2}{2 \\sigma^2} \\right) \\right) \\\\[10pt]\n        &= \\mathop{\\rm arg\\,max}\\limits_{W} -\\frac{1}{2 \\sigma^2}(\\hat{y} - W^Tx)^2 \\\\[10pt]\n        &= \\mathop{\\rm arg\\,min}\\limits_{W} \\frac{1}{2 \\sigma^2}(\\hat{y} - W^Tx)^2\n\\end{align}\n$$\n\nFor simplicity, let's say $\\sigma^2 = 1$, then:\n\n$$\n\\begin{align}\n\nW_{MLE} &= \\mathop{\\rm arg\\,min}\\limits_{W} \\frac{1}{2} (\\hat{y} - W^Tx)^2 \\\\[10pt]\n        &= \\mathop{\\rm arg\\,min}\\limits_{W} \\frac{1}{2} \\sum_i (\\hat{y}_i - W_i x_i)^2 \\\\[10pt]\n        &= \\mathop{\\rm arg\\,min}\\limits_{W} \\frac{1}{2} \\Vert \\hat{y} - W^Tx \\Vert^2_2\n\n\\end{align}\n$$\n\nSo we see, doing MLE on Gaussian likelihood is equal to Linear Regression!\n\n## Bayesian view of Linear Regression\n\nBut what if we want to go Bayesian, i.e. introduce a prior, and working with the posterior instead? Well, then we are doing MAP estimation! The posterior is likelihood times prior:\n\n$$\nP(W \\vert \\hat{y}, x) = P(\\hat{y} \\vert x, W) P(W \\vert \\mu_0, \\sigma^2_0)\n$$\n\nSince we have already known the likelihood, now we ask, what should be the prior? If we set it to be uniformly distributed, then we will be back to the MLE estimation. So, for non-trivial example, let's use Gaussian prior for weight $W$:\n\n$$\nP(W \\vert \\mu_0, \\sigma^2_0) = N(0, \\sigma^2_0)\n$$\n\nExpanding the PDF, and again ignoring the normalizing constant and keeping in mind that $\\mu_0 = 0$, we have:\n\n$$\n\\begin{align}\n\nP(W \\vert \\mu_0, \\sigma^2_0) &= \\frac{1}{\\sqrt{2 \\sigma^2_0 \\pi}} \\, \\exp \\left( -\\frac{(W - \\mu_0)^2}{2 \\sigma^2_0} \\right) \\\\[10pt]\n                             &\\propto \\exp \\left( -\\frac{W^2}{2 \\sigma^2_0} \\right)\n\n\\end{align}\n$$\n\nLet's derive the posterior:\n\n$$\n\\begin{align}\n\nP(W \\vert \\hat{y}, x) &= P(\\hat{y} \\vert x, W) P(W \\vert \\mu_0, \\sigma^2_0) \\\\[10pt]\n                      &\\propto \\exp \\left( -\\frac{(\\hat{y} - W^Tx)^2}{2 \\sigma^2} \\right) \\, \\exp \\left( -\\frac{W^2}{2 \\sigma^2_0} \\right)\n\n\\end{align}\n$$\n\nAnd the log posterior is then:\n\n$$\n\\begin{align}\n\n\\log P(W \\vert \\hat{y}, x) &\\propto -\\frac{1}{2 \\sigma^2}(\\hat{y} - W^Tx)^2 - \\frac{1}{2 \\sigma^2_0} W^2 \\\\[10pt]\n                           &= -\\frac{1}{2 \\sigma^2} \\Vert \\hat{y} - W^Tx\\Vert^2_2 - \\frac{1}{2 \\sigma^2_0} \\Vert W \\Vert^2_2\n\n\\end{align}\n$$\n\nSeems familiar, right! Now if we assume that $\\sigma^2 = 1$ and $\\lambda = \\frac{1}{\\sigma^2_0}$, then our log posterior becomes:\n\n$$\n\\log P(W \\vert \\hat{y}, x) \\propto -\\frac{1}{2} \\Vert \\hat{y} - W^Tx\\Vert^2_2 - \\frac{\\lambda}{2} \\Vert W \\Vert^2_2\n$$\n\nThat is, the log posterior of Gaussian likelihood and Gaussian prior is the same as the objective function for Ridge Regression! Hence, Gaussian prior is equal to $\\ell_2$ regularization!\n\n## Full Bayesian Approach\n\nOf course, above is not a full Bayesian, as we are doing a point estimation in the form of MAP. This is just a \"shortcut\", as we do not need to compute the full posterior distribution. For full Bayesian approach, we report the full posterior distribution. And in test time, we use the posterior to weight the new data, i.e. we marginalize the posterior predictive distribution:\n\n$$\n\\begin{align}\n\nP(y' \\vert \\hat{y}, x) &= \\int_W P(y' \\vert x', W) P(W \\vert \\hat{y}, x) \\\\[10pt]\n                       &= \\mathbb{E}_W \\left[ P(y' \\vert x', W) \\right]\n\n\\end{align}\n$$\n\nthat is, given the likelihood of our new data point $(x', y')$, we compute the likelihood, and weigh it with the posterior.\n\nIntuitively, given all possible value for $W$ in the posterior, we try those values one by one to predict the new data. The result is then averaged proportionality to the probability of those values, hence we are taking expectation.\n\nAnd of course, that is the reason why we use a shortcut in the form of MAP. For illustration, if each component of $W$ is binary, i.e. have two possible values, and there are $K$ components in $W$, we are talking about $2^K$ possible assignments for $W$, which is exponential! In real world, each component of $W$ is a real number, which makes the problem of enumerating all possible values of $W$ intractable!\n\nOf course we could use approximate method like Variational Bayes or MCMC, but they are still more costly than MAP. As MAP and MLE is guaranteed to find one of the modes (local maxima), it is good enough.\n\n## Conclusion\n\nIn this post we saw Linear Regression with several different point of view.\n\nFirst, we looked at the definition of Linear Regression in plain Machine Learning PoV, then frequentist statistics, and finally Bayesian statistics.\n\nWe noted that the Bayesian version of the Linear Regression using MAP estimation is not a full Bayesian approach, since MAP is just a shortcut.\n\nWe then noted why full Bayesian approach is difficult and often intractable, even on this simple regression model.\n\n## References\n\n1. Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.","src/content/post/bayesian-regression.mdx","8a9b11170afce02a","bayesian-regression.mdx","batchnorm",{"id":35,"data":37,"body":45,"filePath":46,"digest":47,"legacyId":48,"deferredRender":23},{"title":38,"description":39,"publishDate":40,"draft":15,"tags":41},"Implementing BatchNorm in Neural Net","BatchNorm is a relatively new technique for training neural net. It gaves us a lot of relaxation when initializing the network and accelerates training.",["Date","2016-07-04T14:53:00.000Z"],[17,42,43,44],"programming","python","neural networks","import BlogImage from \"@/components/BlogImage.astro\";\n\nBatchNorm a.k.a Batch Normalization is a relatively new technique proposed by Ioffe & Szegedy in 2015. It promises us acceleration in training (deep) neural net.\n\nOne difficult thing about training a neural net is to choose the initial weights. BatchNorm promises the remedy: it makes the network less dependant to the initialization strategy. Another key points are that it enables us to use higher learning rate. They even go further to state that BatchNorm could reduce the dependency on Dropout.\n\n## BatchNorm: the algorithm\n\nThe main idea of BatchNorm is this: for the current minibatch while training, in each hidden layer, we normalize the activations so that its distribution is Standard Normal (zero mean and one standard deviation). Then, we apply a linear transform to it with learned parameters so that the network could learn what kind of distribution is the best for the layer's activations.\n\nThis in turn will make the gradient flows better as activations saturation is not a problem anymore. Recall that activations saturation kills neurons in case of ReLU and makes gradient vanish or explode in case of sigmoid.\n\nIt enables us to be less careful with weights initialization as we don't need to worry anymore if our weights initialization makes the activations saturated too quickly which in turn make the gradient explode. BatchNorm takes care of that.\n\nThe forward propagation of BatchNorm is shown below:\n\n\u003CBlogImage imagePath='/img/batchnorm/00.png' />\n\nPretty simple right? We just need to compute the activations mean and variance over the current minibatch and normalize the activations with that. What could go wrong.\n\nWell, we're training neural net with Backpropagation here, so that algorithm is half the story. We still need to derive the backprop scheme for the BatchNorm layer. Which is given by this:\n\n\u003CBlogImage imagePath='/img/batchnorm/01.png' />\n\nIf the above derivation doesn't make any sense, you could try reading [this](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html) in combination with computational graph approach of backprop in [CS231 lecture](https://www.youtube.com/playlist?list=PLLvH2FwAQhnpj1WEB-jHmPuUeQ8mX-XXG).\n\nNow that we know how to do forward and backward propagation for BatchNorm, let's try to implement that.\n\n## Training with BatchNorm\n\nAs always, I will reuse the code from previous posts. It's in my repo here: https://github.com/wiseodd/hipsternet.\n\n```python\ndef batchnorm_forward(X, gamma, beta):\n    mu = np.mean(X, axis=0)\n    var = np.var(X, axis=0)\n\n    X_norm = (X - mu) / np.sqrt(var + 1e-8)\n    out = gamma * X_norm + beta\n\n    cache = (X, X_norm, mu, var, gamma, beta)\n\n    return out, cache, mu, var\n```\n\nThis is the forward propagation algorithm. It's simple. However, remember that we're normalizing each dimension of activations. So, if our activations over a minibatch is MxN matrix, then we want the mean and variance to be 1xN: one value of mean and variance for each dimension. So, if we normalize our activations matrix with that, each dimension will have zero mean and one variance.\n\nAt the end, we're also spitting out the intermediate variable used for normalization as they're essential for the backprop.\n\nThis is how we use that above method:\n\n```python\n# Input to hidden\nh1 = X @ W1 + b1\n\n# BatchNorm\nh1, bn1_cache, mu, var = batchnorm_forward(h1, gamma1, beta1)\n\n# ReLU\nh1[h1 \u003C 0] = 0\n```\n\nIn the BatchNorm paper, they insert the BatchNorm layer before nonlinearity. But it's not set in a stone.\n\nFor the backprop, here's the implementation:\n\n```python\ndef batchnorm_backward(dout, cache):\n    X, X_norm, mu, var, gamma, beta = cache\n\n    N, D = X.shape\n\n    X_mu = X - mu\n    std_inv = 1. / np.sqrt(var + 1e-8)\n\n    dX_norm = dout * gamma\n    dvar = np.sum(dX_norm * X_mu, axis=0) * -.5 * std_inv**3\n    dmu = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n\n    dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmu / N)\n    dgamma = np.sum(dout * X_norm, axis=0)\n    dbeta = np.sum(dout, axis=0)\n\n    return dX, dgamma, dbeta\n```\n\nFor the explanation of the code, refer to the derivation of the BatchNorm gradient in the last section. As we can see, we're also returning derivative of gamma and beta: the linear transform for BatchNorm. It will be used to update the model, so that the net could also learn them.\n\n```python\n# h1\ndh1 = dh2 @ W2.T\n\n# ReLU\ndh1[h1 \u003C= 0] = 0\n\n# Dropout h1\ndh1 *= u1\n\n# BatchNorm\ndh1, dgamma1, dbeta1 = batchnorm_backward(dh2, bn2_cache)\n```\n\nRemember, the order of backprop is important! We will get wrong result if we swap the BatchNorm gradient with ReLU gradient for example.\n\n## Inference with BatchNorm\n\nOne more thing we need to take care of is that we want to fix the normalization at test time. That means we don't want to normalize our activations with the test set. Hence, as we're essentially using SGD, which is stochastic, we're going to estimate the mean and variance of our activations using running average.\n\n```python\n# BatchNorm training forward propagation\nh2, bn2*cache, mu, var = batchnorm_forward(h2, gamma2, beta2)\nbn_params['bn2_mean'] = .9 * bn*params['bn2_mean'] + .1 * mu\nbn_params['bn2_var'] = .9 * bn*params['bn2_var'] + .1 * var\n```\n\nThere, we store each BatchNorm layer's running mean and variance while training. It's a decaying running average.\n\nThen, at the test time, we just use that running average for the normalization:\n\n```python\n# BatchNorm inference forward propagation\nh2 = (h2 - bn_params['bn2_mean']) / np.sqrt(bn_params['bn2_var'] + 1e-8)\nh2 = gamma2 \\* h2 + beta2\n```\n\nAnd that's it. Our implementation of BatchNorm is now complete. Now the test!\n\n## Test and Comparison\n\nWe're going to use a three layer network with 256 neurons in each hidden layer and minibatch size of 256. We use 1000 iterations of Adam for the optimization. We're also using Dropout with probability of 0.5.\n\nFirst, let's test the claim of BatchNorm makes the network less dependant on weights initialization. We're going to just use this initialization scheme: `W1=np.random.randn(D, H)`.\n\n```\n# learning rate = 1e-2\n\n# With BatchNorm:\nIter-100 loss: 14.755845933600622\nIter-200 loss: 6.411304732436082\nIter-300 loss: 3.8945102118430466\nIter-400 loss: 2.4634817943096805\nIter-500 loss: 1.660556228305082\nIter-600 loss: 1.279642232187109\nIter-700 loss: 1.0457504142354406\nIter-800 loss: 0.9400718897268598\nIter-900 loss: 1.0161870928408856\nIter-1000 loss: 0.833607292631844\n\nadam => mean accuracy: 0.8853, std: 0.0079\n\n# Without BatchNorm:\nIter-100 loss: 49.73374055679555\nIter-200 loss: 46.61991288615765\nIter-300 loss: 44.1475586165201\nIter-400 loss: 42.57901406076491\nIter-500 loss: 41.30118729832132\nIter-600 loss: 38.58016074343413\nIter-700 loss: 36.495641945628186\nIter-800 loss: 34.454519834002745\nIter-900 loss: 32.40071379522495\nIter-1000 loss: 30.35279106760543\n\nadam => mean accuracy: 0.1388, std: 0.0219\n```\n\nIt works wonder! The \"unproper\" initialization seems to make the non-BatchNorm network's gradients vanish, so it can't learn well. With BatchNorm, the optimization just zip through and converging in much faster speed.\n\nLet's do one more test. This time we're using the proper initialization: Xavier / 2. How is BatchNorm compared to Dropout? In the paper, the authors made a point that by using BatchNorm, the network could be less dependant to Dropout.\n\n```\n# With only BatchNorm\nrmsprop => mean accuracy: 0.9740, std: 0.0016\n\n# With only Dropout\nrmsprop => mean accuracy: 0.9692, std: 0.0006\n\n# With nothing\nrmsprop => mean accuracy: 0.9640, std: 0.0071\n```\n\nThe network with BatchNorm performs much better!\n\nHowever, not all are rosy. Remember the no free lunch theorem. BatchNorm has a drawback: it makes our training slower. Here's the comparison:\n\n```\n# With BatchNorm:\npython run.py  81.12s user 7.34s system 184% cpu 47.958 total\n\n# Without BatchNorm:\npython run.py  63.94s user 4.42s system 186% cpu 36.616 total\n```\n\nWith BatchNorm, we're expecting 30% slow down.\n\n## Conclusion\n\nIn this post, we're looking at the relatively new technique for training neural nets, called BatchNorm. It does just that: normalizing the activations of the network at each minibatch, so that the activations will be approximately Standard Normal distributed.\n\nWe also implemented BatchNorm in three layers network, then tested it using various parameters. It's by no mean rigorous, but we could catch a glimpse of BatchNorm in action.\n\nIn the test, we found that by using BatchNorm, our network become more tolerant to bad initialization. BatchNorm network also outperform Dropout in our test setting. However, those are with the expense of 30% increase of training time.\n\n## References\n\n- http://arxiv.org/pdf/1502.03167v3.pdf\n- https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html","src/content/post/batchnorm.mdx","a3396355c668d552","batchnorm.mdx","boundary-seeking-gan",{"id":49,"data":51,"body":57,"filePath":58,"digest":59,"legacyId":60,"deferredRender":23},{"title":52,"description":53,"publishDate":54,"draft":15,"tags":55},"Boundary Seeking GAN","Training GAN by moving the generated samples to the decision boundary.",["Date","2017-03-07T05:10:00.000Z"],[17,56],"gan","import BlogImage from \"@/components/BlogImage.astro\";\n\nBoundary Seeking GAN (BGAN) is a recently introduced modification of GAN training. Here, in this post, we will look at the intuition behind BGAN, and also the implementation, which consists of one line change from vanilla GAN.\n\n## Intuition of Boundary Seeking GAN\n\nRecall, in GAN the following objective is optimized:\n\n$$\n    \\min_G \\max_D V(D, G) = \\E_{x \\sim p_\\text{data}}[\\log D(x)] + \\E_{z \\sim p_z(x)} [\\log (1 - D(G(z)))]\n$$\n\nFollowing the objective above, as shown in the original GAN paper [1], the optimal discriminator $D^*_G(x)$ is given by:\n\n$$\nD^*_G(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}\n$$\n\nHence, if we know the optimal discriminator with respect to our generator, $D^*_G(x)$, we are good to go, as we have this following amount by rearranging the above equation:\n\n$$\np_{data}(x) = p_g(x) \\frac{D^*_G(x)}{1 - D^*_G(x)}\n$$\n\nWhat does it tell us is that, even if we have non-optimal generator $G$, we could still find the true data distribution by weighting $p_g(x)$, the generator's distribution, with the ratio of optimal discriminator for that generator.\n\nUnfortunately, perfect discriminator is hard to get. But we can work with its approximation $D(x)$ instead. The assumption is that if we train $D(x)$ more and more, it becomes closer and closer to $D^*_G(x)$, and our GAN training becomes better and better.\n\nIf we think further at the above equation, we would get $p_{data}(x) = p_g(x)$, i.e. our generator is optimal, if the ratio of the discriminator is equal to one. If that ratio is equal to one, then consequently $D(x)$ must be equal to $0.5$. Therefore, the optimal generator is the one that can make make the discriminator to be $0.5$ everywhere. Notice that $D(x) = 0.5$ is the decision boundary. Hence, we want to generate $x \\sim G(z)$ such that $D(x)$ is near the decision boundary. Therefore, the authors of the paper named this method _Boundary Seeking GAN_ (BGAN).\n\nThat statement has a very intuitive explanation. If we consider the generator to be perfect, $D(x)$ can't distinguish the real and the fake data. In other words, real and fake data are equally likely, as far as $D(x)$ concerned. As $D(x)$ has two outputs (real or fake), then, those outputs has the probability of $0.5$ each.\n\nNow, we could modify the generator's objective in order to make the discriminator outputting $0.5$ for every data we generated. One way to do it is to minimize the distance between $D(x)$ and $1 - D(x)$ for all $x$. If we do so, as $D(x)$ is a probability measure, we will get the minimum at $D(x) = 1 - D(x) = 0.5$, which is what we want.\n\nTherefore, the new objective for the generator is:\n\n$$\n\\min_{G} \\, \\mathbb{E}_{z \\sim p_z(z)} \\left[ \\frac{1}{2} (\\log D(x) - \\log(1 - D(x)))^2 \\right]\n$$\n\nwhich is just an $L_2$ loss. We added $\\log$ as $D(x)$ is a probability measure, and we want to undo that, as we are talking about distance, not divergence.\n\n## Implementation\n\nThis should be the shortest ever implementation note in my blog.\n\nWe just need to change the original GAN's $G$ objective from:\n\n```python\nG_loss = -torch.mean(log(D_fake))\n```\n\nto:\n\n```python\nG_loss = 0.5 * torch.mean((log(D_fake) - log(1 - D_fake))**2)\n```\n\nAnd we're done. For full code, check out https://github.com/wiseodd/generative-modes.\n\n## Conclusion\n\nIn this post we looked at a new GAN variation called Boundary Seeking GAN (BGAN). We looked at the intuition of BGAN, and tried to understand why it's called \"boundary seeking\".\n\nWe also implemented BGAN in Pytorch with just one line of code change.\n\n## References\n\n1. Hjelm, R. Devon, et al. \"Boundary-Seeking Generative Adversarial Networks.\" arXiv preprint arXiv:1702.08431 (2017). [arxiv](https://arxiv.org/abs/1702.08431)\n2. Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in Neural Information Processing Systems. 2014. [arxiv](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)","src/content/post/boundary-seeking-gan.mdx","7b3be847a55628ad","boundary-seeking-gan.mdx","conditional-gan-tensorflow",{"id":61,"data":63,"body":68,"filePath":69,"digest":70,"legacyId":71,"deferredRender":23},{"title":64,"description":65,"publishDate":66,"draft":15,"tags":67},"Conditional Generative Adversarial Nets in TensorFlow","Having seen GAN, VAE, and CVAE model, it is only proper to study the Conditional GAN model next!",["Date","2016-12-24T10:30:00.000Z"],[17,42,43,44,56],"import BlogImage from \"@/components/BlogImage.astro\";\n\nWe have seen the Generative Adversarial Nets (GAN) model in the previous post. We have also seen the arch nemesis of GAN, the VAE and its conditional variation: Conditional VAE (CVAE). Hence, it is only proper for us to study conditional variation of GAN, called Conditional GAN or CGAN for short.\n\n## CGAN: Formulation and Architecture\n\nRecall, in GAN, we have two neural nets: the generator $G(z)$ and the discriminator $D(X)$. Now, as we want to condition those networks with some vector $y$, the easiest way to do it is to feed $y$ into both networks. Hence, our generator and discriminator are now $G(z, y)$ and $D(X, y)$ respectively.\n\nWe can see it with a probabilistic point of view. $G(z, y)$ is modeling the distribution of our data, given $z$ and $y$, that is, our data is generated with this scheme $X \\sim G(X \\, \\vert \\, z, y)$.\n\nLikewise for the discriminator, now it tries to find discriminating label for $X$ and $X_G$, that are modeled with $d \\sim D(d \\, \\vert \\, X, y)$.\n\nHence, we could see that both $D$ and $G$ is jointly conditioned to two variables $z$ or $X$ and $y$.\n\nNow, the objective function is given by:\n\n$$\n\\min_G \\max_D V(D, G) = \\mathop{\\mathbb{E}}_{x \\sim p_{data}(x)} [\\log D(x, y)] + \\mathop{\\mathbb{E}}_{z \\sim p_z(z)} [\\log(1 - D(G(z, y), y))]\n$$\n\nIf we compare the above loss to GAN loss, the difference only lies in the additional parameter $y$ in both $D$ and $G$.\n\nThe architecture of CGAN is now as follows (taken from [1]):\n\n\u003CBlogImage\n  imagePath='/img/conditional-gan-tensorflow/arch.png'\n  altText='CGAN architecture.'\n/>\n\nIn contrast with the architecture of GAN, we now has an additional input layer in both discriminator net and generator net.\n\n## CGAN: Implementation in TensorFlow\n\nImplementing CGAN is so simple that we just need to add a handful of lines to the original GAN implementation. So, here we will only look at those modifications.\n\nThe first additional code for CGAN is here:\n\n```python\ny = tf.placeholder(tf.float32, shape=[None, y_dim])\n```\n\nWe are adding new input to hold our variable we are conditioning our CGAN to.\n\nNext, we add it to both our generator net and discriminator net:\n\n```python\ndef generator(z, y):\n    # Concatenate z and y\n    inputs = tf.concat(concat_dim=1, values=[z, y])\n\n    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n    G_prob = tf.nn.sigmoid(G_log_prob)\n\n    return G_prob\n\n\ndef discriminator(x, y):\n    # Concatenate x and y\n    inputs = tf.concat(concat_dim=1, values=[x, y])\n\n    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n    D_prob = tf.nn.sigmoid(D_logit)\n\n    return D_prob, D_logit\n```\n\nThe problem we have here is how to incorporate the new variable $y$ into $D(X)$ and $G(z)$. As we are trying to model the joint conditional, the simplest way to do it is to just concatenate both variables. Hence, in $G(z, y)$, we are concatenating $z$ and $y$ before we feed it into the networks. The same procedure is applied to $D(X, y)$.\n\nOf course, as our inputs for $D(X, y)$ and $G(z, y)$ is now different than the original GAN, we need to modify our weights:\n\n```python\n# Modify input to hidden weights for discriminator\nD_W1 = tf.Variable(shape=[X_dim + y_dim, h_dim]))\n\n# Modify input to hidden weights for generator\nG_W1 = tf.Variable(shape=[Z_dim + y_dim, h_dim]))\n```\n\nThat is, we just adjust the dimensionality of our weights.\n\nNext, we just use our new networks:\n\n```python\n# Add additional parameter y into all networks\nG_sample = generator(Z, y)\nD_real, D_logit_real = discriminator(X, y)\nD_fake, D_logit_fake = discriminator(G_sample, y)\n```\n\nAnd finally, when training, we also feed the value of $y$ into the networks:\n\n```python\nX_mb, y_mb = mnist.train.next_batch(mb_size)\n\nZ_sample = sample_Z(mb_size, Z_dim)\n_, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: Z_sample, y:y_mb})\n_, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: Z_sample, y:y_mb})\n```\n\nAs an example above, we are training our GAN with MNIST data, and the conditional variable $y$ is the labels.\n\n## CGAN: Results\n\nAt test time, we want to generate new data samples with certain label. For example, we set the label to be 5, i.e. we want to generate digit \"5\":\n\n```python\nn_sample = 16\nZ_sample = sample_Z(n_sample, Z_dim)\n\n# Create conditional one-hot vector, with index 5 = 1\ny_sample = np.zeros(shape=[n_sample, y_dim])\ny_sample[:, 5] = 1\n\nsamples = sess.run(G_sample, feed_dict={Z: Z_sample, y:y_sample})\n```\n\nAbove, we just sample $z$, and then construct the conditional variables. In our example case, the conditional variables is a collection of one-hot vectors with value 1 in the 5th index. The last thing we need to is to run the network with those variables as inputs.\n\nHere is the results:\n\n\u003CBlogImage\n  imagePath='/img/conditional-gan-tensorflow/5.png'\n  altText='Conditional samples.'\n/>\n\nLooks pretty much like digit 5, right?\n\nIf we set our one-hot vectors to have value of 1 in the 7th index:\n\n\u003CBlogImage\n  imagePath='/img/conditional-gan-tensorflow/7.png'\n  altText='Conditional samples.'\n/>\n\nThose results confirmed that have successfully trained our CGAN.\n\n## Conclusion\n\nIn this post, we looked at the analogue of CVAE for GAN: the Conditional GAN (CGAN). We show that to make GAN into CGAN, we just need a little modifications to our GAN implementation.\n\nThe conditional variables for CGAN, just like CVAE, could be anything. Hence it makes CGAN an interesting model to work with for data modeling.\n\nThe full code is available at my GitHub repo: https://github.com/wiseodd/generative-models.\n\n## References\n\n1. Mirza, Mehdi, and Simon Osindero. \"Conditional generative adversarial nets.\" arXiv preprint arXiv:1411.1784 (2014).","src/content/post/conditional-gan-tensorflow.mdx","0347d04284dd5d59","conditional-gan-tensorflow.mdx","brouwers-fixed-point",{"id":72,"data":74,"body":80,"filePath":81,"digest":82,"legacyId":83,"deferredRender":23},{"title":75,"description":76,"publishDate":77,"draft":15,"tags":78},"Brouwer's Fixed Point Theorem: A Proof with Reduced Homology","A proof of special case (ball) of Brouwer's Fixed Point Theorem with Reduced Homology.",["Date","2018-07-18T14:00:00.000Z"],[79],"math","import BlogImage from \"@/components/BlogImage.astro\";\n\nThis post is about the proof I found very interesting during the Topology course I took this semester. It highlights the application of Reduced Homology, which is a modification of Homology theory in Algebraic Topology. We will use two results from Reduced Homology as black-boxes for the proof. Everywhere, we will assume $ \\mathbb{Q} $ is used as the coefficient of the Homology space.\n\n**Lemma 1 (Reduced Homology of spheres)**\nGiven a $ d $-sphere $ \\mathbb{S}^d $, then its reduced $ p $-th Homology space is:\n\n$$\n\\tilde{H}_p(\\mathbb{S}^d) = \\begin{cases} \\mathbb{Q}, & \\text{if } p = d \\\\ 0, & \\text{otherwise} \\enspace . \\end{cases}\n$$\n\n$$\n\\qed\n$$\n\n**Lemma 2 (Reduced Homology of balls)**\nGiven a $ d $-ball $ \\mathbb{B}^d $, then its reduced $ p $-th Homology space is trivial, i.e. $\\tilde{H}\\_p(\\mathbb{B}^d) = 0 $, for any $ d $ and $ p $.\n\n$$\n\\qed\n$$\n\nEquipped with these lemmas, we are ready to prove the special case of Brouwer's Fixed Point Theorem, where we consider map from a ball to itself.\n\n**Brouwer's Fixed Point Theorem**\nGiven $ f: \\mathbb{B}^{d+1} \\to \\mathbb{B}^{d+1} $ continuous, then there exists $ x\n\\in \\mathbb{B}^{d+1} $ such that $ f(x) = x $.\n\n_Proof._ &nbsp;&nbsp; For contradiction, assume $ \\forall x \\in \\mathbb{B}^{d+1}: f(x) \\neq x $. We construct a map $ r: \\mathbb{B}^{d+1} \\to \\mathbb{S}^d $, casting ray from the ball to its shell by extending the line segment between $ x $ and $ f(x) $.\n\n\u003CBlogImage\n  imagePath='/img/brouwers-fixed-point/map_r.svg'\n  altText=\"Casting a ray to the ball's shell.\"\n/>\n\nObserve that $ r(x) $ is continuous because $ f(x) $ is. Also, $ x \\in \\mathbb{S}^d \\implies r(x) = x $. Therefore we have the following commutative diagram.\n\n\u003CBlogImage\n  imagePath='/img/brouwers-fixed-point/comm_diag.svg'\n  altText='Commutative diagram.'\n/>\n\nAbove, $ i $ is inclusion map, and $ id $ is identity map. We then look of the Reduced Homology of the above, and this gives us the following commutative diagram.\n\n\u003CBlogImage\n  imagePath='/img/brouwers-fixed-point/comm_diag_hom.svg'\n  altText='Reduced homology.'\n/>\n\nAs the diagram commute, then $ \\tilde{H}\\_d(\\mathbb{S}^d) \\xrightarrow{i^_} \\tilde{H}\\_d(\\mathbb{B}^{d+1}) \\xrightarrow{r^_} \\tilde{H}\\_d(\\mathbb{S}^d) $ should be identity map on $ \\tilde{H}\\_d(\\mathbb{S}^d) $. By Lemma 2, $ \\tilde{H}\\_d(\\mathbb{B}^{d+1}) = 0 $. This implies $ \\tilde{H}\\_d(\\mathbb{S}^d) = 0 $. But this is a contradiction, as By Lemma 1, $ \\tilde{H}\\_d(\\mathbb{S}^d) = \\mathbb{Q} $. Therefore there must be a fixed point.\n\n$$\n\\qed\n$$\n\n## References\n\n1. Hatcher, Allen. \"Algebraic topology.\" (2001).","src/content/post/brouwers-fixed-point.mdx","c38c29d1a43ab6de","brouwers-fixed-point.mdx","chentsov-theorem",{"id":84,"data":86,"body":91,"filePath":92,"digest":93,"legacyId":94,"deferredRender":23},{"title":87,"description":88,"publishDate":89,"draft":15,"tags":90},"Chentsov's Theorem","The Fisher information is often the default choice of the Riemannian metric for manifolds of probability distributions. In this post, we study Chentsov's theorem, which justifies this choice. It says that the Fisher information is the unique Riemannian metric (up to a scaling constant) that is invariant under sufficient statistics. This fact makes the Fisher metric stands out from other choices.",["Date","2021-07-20T04:00:00.000Z"],[79],"Let $p_\\theta(x)$ be a probability density on $\\R^n$, parametrized by $\\theta \\in \\R^d$. The **_Fisher information_** is defined by\n\n$$\n    \\I_{ij}(\\theta) := \\E_{p_\\theta(x)} \\left( \\partial_i \\log p_\\theta(x) \\, \\partial_j \\log p_\\theta(x) \\right)\n$$\n\nwhere $\\partial_i := \\partial/\\partial \\theta^i$ for each $i = 1, \\dots, d$. Note that $\\I(\\theta)$ is positive semi-definite because one can see it as the (expected) outer-product of the gradient of the log-density.\n\n## The Fisher Information under Sufficient Statistics\n\nLet $T : \\R^n \\to \\R^n$ with $x \\mapsto y$ be a bijective transformation of the r.v. $x \\sim p_\\theta(x)$. By the [Fisher-Neyman factorization](https://en.wikipedia.org/wiki/Sufficient_statistic#Fisher%E2%80%93Neyman_factorization_theorem), $T$ is a **_sufficient statistic_** for the parameter $\\theta$ if there exist non-negative functions $g_\\theta$ and $h$, where $g_\\theta$ depends on $\\theta$ while $h$ does not, such that we can write the density $p_\\theta(x)$ as follows:\n\n$$\n    p_\\theta(x) = g_\\theta(T(x)) h(x) .\n$$\n\nThe following proposition shows the behavior of $\\I$ under sufficient statistics.\n\n**Proposition 1.** _The Fisher information is invariant under sufficient statistics._\n\n_Proof._ Let $T$ be a sufficient statistic and so $p_\\theta(x) := g_\\theta(T(x)) h(x)$. Notice that this implies\n\n$$\n    \\partial_i \\log g_\\theta(T(x)) = \\partial_i \\log p_\\theta(x) .\n$$\n\nSo, the Fisher information $\\I(\\theta; T)$ under $T$ is\n\n$$\n\\begin{align}\n    \\I(\\theta; T) &= \\E \\left( \\partial_i \\log (g_\\theta(T(x)) h(x)) \\, \\partial_j \\log (g_\\theta(T(x)) h(x)) \\right) \\\\\n        %\n        &= \\E \\left( \\partial_i \\log g_\\theta(T(x)) \\, \\partial_j \\log g_\\theta(T(x)) \\right) \\\\\n        %\n        &= \\E \\left( \\partial_i \\log p_\\theta(x) \\, \\partial_j \\log p_\\theta(x) \\right) \\\\\n        %\n        &= \\I(\\theta) .\n\\end{align}\n$$\n\nWe conclude that $\\I$ is invariant under sufficient statistics.\n\n$$\n\\qed\n$$\n\n## The Fisher Information as a Riemannian Metric\n\nLet\n\n$$\n    M := \\{ p_\\theta(x) : \\theta \\in \\R^d \\}\n$$\n\nbe the set of the parametric densities $p_\\theta(x)$. We can treat $M$ as a smooth $d$-manifold by imposing a global coordinate chart $p_\\theta(x) \\mapsto \\theta$. Thus, we can identify a point $p_\\theta(x)$ on $M$ with its parameter $\\theta$ interchangeably.\n\nLet us assume that $\\I$ is positive-definite everywhere, and each $\\I_{ij}$ is smooth. Then we can use it as (the coordinates representation of) a Riemannian metric for $M$. This is because $\\I$ is a covariant 2-tensor. (Recall the definition of a Riemannian metric.)\n\n**Proposition 2.** _The component functions $\\I_{ij}$ of $\\I$ follows the covariant transformation rule.\\_\n\n_Proof._ Let $\\theta \\mapsto \\varphi$ be a change of coordinates and let $\\ell(\\varphi) := \\log p_\\varphi(x)$. The component function $\\I_{ij}(\\theta)$ in the \"old\" coordinates is expressed in terms of the \"new\" ones, as follows:\n\n$$\n\\begin{align}\n    \\I_{ij}(\\theta) &= \\E \\left( \\frac{\\partial \\ell}{\\partial \\theta^i} \\, \\frac{\\partial \\ell}{\\partial \\theta^j} \\right) \\\\\n        %\n        &= \\E \\left( \\frac{\\partial \\ell}{\\partial \\varphi^i} \\frac{\\partial \\varphi^i}{\\partial \\theta^i} \\, \\frac{\\partial \\ell}{\\partial \\varphi^j} \\frac{\\partial \\varphi^j}{\\partial \\theta^j} \\right) \\\\\n        %\n        &= \\frac{\\partial \\varphi^i}{\\partial \\theta^i} \\frac{\\partial \\varphi^j}{\\partial \\theta^j} \\E \\left( \\frac{\\partial \\ell}{\\partial \\varphi^i} \\, \\frac{\\partial \\ell}{\\partial \\varphi^j} \\right) \\\\\n        %\n        &= \\frac{\\partial \\varphi^i}{\\partial \\theta^i} \\frac{\\partial \\varphi^j}{\\partial \\theta^j} \\I_{ij}(\\varphi) ,\n\\end{align}\n$$\n\nwhere the second equality follows from the standard chain rule. We conclude that $\\I$ is covariant since the Jacobian $\\partial \\varphi/\\partial \\theta$ of the transformation multiplies the \"new\" component functions $\\I_{ij}(\\varphi)$ of $\\I$ to obtain the \"old\" ones.\n\n$$\n\\qed\n$$\n\n## Chentsov's Theorem\n\nThe previous two results are useful since the Fisher information metric is invariant under sufficient statistics. In this sense, $\\I$ has a statistical invariance property. But this is not a strong enough reason for arguing that $\\I$ is a \"natural\" or \"the best\" metric for $M$.\n\nHere, we shall see a stronger statement, due to Chentsov in 1972, about the Fisher metric: It is the _unique_ statistically-invariant metric for $M$ (up to a scaling constant). This makes $\\I$ stands out over any other metric for $M$.\n\nOriginally, Chentsov's theorem is described on the space of Categorical probability distributions over the sample space $\\Omega := \\\\{ 1, \\dots, n \\\\}$, i.e. the probability simplex. We use the result of Campbell (1986) as a stepping stone. To do so, we need to define the so-called _Markov embeddings_.\n\nLet $\\\\{ A_1, \\dots, A_m \\\\}$ be a partition $\\Omega$, where $2 \\leq n \\leq m$. We define a conditional probability table $Q$ of size $n \\times m$ where\n\n$$\n\\begin{align}\n    q_{ij} &= 0 \\quad \\text{if } j \\not\\in A_i \\\\\n    q_{ij} &> 0 \\quad \\text{if } j \\in A_i \\\\\n    & {\\textstyle\\sum_{j=1}^m} q_{ij} = 1 .\n\\end{align}\n$$\n\nThat is, the $i$-th row of $Q$ gives probabilities signifying the membership of each $j \\in \\Omega$ in $A_i$. Based on this, we define a map $f: \\R^n_{> 0} \\to \\R^m_{>0}$ by\n\n$$\n    y_j := \\sum_{i=1}^n q_{ij} x^i \\qquad \\forall\\enspace j = 1, \\dots, m .\n$$\n\nWe call this map a **_Markov embedding_**. The name suggests that $f$ embeds $\\R^n_{> 0}$ in a higher-dimensional space $\\R^m_{> 0}$.\n\nThe result of Campbell (1986) characterizes the form of the Riemannian metric in $\\R^n_{>0}$ that is invariant under any Markov embedding.\n\n**Lemma 3 (Campbell, 1986).** _Let $g$ be a Riemannian metric on $\\R^n_{>0}$ where $n \\geq 2$. Suppose that every Markov embedding on $(\\R^n_{>0}, g)$ is an isometry. Then\\_\n\n$$\n    g_{ij}(x) = A(\\abs{x}) + \\delta_{ij} \\frac{\\abs{x} B(\\abs{x})}{x^i} ,\n$$\n\n_where $\\abs{x} = \\sum_{i=1}^n x^i$, $\\delta_{ij}$ is the Kronecker delta, and $A, B \\in C^\\infty(\\R_{>0})$ satisfying $B > 0$ and $A + B > 0$.\\_\n\n_Proof._ See Campbell (1986) and Amari (2016, Sec. 3.5).\n\n$$\n\\qed\n$$\n\nLemma 3 is a general statement about the invariant metric in $\\R^n_{>0}$ and it does not say anything about sufficient statistics and probability distributions. To get the main result, we restrict ourselves to the $(n-1)$-**_probability simplex_** $\\Delta^{n-1} \\subset \\R^n_{>0}$, which is the space of (Categorical) probability distribution.\n\nThe fact that the Fisher information is the unique invariant metric under sufficient statistics follows from the fact that when $n = m$, the Markov embedding reduces to a permutation of the components of $x \\in \\R^n_{>0}$---i.e. the permutation of $\\Omega$. This is because permutations of $\\Omega$ are sufficient statistics for Categorical distribution.\n\nLet us, therefore, connect the result in Lemma 3 with the Fisher information on $\\Delta^{n-1}$. We give the latter in the following lemma.\n\n**Lemma 4.** _The Fisher information of a Categorical distribution $p_\\theta(z)$ where $z$ takes values in $\\Omega = \\\\{ 1, \\dots, n \\\\}$ and $\\theta = \\\\{ \\theta^1, \\dots, \\theta^n \\\\} \\in \\Delta^{n-1}$ is given by\\_\n\n$$\n    \\I_{ij}(\\theta) = \\delta_{ij} \\frac{1}{\\theta^i} .\n$$\n\n_That is, $\\I(\\theta)$ is an $(n \\times n)$ diagonal matrix with $i$-th entry $1/\\theta^i$._\n\n_Proof._ By definition,\n\n$$\n    p_\\theta(z) = \\prod_{i=1}^n \\left(\\theta^i\\right)^{(z^i)} ,\n$$\n\nwhere we assume that $z$ is one-hot encoded. Its score function is given by\n\n$$\n\\partial_i \\log p_\\theta(x) = \\partial_i \\sum_{i=1}^n z^i \\log \\theta^i = \\sum_{i=1}^n z^i \\frac{1}{\\theta^i} \\delta_{ij} = \\frac{z^i}{\\theta^i} ,\n$$\n\nfor each $i = 1, \\dots n$. Hence, using the fact that $z$ is one-hot:\n\n$$\n\\begin{align}\n    \\I_{ii}(\\theta) &= \\E \\left( \\frac{z^i}{\\theta^i} \\, \\frac{z^i}{\\theta^i} \\right) \\\\\n        %\n        &= \\frac{1}{(\\theta^i)^2} \\sum_{i=1}^n (z^i)^2 \\theta^i \\\\\n        %\n        &= \\frac{1}{(\\theta^i)^2} \\theta^i \\\\\n        %\n        &= \\frac{1}{\\theta^i} .\n\\end{align}\n$$\n\nUsing similar step, we can show that $\\I_{ij}(\\theta) = 0$ for $i \\neq j$ because $z^i z^j$ is always zero.\n\n$$\n\\qed\n$$\n\nNow we are ready to state the main result.\n\n**Theorem 5 (Chentsov, 1972).** _The Fisher information is the unique Riemannian metric on $\\Delta^{n-1}$ that is invariant under sufficient statistics, up to a multiplicative constant._\n\n_Proof._ By Lemma 3, the invariant metric under Markov embeddings in $\\R^n_{> 0}$ is given by\n\n$$\n    g_{ij}(x) = A(\\abs{x}) + \\delta_{ij} \\frac{\\abs{x} B(\\abs{x})}{x^i} ,\n$$\n\nfor any $x \\in \\R^n_{> 0}$. Therefore, this is the form of the invariant metric under sufficient statistics in $\\Delta^{n-1} \\subset \\R^n_{>0}$, i.e. when $n=m$ in the Markov embedding.\n\nLet us therefore restrict $g$ to $\\Delta^{n-1}$. For each $\\theta \\in \\Delta^{n-1}$, the tangent space $T_\\theta \\Delta^{n-1}$ is orthogonal to the line $x^1 = x^2 = \\dots = x^n$, which direction is given by the vector $\\mathbf{1} = (1, \\dots, 1) \\in \\R^n_{>0}$. This is a vector normal to $\\Delta^{n-1}$, implying that any $v \\in T_\\theta \\Delta^{n-1}$ satisfies $\\inner{\\mathbf{1}, v}_g = 0$, i.e. $\\sum_{i=1}^n v^i = 0$.\n\nMoreover, if $\\theta \\in \\Delta^{n-1}$, then $\\abs{\\theta} = \\sum_{i=1}^n \\theta^i = 1$ by definition. Thus, $A(1)$ and $B(1)$ are constants. So, if $v, w \\in T_\\theta \\Delta^{n-1}$, we have:\n\n$$\n\\begin{align}\n    \\inner{v, w}_{\\theta} &= \\sum_{i=1}^n \\sum_{j=1}^n g_{ij} v^i w^j = A(1) \\sum_{i = 1}^n \\sum_{j = 1}^n v^i w^j + B(1) \\sum_{i=1}^n \\frac{v^i w^i}{\\theta^i} \\\\\n        %\n        &= A(1) \\underbrace{\\left(\\sum_{i = 1}^n v^i\\right)}_{=0} \\underbrace{\\left(\\sum_{j = 1}^n w^j\\right)}_{=0} + B(1) \\sum_{i=1}^n \\frac{v^i w^i}{\\theta^i} .\n\\end{align}\n$$\n\nTherefore $A(1)$ does not contribute to the inner product and we may, w.l.o.g., write the metric as a diagonal matrix:\n\n$$\n    g_{ij}(\\theta) = \\delta_{ij} \\frac{B(1)}{\\theta^i} .\n$$\n\nRecalling that $B(1)$ is a constant, by Lemma 4, we have $g_{ij}(\\theta) \\propto \\I_{ij}(\\theta)$.\n\n$$\n\\qed\n$$\n\nGeneralizations to this (original) version Chentsov's theorem exists. For instance, Ay et al. (2015) showed Chentsov's theorem for arbitrary, parametric probability distributions. Dowty (2018) stated Chentsov's theorem for exponential family distributions.\n\n## References\n\n1. Chentsov, N. N. \"Statistical Decision Rules and Optimal Deductions.\" (1972).\n2. Campbell, L. Lorne. \"An extended Čencov characterization of the information metric.\" Proceedings of the American Mathematical Society 98, no. 1 (1986): 135-141.\n3. Amari, Shun-ichi. Information geometry and its applications. Vol. 194. Springer, 2016.\n4. Ay, Nihat, Jürgen Jost, Hông Vân Lê, and Lorenz Schwachhöfer. \"Information geometry and sufficient statistics.\" Probability Theory and Related Fields 162, no. 1-2 (2015): 327-364.\n5. Dowty, James G. \"Chentsov’s theorem for exponential families.\" Information Geometry 1, no. 1 (2018): 117-135.","src/content/post/chentsov-theorem.mdx","53b92156c3413609","chentsov-theorem.mdx","conditional-vae",{"id":95,"data":97,"body":103,"filePath":104,"digest":105,"legacyId":106,"deferredRender":23},{"title":98,"description":99,"publishDate":100,"draft":15,"tags":101},"Conditional Variational Autoencoder: Intuition and Implementation","An extension to Variational Autoencoder (VAE), Conditional Variational Autoencoder (CVAE) enables us to learn a conditional distribution of our data, which makes VAE more expressive and applicable to many interesting things.",["Date","2016-12-17T16:04:00.000Z"],[42,43,102],"neuralnet","import BlogImage from \"@/components/BlogImage.astro\";\n\nConditional Variational Autoencoder (CVAE) is an extension of Variational Autoencoder (VAE), a generative model that we have studied in the last post. We've seen that by formulating the problem of data generation as a bayesian model, we could optimize its variational lower bound to learn the model.\n\nHowever, we have no control on the data generation process on VAE. This could be problematic if we want to generate some specific data. As an example, suppose we want to convert a unicode character to handwriting. In vanilla VAE, there is no way to generate the handwriting based on the character that the user inputted. Concretely, suppose the user inputted character '2', how do we generate handwriting image that is a character '2'? We couldn't.\n\nHence, CVAE [1] was developed. Whereas VAE essentially models latent variables and data directly, CVAE models lantent variables and data, both conditioned to some random variables.\n\n## Conditional Variational Autoencoder\n\nRecall, on VAE, the objective is:\n\n$$ \\log P(X) - D*{KL}[Q(z \\vert X) \\Vert P(z \\vert X)] = E[\\log P(X \\vert z)] - D*{KL}[Q(z \\vert X) \\Vert P(z)] $$\n\nthat is, we want to optimize the log likelihood of our data $P(X)$ under some \"encoding\" error. The original VAE model has two parts: the encoder $Q(z \\vert X)$ and the decoder $P(X \\vert z)$.\n\nLooking closely at the model, we could see why can't VAE generate specific data, as per our example above. It's because the encoder models the latent variable $z$ directly based on $X$, it doesn't care about the different type of $X$. For example, it doesn't take any account on the label of $X$.\n\nSimilarly, in the decoder part, it only models $X$ directly based on the latent variable $z$.\n\nWe could improve VAE by conditioning the encoder and decoder to another thing(s). Let's say that other thing is $c$, so the encoder is now conditioned to two variables $X$ and $c$: $Q(z \\vert X, c)$. The same with the decoder, it's now conditioned to two variables $z$ and $c$: $P(X \\vert z, c)$.\n\nHence, our variational lower bound objective is now in this following form:\n\n$$ \\log P(X \\vert c) - D*{KL}[Q(z \\vert X, c) \\Vert P(z \\vert X, c)] = E[\\log P(X \\vert z, c)] - D*{KL}[Q(z \\vert X, c) \\Vert P(z \\vert c)] $$\n\ni.e. we just conditioned all of the distributions with a variable $c$.\n\nNow, the real latent variable is distributed under $P(z \\vert c )$. That is, it's now a conditional probability distribution (CPD). Think about it like this: for each possible value of $c$, we would have a $P(z)$. We could also use this form of thinking for the decoder.\n\n## CVAE: Implementation\n\nThe conditional variable $c$ could be anything. We could assume it comes from a categorical distribution expressing the label of our data, gaussian expressing some regression target, or even the same distribution as the data (e.g. for image inpainting: conditioning the model to incomplete image).\n\nLet's use MNIST for example. We could use the label as our conditional variable $c$. In this case, $c$ is categorically distributed, or in other words, it takes form as an one-hot vector of label:\n\n```python\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\nX_train, y_train = mnist.train.images, mnist.train.labels\nX_test, y_test = mnist.test.images, mnist.test.labels\n\nm = 50\nn_x = X_train.shape[1]\nn_y = y_train.shape[1]\nn_z = 2\nn_epoch = 20\n\n# Q(z|X,y) -- encoder\nX = Input(batch_shape=(m, n_x))\ncond = Input(batch_shape=(m, n_y))\n```\n\nThe natural question to arise is how do we incorporate the new conditional variable into our existing neural net? Well, let's do the simplest thing: concatenation.\n\n```python\ninputs = merge([X, cond], mode='concat', concat_axis=1)\n\nh_q = Dense(512, activation='relu')(inputs)\nmu = Dense(n_z, activation='linear')(h_q)\nlog_sigma = Dense(n_z, activation='linear')(h_q)\n```\n\nSimilarly, the decoder is also concatenated with the conditional vector:\n\n```python\ndef sample_z(args):\n    mu, log_sigma = args\n    eps = K.random_normal(shape=(m, n_z), mean=0., std=1.)\n    return mu + K.exp(log_sigma / 2) * eps\n\n# Sample z ~ Q(z|X,y)\nz = Lambda(sample_z)([mu, log_sigma])\nz_cond = merge([z, cond], mode='concat', concat_axis=1) # \u003C--- NEW!\n\n# P(X|z,y) -- decoder\ndecoder_hidden = Dense(512, activation='relu')\ndecoder_out = Dense(784, activation='sigmoid')\n\nh_p = decoder_hidden(z_cond)\noutputs = decoder_out(h_p)\n```\n\nThe rest is similar to VAE. Heck, even we don't need to modify the objective. Everything is already expressed in our neural net models.\n\n```python\ndef vae_loss(y_true, y_pred):\n    \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n    # E[log P(X|z,y)]\n    recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)\n    # D_KL(Q(z|X,y) || P(z|X)); calculate in closed form as both dist. are Gaussian\n    kl = 0.5 * K.sum(K.exp(log_sigma) + K.square(mu) - 1. - log_sigma, axis=1)\n\n    return recon + kl\n```\n\nFor the full explanation of the code, please refer to my original VAE post. The full code could be found in my Github repo: https://github.com/wiseodd/generative-models.\n\n## Conditional MNIST\n\nWe will test our CVAE model to generate MNIST data, conditioned to its label. With the above model, we could specify which digit we want to generate, as it is conditioned to the label!\n\nFirst thing first, let's visualize $Q(z \\vert X, c)$:\n\n\u003CBlogImage imagePath='/img/conditional-vae/z_dist_cvae.png' />\n\nThings are messy here, in contrast to VAE's $Q(z \\vert X)$, which nicely clusters $z$. But if we look at it closely, we could see that given a specific value of $c = y$, $Q(z \\vert X, c=y)$ is roughly $N(0, 1)$! It's because, if we look at our objective above, we are now modeling $P(z \\vert c)$, which we infer variationally with a $N(0, 1)$.\n\nNext, let's try to reconstruct some images:\n\n\u003CBlogImage imagePath='/img/conditional-vae/reconstruction_cvae.png' fullWidth />\n\nSubjectively, we could say the reconstruction results are way better than the original VAE! We could argue that because each data under specific label has its own distribution, hence it is easy to sample data with a specific label. If we look back at the result of the original VAE, the reconstructions suffer at the edge cases, e.g. when the model is not sure if it's 3, 8, or 5, as they look very similar. No such problem here!\n\n\u003CBlogImage imagePath='/img/conditional-vae/generation_cvae.png' fullWidth />\n\nNow the interesting part. We could generate a new data under our specific condition. Above, for example, we generate new data which has the label of '5', i.e. $c = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]$. CVAE make it possible for us to do that.\n\n## Conclusion\n\nIn this post, we looked at the extension of VAE, the Conditional VAE (CVAE).\n\nIn CVAE, we could generate data with specific attribute, an operation that can't be done with the vanilla VAE. We showed this by applying CVAE on MNIST data and conditioned the model to the images' labels. The resulting model allows us to sample data under specific label.\n\nWe also noticed that by conditioning our MNIST data to their labels, the reconstruction results is much better than the vanilla VAE's. Hence, it is a good thing, to incorporate labels to VAE, if available.\n\nFinally, CVAE could be conditioned to anything we want, which could result on many interesting applications, e.g. image inpainting.\n\n## References\n\n1. Sohn, Kihyuk, Honglak Lee, and Xinchen Yan. “Learning Structured Output Representation using Deep Conditional Generative Models.” Advances in Neural Information Processing Systems. 2015.","src/content/post/conditional-vae.mdx","9877fba6b8bf1e27","conditional-vae.mdx","contractive-autoencoder",{"id":107,"data":109,"body":114,"filePath":115,"digest":116,"legacyId":117,"deferredRender":23},{"title":110,"description":111,"publishDate":112,"draft":15,"tags":113},"Deriving Contractive Autoencoder and Implementing it in Keras","Contractive Autoencoder is more sophisticated kind of Autoencoder compared to the last post. Here, we will dissect the loss function of Contractive Autoencoder and derive it so that we could implement it in Keras.",["Date","2016-12-05T17:55:00.000Z"],[42,43,102],"In the last post, we have seen many different flavors of a family of methods called Autoencoders. However, there is one more autoencoding method on top of them, dubbed Contractive Autoencoder (Rifai et al., 2011).\n\nThe idea of Contractive Autoencoder is to make the learned representation to be robust towards small changes around the training examples. It achieves that by using different penalty term imposed to the representation.\n\nThe loss function for the reconstruction term is similar to previous Autoencoders that we have been seen, i.e. using $\\ell_2$ loss. The penalty term, however is more complicated: we need to calculate the representation's jacobian matrix with regards of the training data.\n\nHence, the loss function is as follows:\n\n$$\n    L = \\lVert X - \\hat{X} \\rVert_2^2 + \\lambda \\lVert J_h(X) \\rVert_F^2\n$$\n\nin which\n\n$$\n    \\lVert J_h(X) \\rVert_F^2 = \\sum_{ij} \\left( \\frac{\\partial h_j(X)}{\\partial X_i} \\right)^2\n$$\n\nthat is, the penalty term is the Frobenius norm of the jacobian matrix, which is the sum squared over all elements inside the matrix. We could think Frobenius norm as the generalization of euclidean norm.\n\nIn the loss above, clearly it's the calculation of the jacobian that's not straightforward. Calculating a jacobian of the hidden layer with respect to input is similar to gradient calculation. Recall than jacobian is the generalization of gradient, i.e. when a function is a vector valued function, the partial derivative is a matrix called jacobian.\n\nLet's calculate the jacobian of the hidden layer of our autoencoder then. Let's say:\n\n$$\n\\begin{align}\nZ_j &= W_i X_i \\\\[10pt]\nh_j &= \\phi(Z_j)\n\\end{align}\n$$\n\nwhere $\\phi$ is sigmoid nonlinearity. That is, to get the $j\\text{-th}$ hidden unit, we need to get the dot product of the $i\\text{-th}$ feature and the corresponding weight. Then using chain rule:\n\n$$\n\\begin{align}\n\\frac{\\partial h_j}{\\partial X_i} &= \\frac{\\partial \\phi(Z_j)}{\\partial X_i} \\\\[10pt]\n                                 &= \\frac{\\partial \\phi(W_i X_i)}{\\partial W_i X_i} \\frac{\\partial W_i X_i}{\\partial X_i} \\\\[10pt]\n                                 &= [\\phi(W_i X_i)(1 - \\phi(W_i X_i))] \\, W_{i} \\\\[10pt]\n                                 &= [h_j(1 - h_j)] \\, W_i\n\\end{align}\n$$\n\nIt looks familiar, doesn't it? Because it's exactly how we calculate gradient. The difference is however, that we treat $h(X)$ as a vector valued function. That is, we treat $h\\_{i}(X)$ each as a separate output. Intuitively, let's say for example we have 64 hidden units, then we have 64 function outputs, and so we will have a gradient vector for each of those 64 hidden unit. Hence, when we get the derivative of that hidden layer, what we get instead is a jacobian matrix. And as we now know how to calculate the jacobian, we can calculate the penalty term in our loss.\n\nLet $diag(x)$ be a diagonal matrix, the matrix form of the above derivative is as follows:\n\n$$\n    \\frac{\\partial h}{\\partial X} = diag[h(1 - h)] \\, W^T\n$$\n\nWe need to form a diagonal matrix of the gradient of $h$ because if we look carefully at the original equation, the first term doesn't depend on $i$. Hence, for all values of $W_i$, we want to multiply it with the correspondent $h_j$. And the nice way to do that is to use diagonal matrix.\n\nAs our main objective is to calculate the norm, we could simplify that in our implementation so that we don't need to construct the diagonal matrix:\n\n$$\n\\begin{align}\n\n\\lVert J_h(X) \\rVert_F^2 &= \\sum_{ij} \\left( \\frac{\\partial h_j}{\\partial X_i} \\right)^2 \\\\[10pt]\n                         &= \\sum_i \\sum_j [h_j(1 - h_j)]^2 (W_{ji}^T)^2 \\\\[10pt]\n                         &= \\sum_j [h_j(1 - h_j)]^2 \\sum_i (W_{ji}^T)^2 \\\\[10pt]\n\n\\end{align}\n$$\n\nTranslated to code:\n\n```python\nimport numpy as np\n\n# Let's say we have minibatch of 32, and 64 hidden units\n# Our input is 786 elements vector\nX = np.random.randn(32, 786)\nW = np.random.randn(786, 64)\n\nZ = np.dot(W, X)\nh = sigmoid(Z) # 32x64\n\nWj_sqr = np.sum(W.T**2, axis=1) # Marginalize i (note the transpose), 64x1\ndhj_sqr = (h * (1 - h))**2 # Derivative of h, 32x64\nJ_norm = np.sum(dhj_sqr * Wj_sqr, axis=1) # 32x1, i.e. 1 jacobian norm for each data point\n```\n\nPutting all of those together, we have our full Contractive Autoencoder implemented in Keras:\n\n```python\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nimport keras.backend as K\n\nlam = 1e-4\n\ninputs = Input(shape=(N,))\nencoded = Dense(N_hidden, activation='sigmoid', name='encoded')(inputs)\noutputs = Dense(N, activation='linear')(encoded)\n\nmodel = Model(input=inputs, output=outputs)\n\ndef contractive_loss(y_pred, y_true):\n    mse = K.mean(K.square(y_true - y_pred), axis=1)\n\n    W = K.variable(value=model.get_layer('encoded').get_weights()[0])  # N x N_hidden\n    W = K.transpose(W)  # N_hidden x N\n    h = model.get_layer('encoded').output\n    dh = h * (1 - h)  # N_batch x N_hidden\n\n    # N_batch x N_hidden * N_hidden x 1 = N_batch x 1\n    contractive = lam * K.sum(dh**2 * K.sum(W**2, axis=1), axis=1)\n\n    return mse + contractive\n\nmodel.compile(optimizer='adam', loss=contractive_loss)\nmodel.fit(X, X, batch_size=N_batch, nb_epoch=5)\n```\n\nAnd that is it! The full code could be found in my Github repository: https://github.com/wiseodd/hipsternet.\n\n## References\n\n1. Rifai, Salah, et al. \"Contractive auto-encoders: Explicit invariance during feature extraction.\" Proceedings of the 28th international conference on machine learning (ICML-11). 2011.","src/content/post/contractive-autoencoder.mdx","5508ddb51be7557c","contractive-autoencoder.mdx","autoencoders",{"id":118,"data":120,"body":125,"filePath":126,"digest":127,"legacyId":128,"deferredRender":23},{"title":121,"description":122,"publishDate":123,"draft":15,"tags":124},"Many flavors of Autoencoder","Autoencoder is a family of methods that answers the problem of data reconstruction using neural net. There are several variation of Autoencoder: sparse, multilayer, and convolutional. In this post, we will look at those different kind of Autoencoders and learn how to implement them with Keras.",["Date","2016-12-03T17:20:00.000Z"],[42,43,102],"Consider a neural net. Usually we use it for classification and regression task, that is, given an input vector $X$, we want to find $y$. In other words, we want neural net to find a mapping $y = f(X)$.\n\nNow, what happens if we use the same data as codomain of the function? That is, we want to find a mapping $X = f(X)$. Well, the neural net now will learn an identity mapping of $X$. We probably would ask, how is that useful?\n\nIt turns out, the hidden layer(s) of neural net learns a very interesting respresentation of the data. Hence, we can use the hidden layer representation for many things, for example data compression, dimensionality reduction, and feature learning. This is exactly the last decade idea of Deep Learning: by stacking Autoencoders to learn the representation of data, and train it greedily, hopefully we can train deep net effectively.\n\n## Vanilla Autoencoder\n\nIn its simplest form, Autoencoder is a two layer net, i.e. a neural net with one hidden layer. The input and output are the same, and we learn how to reconstruct the input, for example using the $\\ell_{2}$ norm.\n\n```python\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.regularizers import activity_l1\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras.backend as K\nimport tensorflow as tf\n\nmnist = input_data.read_data_sets('../data/MNIST_data', one_hot=True)\nX, _ = mnist.train.images, mnist.train.labels\n\ninputs = Input(shape=(784,))\nh = Dense(64, activation='sigmoid')(inputs)\noutputs = Dense(784)(h)\n\nmodel = Model(input=inputs, output=outputs)\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X, X, batch_size=64, nb_epoch=5)\n```\n\nOne question that might surface is if we are essentially learning an identity mapping, why do we even bother using a fancy algorithm? Isn't identity mapping trivial? Well, we are trying to learn identity mapping with some constraints, hence it's non trivial. The constraints might arise because of the architectural decision of the neural net.\n\nConsider this. In our implementation above, we use a hidden layer with dimension of 64. The data we are going to learn is a vector with dimension of 784. Hence, we can see that we are imposing a constraint in our neural net such that we learn a compressed representation of data.\n\n## Sparse Autoencoder\n\nAnother way we can constraint the reconstruction of Autoencoder is to impose a constraint in its loss. We could, for example, add a reguralization term in the loss function. Doing this will make our Autoencoder to learn sparse representation of data.\n\n```python\ninputs = Input(shape=(784,))\nh = Dense(64, activation='sigmoid', activity_regularizer=activity_l1(1e-5))(inputs)\noutputs = Dense(784)(h)\n\nmodel = Model(input=inputs, output=outputs)\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X, X, batch_size=64, nb_epoch=5)\n```\n\nNotice in our hidden layer, we added an $\\ell_{1}$ penalty. As a result, the representation is now sparser compared to the vanilla Autoencoder. We could see that by looking at the statistics of the hidden layer. The mean value of vanilla Autoencoder is 0.512477, whereas Sparse Autoencoder 0.148664.\n\n## Multilayer Autoencoder\n\nOne natural thought that might arise is to extend the Autoencoder beyond just single layer.\n\n```python\ninputs = Input(shape=(784,))\nh = Dense(128, activation='relu')(inputs)\nencoded = Dense(64, activation='relu', activity_regularizer=activity_l1(1e-5))(h)\nh = Dense(128, activation='relu')(encoded)\noutputs = Dense(784)(h)\n\nmodel = Model(input=inputs, output=outputs)\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X, X, batch_size=64, nb_epoch=5)\n```\n\nNow our implementation uses 3 hidden layers instead of just one. We could pick any layer as the feature representation, but for simplicity sake, let's make it simmetrical and use the middle-most layer.\n\n## Convolutional Autoencoder\n\nWe then naturally extend our thinking: can we use convnet instead of FCN?\n\n```python\ninputs = Input(shape=(28, 28, 1))\nh = Conv2D(4, 3, 3, activation='relu', border_mode='same')(inputs)\nencoded = MaxPooling2D((2, 2))(h)\nh = Conv2D(4, 3, 3, activation='relu', border_mode='same')(encoded)\nh = UpSampling2D((2, 2))(h)\noutputs = Conv2D(1, 3, 3, activation='relu', border_mode='same')(h)\n\nmodel = Model(input=inputs, output=outputs)\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X, X, batch_size=64, nb_epoch=5)\n```\n\nAbove we could see that instead of using fully connected layer, we use convolution and pooling layers as seen in convnet.\n\n## Conclusion\n\nIn this post we looked at many different types of Autoencoder: vanilla, sparse, multilayer, convolutional. Each has different intriguing property that comes from the imposed constraints, be it from the architectural choice or additional penalty term in the loss function.\n\nThe learned representation of Autoencoder can be used for dimensionality reduction or compression, and can be used as a features for another task. The way it is being used is analogous of using things like PCA to transform the features. It has been shown empirically that using learned features of Autoencoder, one can get significant boost in classification performance [3].\n\n## References\n\n1. https://en.wikipedia.org/wiki/Autoencoder\n2. https://blog.keras.io/building-autoencoders-in-keras.html\n3. Rifai, Salah, et al. \"Contractive auto-encoders: Explicit invariance during feature extraction.\" Proceedings of the 28th international conference on machine learning (ICML-11). 2011.","src/content/post/autoencoders.mdx","176da65d54c99a27","autoencoders.mdx","convnet-maxpool-layer",{"id":129,"data":131,"body":136,"filePath":137,"digest":138,"legacyId":139,"deferredRender":23},{"title":132,"description":133,"publishDate":134,"draft":15,"tags":135},"Convnet: Implementing Maxpool Layer with Numpy","Another important building block in convnet is the pooling layer. Nowadays, the most widely used is the max pool layer. Let's dissect its Numpy implementation!",["Date","2016-07-18T06:00:00.000Z"],[17,42,43,44],"import BlogImage from \"@/components/BlogImage.astro\";\n\nTraditionally, convnet consists of several layers: convolution, pooling, fully connected, and softmax. Although it's not true anymore with the recent development. A lot of things going on out there and the architecture of convent has been steadily (r)evolving, something like Google's Inception module found in [GoogLeNet](http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf) and the recent ImageNet champion: [ResNet](https://arxiv.org/pdf/1512.03385v1).\n\nNevertheless, conv and pool layers are still the essential foundations of convnet. We've covered the conv layer in the last post. Now let's dig into pool layer, especially maxpool layer.\n\n## Pool layer\n\nWhereas conv layer applies filter to the input images, pool layer reduce the dimensionality of the output. Reducing the dimensionality of the images is a necessity in convnet as we're dealing with a high dimensional data and a lot of filters, which implies that we will have huge amount of parameters in our convnet.\n\nWhat pool layer does is simple: at each patch of the image, do a summarization operation on it. If we recall, that's mildly similar to what conv layer does. Whereas at each location we're taking the dot product in conv layer, we're going to do simple summarization in a particular image patch.\n\nThe summarization operation could be any summary statistics: average, max, min, median, you name it. But, the most widely used operation is the max operation. This, combined with the adjustment of the size, padding, and stride of our image patch will result in some nice properties that are useful for our model.\n\nThose are, mainly, the reduction of the dimensionality = less parameter = less computation burden; and slightly more robust model, because we're taking \"high level views\" of our images, the network will be slightly invariant towards small changes like rotation, translation, etc.\n\nFor more about theoritical and best practices about pool layer, head to CS231n lecture page: http://cs231n.github.io/convolutional-networks/#pool.\n\n## Maxpool layer\n\nKnowing what pool layer does, it's trivial to think about the summarization operation. For maxpool layer, it's just taking the maximum value of each image patch.\n\nIt's just the same as conv layer with one exception: max instead of dot product.\n\n## Maxpool forward\n\nAs we already know that maxpool layer is similar to conv layer, implementing it is somewhat easier.\n\n```python\n# Let say our input X is 5x10x28x28\n# Our pooling parameter are: size = 2x2, stride = 2, padding = 0\n# i.e. result of 10 filters of 3x3 applied to 5 imgs of 28x28 with stride = 1 and padding = 1\n# First, reshape it to 50x1x28x28 to make im2col arranges it fully in column\nX_reshaped = X.reshape(n \\* d, 1, h, w)\n\n# The result will be 4x9800\n# Note if we apply im2col to our 5x10x28x28 input, the result won't be as nice: 40x980\nX_col = im2col_indices(X_reshaped, size, size, padding=0, stride=stride)\n\n# Next, at each possible patch location, i.e. at each column, we're taking the max index\nmax_idx = np.argmax(X_col, axis=0)\n\n# Finally, we get all the max value at each column\n# The result will be 1x9800\nout = X_col[max_idx, range(max_idx.size)]\n\n# Reshape to the output size: 14x14x5x10\nout = out.reshape(h_out, w_out, n, d)\n\n# Transpose to get 5x10x14x14 output\nout = out.transpose(2, 3, 0, 1)\n```\n\nThat's it for the forward computation of maxpool layer. However, instead of getting the maximum value directly, we did an intermediate step: getting the maximum index first. This is because the index are useful for the backward computation.\n\nAt above example, we could see how maxpool layer will reduce the computation for the subsequent step. Doing 2x2 pooling with stride of 2 and no padding will essentially reduce the image dimension by half.\n\nFor example, we have this single MNIST data of 28x28:\n\n\u003CBlogImage imagePath='/img/convnet-maxpool-layer/pool_input.png' />\n\nAfter we fed the image to our maxpool layer, the result will look like this:\n\n\u003CBlogImage imagePath='/img/convnet-maxpool-layer/pool_output.png' />\n\n## Maxpool backward\n\nRecall, how do we compute the gradient for ReLU layer. We let the gradient pass through when the ReLU result is non zero, and otherwise we block the gradient by setting it to zero.\n\nMaxpool layer is similar, because that's essentially what max operation do in backpropagation.\n\n```python\n# X_col and max_idx are the intermediate variables from the forward propagation step\n# Suppose our output from forward propagation step is 5x10x14x14\n# We want to upscale that back to 5x10x28x28, as in the forward step\n# 4x9800, as in the forward step\ndX_col = np.zeros_like(X_col)\n\n# 5x10x14x14 => 14x14x5x10, then flattened to 1x9800\n# Transpose step is necessary to get the correct arrangement\ndout_flat = dout.transpose(2, 3, 0, 1).ravel()\n\n# Fill the maximum index of each column with the gradient\n# Essentially putting each of the 9800 grads\n# to one of the 4 row in 9800 locations, one at each column\ndX_col[max_idx, range(max_idx.size)] = dout_flat\n\n# We now have the stretched matrix of 4x9800, then undo it with col2im operation\n# dX would be 50x1x28x28\ndX = col2im_indices(dX_col, (n \\* d, 1, h, w), size, size, padding=0, stride=stride)\n\n# Reshape back to match the input dimension: 5x10x28x28\ndX = dX.reshape(X.shape)\n```\n\nRecall the ReLU gradient is `dX[X \u003C= 0] = 0`, as we're doing `max(0, x)` in ReLU. We're basically applying that to our stretched image patches. Only, we start with 0 matrix and put the gradient in the correct location, and we're taking the max of the image patch, instead of comparing it with 0 like we do in ReLU.\n\n## Conclusion\n\nWe see that pool layer, specifically maxpool layer is similar to conv and ReLU layer. It's similar as conv as we need to stretch our input image with im2col to get the all possible image patches where we are going to take the maximum value over. It's similar as ReLU as we're doing the same operation: max.\n\nWe also see that doing maxpool with certain parameters, e.g. 2x2 maxpool with stride of 2 and padding of 0 will essentially halve the dimension of the input.\n\n## References\n\n- http://cs231n.github.io/convolutional-networks/#pool\n- http://vision.stanford.edu/teaching/cs231n/winter1516_assignment2.zip\n- https://arxiv.org/pdf/1512.03385v1\n- http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf","src/content/post/convnet-maxpool-layer.mdx","bc8e71e85b8952aa","convnet-maxpool-layer.mdx","convnet-conv-layer",{"id":140,"data":142,"body":147,"filePath":148,"digest":149,"legacyId":150,"deferredRender":23},{"title":143,"description":144,"publishDate":145,"draft":15,"tags":146},"Convnet: Implementing Convolution Layer with Numpy","Convnet is dominating the world of computer vision right now. What make it special of course the convolution layer, hence the name. Let's study it further by implementing it from scratch using Numpy!",["Date","2016-07-16T14:55:00.000Z"],[17,42,43,44],"Convolutional Neural Network or CNN or convnet for short, is everywhere right now in the wild. Almost every computer vision systems that was recently built are using some kind of convnet architecture. Since the [AlexNet's](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) groundbreaking result in ImageNet 2012 challenge, every year, it was convnet's year.\n\nThere are many factors that contribute to convnet's success. One of them is the effectiveness of the convolution layer; the heart of convnet.\n\nArchitecture wise, convnet is just a usual feed forward net, put on top of convolution layer(s). So really, convolution layer is a kind of feature extractor that can effectively learn the optimal features, which makes the linear classifier put on top of it looks good.\n\nNowadays, to build a convnet model, it's easy: install one of those popular Deep Learning libraries like TensorFlow, Torch, or Theano, and use the prebuilt module to rapidly build the model.\n\nHowever, to understand the convnet better, it's essential to get our hands dirty. So, let's try implementing the conv layer from scratch using Numpy!\n\n## Conv layer\n\nAs we've already know, every layer in a neural net consists of forward and backward computation, because of the backpropagation. Conv layer is no different, as essentially, it's just another neural net layer.\n\nBefore we get started, of course we need some theoritical knowledge of convnet. For this, I'd direct you to the excellent [CS231n class](http://cs231n.github.io/convolutional-networks/).\n\nHaving already understand the theories, it's time for us to implement it. First, the forward computation!\n\n## Conv layer forward\n\nAs stated in the [CS231n class](http://cs231n.github.io/convolutional-networks/), we can think of convolutional operation as a matrix multiplication, as essentially at every patch of images, we apply a filter on it by taking their dot product. What nice about this is that we could think about conv layer as feed forward layer (your usual neural net hidden layer) to some extent.\n\nThe implication is very nice: we can reuse our knowledge and thought process when implementing conv layer! We will see that later, especially in the backward computation.\n\nAlright, let's define our function:\n\n```python\ndef conv_forward(X, W, b, stride=1, padding=1):\n    pass\n```\n\nOur conv layer will accept an input in `X: DxCxHxW` dimension, input filter `W: NFxCxHFxHW`, and bias `b: Fx1`, where:\n\n- `D` is the number of input\n- `C` is the number of image channel\n- `H` is the height of image\n- `W` is the width of the image\n- `NF` is the number of filter in the filter map `W`\n- `HF` is the height of the filter, and finally\n- `HW` is the width of the filter.\n\nAnother important parameters are the stride and padding of the convolution operation.\n\nAs stated before, we will approach the conv layer as kind of normal feed forward layer, which is just the matrix multiplication between the input and the weight. To do this, we will use a utility function called `im2col`, which essentially will stretch our input image depending on the filter, stride, and width. `im2col` utilities could be found in the [second assigment files of CS231n](http://vision.stanford.edu/teaching/cs231n/winter1516_assignment2.zip).\n\nLet's say we have a single image of `1x1x10x10` size and a single filter of `1x1x3x3`. We also use stride of 1 and padding of 1. Then, naively, if we're going to do convolution operation for our filter on the image, we will loop over the image, and take the dot product at each `3x3` location, because our filter size is `3x3`. The result is a single `1x1x10x10` image.\n\nBut, what if we don't want to do the loop? Or is there a nice way to do it?\n\nYes, there is!\n\nWhat we need is to gather all the possible locations that we can apply our filter at, then do a single matrix multiplication to get the dot product at each of those possible locations. Hence, with the above setup, we will have 100 possible locations, the same as our original input, because doing `3x3` convolution with 1 stride and 1 padding will preserve the input dimension.\n\nAt every those 100 possible location, there exists the `3x3` patch, stretched to `9x1` column vector that we can do our `3x3` convolution on. So, with `im2col`, our image dimension now is: `9x100`.\n\nTo make the operation compatible, we will arrange our filter to `1x9`. Now, if we do a matrix multiplication over our stretched image and filter, we will have `1x100` image as a result, which we could reshape it back to `10x10` or `1x1x10x10` image.\n\nLet's see the code for that.\n\n```python\n# Let this be 3x3 convolution with stride = 1 and padding = 1\n# Suppose our X is 5x1x10x10, X_col will be a 9x500 matrix\nX_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n\n# Suppose we have 20 of 3x3 filter: 20x1x3x3. W_col will be 20x9 matrix\nW_col = W.reshape(n_filters, -1)\n\n# 20x9 x 9x500 = 20x500\nout = W_col @ X_col + b\n\n# Reshape back from 20x500 to 5x20x10x10\n# i.e. for each of our 5 images, we have 20 results with size of 10x10\nout = out.reshape(n_filters, h_out, w_out, n_x)\nout = out.transpose(3, 0, 1, 2)\n```\n\nThat basically it for the forward computation of the convolution layer. It's similar to the feed forward layer with two additions: `im2col` operation and thinkering about the dimension of our matrices.\n\nIt's definitely harder to implement, mainly because thinking in multidimension isn't that nice. We have to be careful with the dimension manipulation operations like the reshape and transpose as it's tricky to work with.\n\n## Conv layer backward\n\nOne of the trickiest part of implementing neural net model from scratch is to derive the partial derivative of a layer. Conv layer is no different, it's even more trickier as we have to deal with different operation (convolution instead of just affine transformation) and higher dimensional matrices.\n\nThankfully because we're using the `im2col` trick, we at least could reuse our knowledge on implementing the feed forward layer's backward computation!\n\nFirst let's compute our bias gradient.\n\n```python\ndb = np.sum(dout, axis=(0, 2, 3))\ndb = db.reshape(n_filter, -1)\n```\n\nRemember that the matrix we're dealing with, i.e. `dout` is a `5x20x10x10` matrix, similar to the output of the forward computation step. As the bias is added to each of our filter, we're accumulating the gradient to the dimension that represent of the number of filter, which is the second dimension. Hence the sum is operated on all axis except the second.\n\nNext, we will compute the gradient of the the filters `dW`.\n\n```python\n# Transpose from 5x20x10x10 into 20x10x10x5, then reshape into 20x500\ndout_reshaped = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n\n# 20x500 x 500x9 = 20x9\ndW = dout_reshaped @ X_col.T\n\n# Reshape back to 20x1x3x3\ndW = dW.reshape(W.shape)\n```\n\nIt's similar with the normal feed forward layer, except with more convoluted (ha!) dimension manipulation.\n\nLastly, the input gradient `dX`. We're almost there!\n\n```python\n# Reshape from 20x1x3x3 into 20x9\nW_reshape = W.reshape(n_filter, -1)\n\n# 9x20 x 20x500 = 9x500\ndX_col = W_reshape.T @ dout_reshaped\n\n# Stretched out image to the real image: 9x500 => 5x1x10x10\ndX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n```\n\nAgain, it's the same as feed forward layer with some careful reshaping! At the end though, we're getting the gradient of the stretched image (recall the use of `im2col`). To undo this, and getting the real image gradient, we're going to de-im2col that. We're going to apply the operation called `col2im` to the stretched image. And now we have our image input gradient!\n\n## Full source code\n\nHere's the full source code for the forward and backward computation of the conv layer.\n\n```python\ndef conv_forward(X, W, b, stride=1, padding=1):\n    cache = W, b, stride, padding\n    n_filters, d_filter, h_filter, w_filter = W.shape\n    n_x, d_x, h_x, w_x = X.shape\n    h_out = (h_x - h_filter + 2 * padding) / stride + 1\n    w_out = (w_x - w_filter + 2 * padding) / stride + 1\n\n    if not h_out.is_integer() or not w_out.is_integer():\n        raise Exception('Invalid output dimension!')\n\n    h_out, w_out = int(h_out), int(w_out)\n\n    X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n    W_col = W.reshape(n_filters, -1)\n\n    out = W_col @ X_col + b\n    out = out.reshape(n_filters, h_out, w_out, n_x)\n    out = out.transpose(3, 0, 1, 2)\n\n    cache = (X, W, b, stride, padding, X_col)\n\n    return out, cache\n\ndef conv_backward(dout, cache):\n    X, W, b, stride, padding, X_col = cache\n    n_filter, d_filter, h_filter, w_filter = W.shape\n\n    db = np.sum(dout, axis=(0, 2, 3))\n    db = db.reshape(n_filter, -1)\n\n    dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n    dW = dout_reshaped @ X_col.T\n    dW = dW.reshape(W.shape)\n\n    W_reshape = W.reshape(n_filter, -1)\n    dX_col = W_reshape.T @ dout_reshaped\n    dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n\n    return dX, dW, db\n```\n\nAlso check out the complete code in my repository: https://github.com/wiseodd/hipsternet!\n\n## Conclusion\n\nAs we can see, conv layer is just an extension of the normal feed forward layer, with some additions. Those are the `im2col` operation and dimension manipulation, as convnet, particularly for computer vision task assumes our input and weight to be arranged as 2d or 3d images, because we're trying to capture the spatial information in our data.\n\nDealing with multidimensional matrices as we will always encounter in convnet is tricky. A lot of careful dimension manipulation need to be done. But it's definitely a great exercise to visualize them in our mind!\n\n## References\n\n- http://cs231n.github.io/convolutional-networks/\n- http://vision.stanford.edu/teaching/cs231n/winter1516_assignment2.zip\n- http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf","src/content/post/convnet-conv-layer.mdx","4558ba8fa53a4ed2","convnet-conv-layer.mdx","coupled-gan",{"id":151,"data":153,"body":158,"filePath":159,"digest":160,"legacyId":161,"deferredRender":23},{"title":154,"description":155,"publishDate":156,"draft":15,"tags":157},"CoGAN: Learning joint distribution with GAN","Original GAN and Conditional GAN are for learning marginal and conditional distribution of data respectively. But how can we extend them to learn joint distribution instead?",["Date","2017-02-18T09:27:00.000Z"],[17,56],"import BlogImage from \"@/components/BlogImage.astro\";\n\nThe full code is available here: https://github.com/wiseodd/generative-models.\n\nVanilla GAN is a method to learn marginal distribution of data $P(X)$. Since then, it has been extended to make it learns conditional distribution $P(X \\vert c)$. Naturally, the next extension of GAN is to learn joint distribution of data $P(X_1, X_2)$, where $X_1$ and $X_2$ are from different domain, e.g. color image and its corresponding B&W version.\n\nCoupled GAN (CoGAN) is a method that extends GAN so that it could learn joint distribution, by only needing samples from the marginals. What it means is that we do not need to sample from joint distribution $P(X_1, X_2)$, i.e. a tuple of $(x_1, x_2)$, during training. We only need $x_1 \\sim P(X_1)$ and $x_2 \\sim P(X_2)$, samples from the marginal distributions. This property makes CoGAN very useful as collecting representing samples of joint distribution is costly due to curse of dimensionality.\n\n## Learning joint distribution by sharing weights\n\nSo, how exactly does CoGAN learn joint distribution by only using the marginals?\n\nThe trick here is to add a constraint such that high level representations of data are shared. Specifically, we constraint our networks to have the same weights on several layers. The intuition is that by constraining the weights to be identical to each other, CoGAN will converge to the optimum solution where those weights represent shared representation (joint representation) of both domains of data.\n\n\u003CBlogImage\n  imagePath='/img/coupled-gan/schematic.png'\n  altText='CoGAN schematic.'\n  fullWidth\n/>\n\nBut which layers should be constrained? To answer this, we need to observe that neural nets that are used for classification tasks learn data representation in bottom-up fashion, i.e. from low level representation to high level representation. We notice that low level representation is highly specialized on data, which is not general enough. Hence, we constraint our neural net on several layers that encode the high level representation.\n\nIntuitively, the lower level layers capture image specific features, e.g. the thickness of edges, the saturation of colors, etc. But, higher level layers capture more general features, such as the abstract representation of \"bird\", \"dog\", etc., ignoring the color or the thickness of the images. So, naturally, to capture joint representation of data, we want to use higher level layers, then use lower level layers to encode those abstract representation into image specific features, so that we get the correct (in general sense) and plausible (in detailed sense) images.\n\nUsing that reasoning, we then could choose which layers should be constrained. For discriminator, it should be the last layers. For generator, it should be the first layers, as generator in GAN solves inverse problem: from latent representation $z$ to image $X$.\n\n## CoGAN algorithm\n\nIf we want to learn joint distribution of $K$ domains, then we need to use $2K$ neural nets, as for each domain we need a discriminator and a generator. Fortunately, as CoGAN is centered on weight sharing, this could prove helpful to reduce the computation cost.\n\nThe algorithm for CoGAN for 2 domains is as follows:\n\n\u003CBlogImage imagePath='/img/coupled-gan/algo.png' altText='CoGAN algorithm.' fullWidth />\n\nNotice that CoGAN draws samples from each marginal distribution. That means, we only need 2 sets of training data. We do not need to construct specialized training data that captures joint distribution of those two domains. However, as we learn joint distribution by weight sharing on high level features, to make CoGAN training successful, we have to make sure that those two domains of data share some high level representations.\n\n## Pytorch implementation of CoGAN\n\nIn this implementation, we are going to learn joint distribution of two domains of MNIST data: normal MNIST data and rotated MNIST data (90 degree). Notice that those domains of data share the same high level representation (digit), and only differ on the presentation (low level features). Here's the code to generate those training sets:\n\n```python\nX_train = mnist.train.images\nhalf = int(X_train.shape[0] / 2)\n\n# Real image\nX_train1 = X_train[:half]\n\n# Rotated image\nX_train2 = X_train[half:].reshape(-1, 28, 28)\nX_train2 = scipy.ndimage.interpolation.rotate(X_train2, 90, axes=(1, 2))\nX_train2 = X_train2.reshape(-1, 28*28)\n```\n\nLet's declare the generators first, which are two layers fully connected nets, with first weight (input to hidden) shared:\n\n```python\n\"\"\" Shared Generator weights \"\"\"\nG_shared = torch.nn.Sequential(\n    torch.nn.Linear(z_dim, h_dim),\n    torch.nn.ReLU(),\n)\n\n\"\"\" Generator 1 \"\"\"\nG1_ = torch.nn.Sequential(\n    torch.nn.Linear(h_dim, X_dim),\n    torch.nn.Sigmoid()\n)\n\n\"\"\" Generator 2 \"\"\"\nG2_ = torch.nn.Sequential(\n    torch.nn.Linear(h_dim, X_dim),\n    torch.nn.Sigmoid()\n)\n```\n\nThen we make a wrapper for those nets:\n\n```python\ndef G1(z):\n    h = G_shared(z)\n    X = G1_(h)\n    return X\n\ndef G2(z):\n    h = G_shared(z)\n    X = G2_(h)\n    return X\n```\n\nNotice that `G_shared` are being used in those two nets.\n\nThe discriminators are also two layers nets, similar to the generators, but share weights on the last section: hidden to output.\n\n```python\n\"\"\" Shared Discriminator weights \"\"\"\nD_shared = torch.nn.Sequential(\n    torch.nn.Linear(h_dim, 1),\n    torch.nn.Sigmoid()\n)\n\n\"\"\" Discriminator 1 \"\"\"\nD1_ = torch.nn.Sequential(\n    torch.nn.Linear(X_dim, h_dim),\n    torch.nn.ReLU()\n)\n\n\"\"\" Discriminator 2 \"\"\"\nD2_ = torch.nn.Sequential(\n    torch.nn.Linear(X_dim, h_dim),\n    torch.nn.ReLU()\n)\n\ndef D1(X):\n    h = D1_(X)\n    y = D_shared(h)\n    return y\n\ndef D2(X):\n    h = D2_(X)\n    y = D_shared(h)\n    return y\n```\n\nNext, we construct the optimizer:\n\n```python\nD_params = (list(D1.parameters()) + list(D2.parameters()) + list(D_shared.parameters()))\nG_params = (list(G1.parameters()) + list(G2.parameters()) + list(G_shared.parameters()))\n\nG_solver = optim.Adam(G_params, lr=lr)\nD_solver = optim.Adam(D_params, lr=lr)\n```\n\nNow we are ready to train CoGAN. At each training iteration, we do these steps below. First, we sample images from both marginal training sets, and $z$ from our prior:\n\n```python\nX1 = sample_x(X_train1, mb_size)\nX2 = sample_x(X_train2, mb_size)\nz = Variable(torch.randn(mb_size, z_dim))\n```\n\nThen, train the discriminators by using using `X1` for `D1` and `X2` for `D2`. On both discriminators, we use the same `z`. The loss function is just vanilla GAN loss.\n\n```python\nG1_sample = G1(z)\nD1_real = D1(X1)\nD1_fake = D1(G1_sample)\n\nD1_loss = torch.mean(-torch.log(D1_real + 1e-8) - torch.log(1. - D1_fake + 1e-8))\nD2_loss = torch.mean(-torch.log(D2_real + 1e-8) - torch.log(1. - D2_fake + 1e-8))\n```\n\nThen we just add up those loss. During backpropagation, `D_shared` will naturally get gradients from both `D1` and `D2`, i.e. sum of both branches. All we need to do to get the average is to scale them:\n\n```python\nD_loss = D1_loss + D2_loss\nD_loss.backward()\n\n# Average the gradients\nfor p in D_shared.parameters():\n    p.grad.data = 0.5 * p.grad.data\n```\n\nAs we have all the gradients, we could update the weights:\n\n```python\nD_solver.step()\nreset_grad()\n```\n\nFor generators training, the procedure is similar to discriminators training, where we need to average the loss of `G1` and `G2` w.r.t. `G_shared`.\n\n```python\n# Generator\nG1_sample = G1(z)\nD1_fake = D1(G1_sample)\n\nG2_sample = G2(z)\nD2_fake = D2(G2_sample)\n\nG1_loss = torch.mean(-torch.log(D1_fake + 1e-8))\nG2_loss = torch.mean(-torch.log(D2_fake + 1e-8))\nG_loss = G1_loss + G2_loss\n\nG_loss.backward()\n\n# Average the gradients\nfor p in G_shared.parameters():\n    p.grad.data = 0.5 * p.grad.data\n\nG_solver.step()\nreset_grad()\n```\n\n## Results\n\nAfter many thousands of iterations, `G1` and `G2` will produce these kind of samples. Note, first two rows are the normal MNIST images, the next two rows are the rotated images. Also, the $z$ that were fed into `G1` and `G2` are the same so that we could see given the same latent code $z$, we could sample $( x_1, x_2 )$ that are corresponding to each other from the joint distribution.\n\n\u003CBlogImage imagePath='/img/coupled-gan/res1.png' altText='Result.' />\n\n\u003CBlogImage imagePath='/img/coupled-gan/res2.png' altText='Result.' />\n\nObviously, if we swap our nets with more powerful ones, we could get higher quality samples.\n\nIf we squint, we could see that _roughly_, images at the third row are the 90 degree rotation of the first row. Also, the fourth row are the corresponding images of the second row.\n\nThis is a marvelous results considering we did not explicitly show CoGAN the samples from joint distribution (i.e. a tuple of $(x_1, x_2)$). We only show samples from disjoint marginals. In summary, CoGAN is able to infer the joint distribution by itself.\n\n## Conclusion\n\nIn this post, we looked at CoGAN: Coupled GAN, a GAN model that is used to learn joint distribution of data from different domains.\n\nWe learned that CoGAN learned joint distribution by enforcing weight sharing constraint on its high level representation weights. We also noticed that CoGAN only needs to see samples from marginal distributions, not the joint itself.\n\nFinally, by inspecting the samples acquired from generators, we saw that CoGAN correctly learns joint distribution, as those samples are correspond to each other.\n\n## References\n\n1. Liu, Ming-Yu, and Oncel Tuzel. \"Coupled generative adversarial networks.\" Advances in Neural Information Processing Systems. 2016.","src/content/post/coupled-gan.mdx","ed4153c349dceae9","coupled-gan.mdx","conv-probit",{"id":162,"data":164,"body":169,"filePath":170,"digest":171,"legacyId":172,"deferredRender":23},{"title":165,"description":166,"publishDate":167,"draft":15,"tags":168},"Convolution of Gaussians and the Probit Integral","Gaussian distributions are very useful in Bayesian inference due to their (many!) convenient properties. In this post we take a look at two of them: the convolution of two Gaussian pdfs and the integral of the probit function w.r.t. a Gaussian measure.",["Date","2022-06-25T04:00:00.000Z"],[],"Gaussian distributions are very useful in Bayesian inference due to their (many!) convenient properties.\nIn this post we take a look at two of them: the convolution of two Gaussian pdfs and the integral of the probit function w.r.t. a Gaussian measure.\n\n## Convolution and the Predictive Distribution of Gaussian Regression\n\nLet's start with the **_convolution_** $\\N(z_1 \\mid \\mu_1, \\sigma^2_1) * \\N(z_2 \\mid \\mu_2, \\sigma^2_2)$ of two Gaussians $\\N(z_1 \\mid \\mu_1, \\sigma^2_1)$ and $\\N(z_2 \\mid \\mu_2, \\sigma^2_2)$ on $\\R$:\n\n$$\n  \\N(z_1 \\mid \\mu_1, \\sigma^2_1) * \\N(z_2 \\mid \\mu_2, \\sigma^2_2) := \\int_{\\R} \\N(z_1 - z_2 \\mid \\mu_1, \\sigma^2_1) \\, \\N(z_2 \\mid \\mu_2, \\sigma^2_2) \\,dz_2 .\n$$\n\n**Proposition 1 (Convolution of Gaussians)** _Let $\\N(z_1 \\mid \\mu_1, \\sigma^2_1)$ and $\\N(z_2 \\mid \\mu_2, \\sigma^2_2)$ be two Gaussians on $\\R$._\n\n$$\n  \\N(z_1 \\mid \\mu_1, \\sigma^2_1) * \\N(z_2 \\mid \\mu_2, \\sigma^2_2) = \\N(z_1 \\mid \\mu_1+\\mu_2, \\sigma^2_1+\\sigma^2_2) .\n$$\n\n_Proof._\nBy the [convolution theorem](https://en.wikipedia.org/wiki/Convolution_theorem), the convolution of two functions is equivalent to the product of the functions' Fourier transforms.\nThe Fourier transform of a density function is given by its [characteristic function](https://en.wikipedia.org/wiki/Normal_distribution#Fourier_transform_and_characteristic_function).\nFor a Gaussian $f(x) := \\N(x, \\mu, \\sigma^2)$, it is $\\varphi(u) := \\exp\\left(-iu\\mu - \\frac{1}{2}u^2\\sigma^2\\right)$.\nTherefore, if $\\varphi_1$ and $\\varphi_2$ are the characteristic functions of $\\N(z_1 \\mid \\mu_1, \\sigma^2_1)$ and $\\N(z_2 \\mid \\mu_2, \\sigma^2_2)$, respectively, then\n\n$$\n\\begin{align}\n  (\\varphi_1 \\varphi_2)(u) &= \\exp\\left(-iu\\mu_1 - \\frac{1}{2}u^2\\sigma_1^2\\right) \\exp\\left(-iu\\mu_2 - \\frac{1}{2}u^2\\sigma_2^2\\right) \\\\[5pt]\n    %\n    &=  \\exp\\left(-iu(\\mu_1+\\mu_2) - \\frac{1}{2}u^2(\\sigma_1^2 + \\sigma_2^2)\\right) ,\n\\end{align}\n$$\n\nwhich we can immediately identify as the characteristic function of a Gaussian with mean $\\mu_1 + \\mu_2$ and variance $\\sigma_1^2 + \\sigma_2^2$.\n\n$$\n\\qed\n$$\n\nThis result is very useful in Bayesian machine learning, especially to obtain the predictive distribution of a Bayesian regression model.\nFor instance, when one knows that the distribution over the regressor's output is a Gaussian $\\N(f \\mid \\mu, \\sigma^2)$ and we assume that the output is noisy $\\N(y \\mid f, s^2)$.\n\n**Corollary 2 (Gaussian Regression).** _Let $p(y \\mid f) = \\N(y \\mid f, s^2)$ and $p(f) = \\N(f \\mid \\mu, \\sigma^2)$ are Gaussians on $\\R$. Then,_\n\n$$\n  p(y) = \\int_\\R p(y \\mid f) \\, p(f) \\,df = \\N(y \\mid f, \\sigma^2 + s^2) .\n$$\n\n_Proof._\nFirst, notice that Gaussian is symmetric:\n\n$$\n\\begin{align}\n  \\N(x - a \\mid \\mu, \\sigma^2) &= \\frac{1}{Z} \\exp\\left(-\\frac{1}{2\\sigma^2} ((x-a)-\\mu)^2\\right) \\\\[5pt]\n    %\n    &=  \\frac{1}{Z} \\exp\\left(-\\frac{1}{2\\sigma^2} (x-(\\mu+a))^2\\right) \\\\[5pt]\n    %\n    &= \\N(x \\mid \\mu + a, \\sigma^2) ,\n\\end{align}\n$$\n\nfor $x, a \\in \\R$, where $Z$ is the normalizing constant.\nUsing this, we can write the integral above as a convolution:\n\n$$\n\\begin{align}\n  \\int_\\R \\N(y \\mid f, s^2) \\, \\N(f \\mid \\mu, \\sigma^2) \\,df &= \\int_\\R \\N(y \\mid 0+f, s^2) \\, \\N(f \\mid \\mu, \\sigma^2) \\,df \\\\[5pt]\n    %\n    &= \\N(y \\mid 0, s^2) * \\N(f \\mid \\mu, \\sigma^2) .\n\\end{align}\n$$\n\nThus, by Proposition 1, we have $p(y) = \\N(y \\mid f, s^2 + \\sigma^2)$.\n\n$$\n\\qed\n$$\n\n## The Probit Integral and the Probit Approximation\n\n**_The probit function_** $\\Phi$ is the cumulative distribution function of the standard Normal distribution $\\N(x \\mid 0, 1)$ on $\\R$, i.e., $\\Phi(z) := \\int_{-\\infty}^z \\N(x \\mid 0, 1) \\,dx$.\nIt can conveniently be written in terms of the **_error function_**\n\n$$\n  \\mathrm{erf}(z) := \\frac{2}{\\sqrt{\\pi}} \\int_0^z \\exp(-x^2) \\,dx\n$$\n\nby\n\n$$\n  \\Phi(z) = \\frac{1}{2} \\left( 1 + \\mathrm{erf}\\left(\\frac{z}{\\sqrt{2}}\\right) \\right) .\n$$\n\n**Proposition 3 (The Probit Integral).** _If $\\N(x \\mid \\mu, \\sigma^2)$ be a Gaussian on $\\R$ and $a, b \\in \\R$ then_\n\n$$\n  \\int_{\\R} \\Phi(ax + b) \\, \\N(x \\mid \\mu, \\sigma^2) \\,dx = \\Phi\\left(\\frac{a\\mu + b}{\\sqrt{1 + a^2 \\sigma^2}}\\right).\n$$\n\n_Proof._\nThe standard property of the error function [2] says that\n\n$$\n  \\int_{\\R} \\mathrm{erf}(ax + b) \\, \\N(x \\mid \\mu, \\sigma^2) \\, dx = \\mathrm{erf}\\left(\\frac{a\\mu+b}{\\sqrt{1 + 2 a^2 \\sigma^2}}\\right) .\n$$\n\nSo,\n\n$$\n\\begin{align}\n  \\int_{\\R} &\\left(\\frac{1}{2} + \\frac{1}{2} \\mathrm{erf}\\left(\\frac{ax+b}{\\sqrt{2}}\\right)\\right) \\, \\N(x \\mid \\mu, \\sigma^2) \\,dx \\\\[5pt]\n    %\n    &= \\frac{1}{2} + \\frac{1}{2} \\int_{\\R} \\mathrm{erf}\\left(\\left(\\frac{a}{\\sqrt{2}}\\right)x+\\left(\\frac{b}{\\sqrt{2}}\\right)\\right) \\, \\N(x \\mid \\mu, \\sigma^2) \\,dx \\\\[5pt]\n    %\n    &= \\frac{1}{2} + \\frac{1}{2} \\mathrm{erf}\\left(\\frac{(a\\mu+b)/\\sqrt{2}}{\\sqrt{1 + \\cancel{2} (a/\\cancel{\\sqrt{2}})^2 \\sigma^2}}\\right) \\\\[5pt]\n    %\n    &= \\frac{1}{2} \\left(1 + \\mathrm{erf}\\left(\\frac{a\\mu+b}{\\sqrt{2} \\sqrt{1 + a^2 \\sigma^2}}\\right) \\right) \\\\[5pt]\n    %\n    &= \\Phi\\left(\\frac{a\\mu+b}{\\sqrt{1 + a^2 \\sigma^2}}\\right) .\n\\end{align}\n$$\n\n$$\n\\qed\n$$\n\nThis integral is very useful for Bayesian inference since it enables us to approximate the following integral that is ubiquitous in Bayesian binary classifications\n\n$$\n  \\int_{\\R} \\sigma(z) \\, \\N(z \\mid m, s^2) \\,dx ,\n$$\n\nwhere $\\sigma(z) := 1/(1 + \\exp(-z))$ is the **_logistic function_**.\n\nThe key idea is to notice that the probit and logistic function are both _sigmoid_ functions.\nThat is, their graphs have a similar \"S-shape\".\nMoreover, their images are both $[0, 1]$.\nHowever, they are a bit different---the probit function is more \"horizontally stretched\" compared to the logistic function.\n\nSo, the strategy to approximate the integral above is as follows: (i) horizontally \"contract\" the probit function and then (ii) use Proposition 3 to get an analytic approximation to the integral.\n\nFor the first step, this can be done by a simple change of coordinate: stretch the domain of the probit function with a constant $\\lambda$, i.e., $z \\mapsto \\lambda z$.\nThere are several \"good\" values for $\\lambda$, but commonly it is chosen to be $\\lambda = \\sqrt{\\pi/8}$, which makes the probit function have the same derivative as the logistic function at zero.\nThat is, we have the approximation $\\sigma(z) \\approx \\Phi(\\lambda z) = \\Phi(\\sqrt{\\pi/8} \\, z)$.\n\n**Corollary 4.** _If $\\N(z \\mid m, s^2)$ is a Gaussian on $\\R$, then_\n\n$$\n  \\int_{\\R} \\Phi(\\lambda z) \\, \\N(z \\mid m, s^2) \\, dz = \\Phi\\left( \\frac{m}{\\sqrt{\\lambda^{-2} + s^2}} \\right) .\n$$\n\n_Proof._\nBy Proposition 3, we have\n\n$$\n\\begin{align}\n  \\int_{\\R} \\Phi(\\lambda \\, z) \\, \\N(z \\mid m, s^2) \\, dz &= \\Phi\\left( \\frac{\\lambda \\mu}{\\sqrt{1 + \\lambda^2 s^2}} \\right) \\\\[5pt]\n    %\n    &= \\Phi\\left( \\frac{\\cancel{\\lambda} \\mu}{\\cancel{\\lambda} \\sqrt{\\lambda^{-2} + s^2}} \\right) .\n\\end{align}\n$$\n\n$$\n\\qed\n$$\n\nNow we are ready to obtain the final approximation, often called the **_probit approximation_**.\n\n**Proposition 5 (Probit Approximation)** _If $\\N(z \\mid m, s^2)$ is a Gaussian on $\\R$ and $\\sigma(z) \\approx \\Phi\\left(\\sqrt{\\pi/8} \\, z\\right)$, then_\n\n$$\n  \\int_{\\R} \\sigma(z) \\, \\N(z \\mid m, s^2) \\, dz \\approx \\sigma\\left( \\frac{m}{\\sqrt{1 + \\pi/8 \\, s^2}} \\right) .\n$$\n\n_Proof._\nLet $\\lambda = \\sqrt{\\pi/8}$.\nUsing Corollary 4 and substituting $\\Phi(z) \\approx \\sigma\\left(\\lambda^{-1} \\, z\\right)$:\n\n$$\n\\begin{align}\n  \\int_{\\R} \\sigma(z) \\, \\N(z \\mid m, s^2) \\,dz &\\approx \\Phi\\left( \\frac{m}{\\sqrt{\\lambda^{-2} + s^2}} \\right) \\\\[5pt]\n    %\n    &= \\sigma\\left( \\frac{\\lambda^{-1} \\, m}{\\sqrt{\\lambda^{-2} + s^2}} \\right) \\\\[5pt]\n    %\n    &= \\sigma\\left( \\frac{\\cancel{\\lambda^{-1}} \\, m}{\\cancel{\\lambda^{-1}} \\, \\sqrt{1 + \\lambda^2 \\, s^2}} \\right) .\n\\end{align}\n$$\n\nSubstituting $\\lambda^2 = \\pi/8$ into the last equation yields the desired result.\n\n$$\n\\qed\n$$\n\nThe probit approximation can also be used to obtain an approximation to the following integral, ubiquitous in multi-class classifications:\n\n$$\n  \\int_{\\R^k} \\mathrm{softmax}(z) \\, \\N(z \\mid \\mu, \\varSigma) \\, dz ,\n$$\n\nwhere the Gaussian is defined on $\\R^k$ and the softmax function is identified by its components $\\exp(z_i)/\\sum_{j=1}^k \\exp(z_j)$ for $i = 1, \\dots, k$.\n\n**Proposition 6 (Multiclass Probit Approximation; Gibbs, 1998).** _If $\\N(z \\mid \\mu, \\varSigma)$ is a Gaussian on $\\R^k$ and $\\sigma(z) \\approx \\Phi(\\sqrt{\\pi/8}\\,z)$, then_\n\n$$\n  \\int_{\\R^k} \\mathrm{softmax}(z) \\, \\N(z \\mid \\mu, \\varSigma) \\, dz \\approx \\mathrm{softmax}\\left( \\frac{\\mu}{\\sqrt{1 + \\pi/8 \\, \\diag \\varSigma}} \\right) ,\n$$\n\n_where the division in the r.h.s. is component-wise._\n\n_Proof._\nThe proof is based on [3].\nNotice that we can write the $i$-th component of $\\mathrm{softmax}(z)$ as $1/(1 + \\sum_{j \\neq i} \\exp(-(z_i - z_j)))$.\nSo, for each $i = 1, \\dots, k$, using $z_{ij} := z_i - z_j$, we can write\n\n$$\n\\begin{align}\n  \\frac{1}{1 + \\sum_{j \\neq i} \\exp(-z_{ij})} &= \\frac{1}{1 - (K-1) + \\sum_{j \\neq i} \\frac{1}{\\frac{1}{1 + \\exp(-z_{ij})}}} \\\\[5pt]\n    %\n    &= \\frac{1}{2-K+\\sum_{j \\neq i} \\frac{1}{\\sigma(z_{ij})}} .\n\\end{align}\n$$\n\nThen, we use the following approximations (which admittedly might be quite loose):\n\n1. $\\E(f(x)) \\approx f(\\E(x))$,\n2. the mean-field approximation $\\N(z \\mid \\mu, \\varSigma) \\approx \\N(z \\mid \\mu, \\diag{\\varSigma})$, and thus we have $z_i - z_j \\sim \\N(z_{ij} \\mid \\mu_i - \\mu_j, \\varSigma_{ii} + \\varSigma_{jj})$, and\n3. using the probit approximation (Proposition 5), with a further approximation\n\n$$\n\\begin{align}\n  \\int_{\\R} \\sigma(z_{ij}) \\, \\N(z_{ij} \\mid \\mu_i - \\mu_j, \\varSigma_{ii} + \\varSigma_{jj}) \\, dz_{ij} &\\approx \\sigma \\left( \\frac{\\mu_i - \\mu_j}{\\sqrt{1 + \\pi/8 \\, \\varSigma_{ii} + \\varSigma_{jj}}} \\right) \\\\[5pt]\n    %\n    &\\approx \\sigma \\left( \\frac{\\mu_i}{\\sqrt{1 + \\pi/8 \\, \\varSigma_{ii}}} - \\frac{\\mu_j}{\\sqrt{1 + \\pi/8 \\, \\varSigma_{jj}}} \\right) ,\n\\end{align}\n$$\n\nwe obtain\n\n$$\n\\begin{align}\n  \\int_{\\R^k} \\mathrm{softmax}_i(z) \\, \\N(z \\mid \\mu, \\varSigma) &\\approx \\frac{1}{2-K+\\sum_{j \\neq i} \\frac{1}{\\E \\sigma(z_{ij})}} \\\\[5pt]\n    %\n    &\\approx \\frac{1}{2-K+\\sum_{j \\neq i} \\frac{1}{\\sigma \\left( \\frac{\\mu_i}{\\sqrt{1 + \\pi/8 \\, \\varSigma_{ii}}} - \\frac{\\mu_j}{\\sqrt{1 + \\pi/8 \\, \\varSigma_{jj}}} \\right)}} \\\\[5pt]\n    %\n    &= \\frac{1}{1 + \\sum_{j \\neq i} \\exp\\left( -\\left(\\frac{\\mu_i}{\\sqrt{1 + \\pi/8 \\, \\varSigma_{ii}}} - \\frac{\\mu_j}{\\sqrt{1 + \\pi/8 \\, \\varSigma_{jj}}} \\right)\\right)} \\\\[5pt]\n    %\n    &= \\frac{\\exp\\left(\\mu_i/\\sqrt{1 + \\pi/8 \\, \\varSigma_{ii}}\\right)}{\\sum_{j=1}^k \\exp\\left(\\mu_j/\\sqrt{1 + \\pi/8 \\, \\varSigma_{jj}}\\right)}\n\\end{align}\n$$\n\nWe identify the last equation above as the $i$-th component of $\\mathrm{softmax}\\left( \\frac{\\mu}{\\sqrt{1 + \\pi/8 \\, \\diag \\varSigma}} \\right)$.\n\n$$\n\\qed\n$$\n\n## References\n\n1. Ng, Edward W., and Murray Geller. \"A table of integrals of the error functions.\" _Journal of Research of the National Bureau of Standards B 73_, no. 1 (1969): 1-20.\n2. Gibbs, Mark N. _Bayesian Gaussian processes for regression and classification_. Dissertation, University of Cambridge, 1998.\n3. Lu, Zhiyun, Eugene Ie, and Fei Sha. \"Mean-Field Approximation to Gaussian-Softmax Integral with Application to Uncertainty Estimation.\" _arXiv preprint arXiv:2006.07584_ (2020).","src/content/post/conv-probit.mdx","e8c479d95190f214","conv-probit.mdx","deploying-wagtail",{"id":173,"data":175,"body":182,"filePath":183,"digest":184,"legacyId":185,"deferredRender":23},{"title":176,"description":177,"publishDate":178,"draft":15,"tags":179},"Deploying Wagtail App","In this post, I'll show you how to deploy our blog and how to solve some common problems when deploying Wagtail app.",["Date","2015-06-23T01:39:00.000Z"],[43,42,180,181],"wagtail","web","Today we'll talk about deployment. Yay? We software developers often don't like to deal with deployment. Let the dev ops do that! Our job is to code, not dealing with the production server! Yes? Well, no. We, software developers, should be a T shaped person. Meaning that we should have a broad knowledge, but we are specialized in one or two skills. So, actually, being able to deploy our application on our own is a very good thing, even we might not the best person to do it.\n\nOk, let's get started. Oh, before that, I'll make some disclaimer. Because this is about my experience developing and deploying this blog using Wagtail, the tools and systems that I use in this post will be a reflection of what I used when I built this blog.\n\nFirst, we need to prepare our system. For this, I use cloud VPS. After investigated some of cloud providers out there, I chose DigitalOcean because it's so simple and so cheap. For the basic tier, you'll be charged just 5 bucks a month. Granted there are more powerful cloud provider out there, with Amazon AWS being the prime example. Unfortunately it doesn't have the simplicity DigitalOcean has. AWS's pricing is so convoluted and sophisticated I feel, and also it's directed to power user like devops guys.\n\nTo prepare the production environment, I'm strongly urge you to follow this guide: https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04.\n\nWhen everything (nginx, uwsgi) is ready, we can begin to really deploy our blog in the cloud. Here's the workflow that I use whenever I made some code change to this blog:\n\n1. Pull the latest code from git\n2. Change the virtualenv\n3. Install Python dependencies: `pip install -r requirements.txt`\n4. Migrate the DB: `./manage.py migrate`\n5. Run `./manage.py bower install`, because I use Bower to manage my Javascript & CSS dependencies.\n6. Collect the static file: `./manage.py collectstatic`\n7. Compress the static file, to minimize CSS & JS file size: `./manage.py compress`\n8. Restart the application server (uwsgi): `sudo service uwsgi restart`\n\nI think two thinks that need some explanation are number 5 and 7. Number 5 is needed because in production environment (whenever `DEBUG = False` in Django config), we have to serve the static file ourselves. Meaning that we need to put those files directly behind the web server, because it will be faster as serving static files via Django would be slower, which is not an ideal condition for production setting.\n\nFor number 7, we need to restart uwsgi because when uwsgi started, it will \"compile\" our Python files into .pyc files. And uwsgi will use those .pyc files. If we don't restart uwsgi, our changes in .py and .html files won't be served.\n\nWe already know that Django has multiple setting files: base.py, dev.py, and production.py. In our production environment we surely want to use production.py file to override our base.py. To make our life easier, we can put a script to automatically pick which setting file we have to use depending on what environment we are in. To do this, we'll edit our `__init__.py` inside the settings directory.\n\n```python\n# settings/**init**.py\nimport os\n\nENV = os.getenv('MYBLOG_ENV', 'dev')\n\nif ENV == 'dev':\n    from .dev import *\nelif ENV == 'prod':\n    from .production import *\n```\n\nFirst, we find an environment variable named 'MYBLOG_ENV' in our operating system, with 'dev' as the default value. If that environment variable value is 'dev', we use dev.py, otherwise we use production.py. As simple as that. To create the environment variable, there are two ways, first by add it directly to the system, or put it in uwsgi. I will go with the second option because, just like the use of virtualenv, it aligns with our spirit of software environment isolation. This this line to your uwsgi config file, and restart uwsgi to apply the change:\n\n```bash\nenv=MYBLOG_ENV=prod\n```\n\nNow, our Wagtail app in the production server will always use production.py, and whenever we open it in our development machine, it will use dev.py setting. Neat!\n\n## Common Problems\n\nI spent a lot of time trying to figure out how to deploy this blog. I'd say most of my time creating this blog is here, in the deployment stage, debugging things that didn't work. Here's the list of some problems that I encountered while trying to make this blog up and running:\n\n_Error 400:_ Check your setting, make sure you've added your domain/IP in `allowed_sites` field\n_Error 500:_ Don't forget to do all of the above steps! I encountered this problem because I didn't compress my static files\nCannot upload image: The root cause is you don't have libjpeg and libpng in your machine\n\n```bash\nsudo apt-get install libjpeg-dev libpng12-dev\npip uninstall pillow\npip install PIL --allow-external PIL --allow-unverified PIL\npip install pillow\n```\n\nImages, CSS, JS won't load: In production setting, we have to serve our static files ourselves. Probably you forgot to serve the \"static\" directory in the nginx. Check out your nginx sites-availables:\n\n```nginx\nlocation /static/ {\n    root /your/project/path;\n}\nlocation /media/ {\n    root /your/project/path;\n}\n```\n\nProbably also because the nginx doesn't have the permission to access the directories.\n\n```bash\nchmod 664 -R static\nchmod 664 -R media\nchown -R yourusername:www-data static\nchown -R yourusername:www-data media\n```\n\nAaaaaand that's it! Now your Wagtail site should be up and running nicely in the production server!\n\nObviously, this is my limited first time experience deploying Django app. If you see there's more room for improvement, please let me know! I'll be eager to improve it!","src/content/post/deploying-wagtail.mdx","8086e9422fefbdf6","deploying-wagtail.mdx","developing-wagtail",{"id":186,"data":188,"body":193,"filePath":194,"digest":195,"legacyId":196,"deferredRender":23},{"title":189,"description":190,"publishDate":191,"draft":15,"tags":192},"Developing Blog with Wagtail","My experience on building this blog using Wagtail CMS, with zero Django knowledge. Let’s code our blog!",["Date","2015-06-22T12:51:00.000Z"],[43,42,181],"import BlogImage from \"@/components/BlogImage.astro\";\n\nNow that we’ve set our Wagtail development environment up, we’re ready to actually write our code. Now, Wagtail is unlike Wordpress and such, where all you have to do is just use their admin panel, and everything will be set up for you. Wagtail is actually just give you foundation, and how you build your site/blog is entirely up to you. There are of course pros and cons. On one side, you have so much flexibility and customizability. On the other side, if you’re not familiar with Django or web development in general, the learning curve could be a bit steep. Nevertheless, actually I really like how Wagtail provide us developers bare minimum CMS that could be extended and customize as we like. Let’s get started!\n\nOh, before you started run the development server first!\n\n```bash\n./manage.py runserver\n```\n\nWagtail Page\nOne of Wagtail’s building block is the Page class. You’ll need to extend it to create your desired page. In this tutorial, we’ll create our home page, blog page, and a generic page for everything else.\n\nFirst, we’ll create our generic page class as home page class is already created by default when we started Wagtail project.\n\n```python\n# core/models.py\n\nfrom wagtail.wagtailcore.models import Page\nfrom wagtail.wagtailcore.fields import RichTextField\nfrom wagtail.wagtailadmin.edit_handlers import FieldPanel\nfrom wagtail.wagtailsearch import index\n\n# We’re deriving our GenericPage from Page class, so that our GenericPage also has Page’s field, e.g. title\n\nclass GenericPage(Page):\n    # Let’s create our custom field, named body which is a rich text\n    body = RichTextField()\n\n    # Index the body field, so that it will be searchable\n    search_fields = Page.search_fields + (index.SearchField(‘body'),)\n\n    # To show our body field in admin panel, we have to wrap it with FieldPanel and add it to Page’s field panel\n    content_panels = Page.content_panels + [FieldPanel('body', classname=‘full’)]\n```\n\nFor explanation about every fields and panels in Wagtail, I suggest you to read the Wagtail documentation.\n\nLet’s see the result in the admin panel. Fire up your browser, and go to http://localhost:8000/admin and login using the superuser account you already created before. You’ll see there are two fields there, title and body. Title is a field derived from the base class, Page. Body is a field that we created before.\n\n\u003CBlogImage imagePath='/img/developing-wagtail/00.jpg' />\n\n\u003CBlogImage imagePath='/img/developing-wagtail/01.jpg' />\n\nSuccess! We’ve created our first page! Try to put some content there, and then publish it. But wait… Why can’t we publish it? Turns out, our database don’t have the table to save the page content yet. So now our task is to create the table, which in Django, is really easy. You don’t have to get your hand dirty with SQL code, or migration code, Django does it for you. Django kinda reverse engineer your models, then creates the appropriate migration scripts. So let’s do that.\n\n```bash\n./manage.py makemigrations\n./manage.py migrate\n```\n\nTry again to publish our GenericPage content. It will actually work! But, we are one step short from having our content actually published. We need to create a template to show the page content. One page will need to have one template. So, our GenericPage will need generic_page.html template.\n\n```jinja\n# core/templates/generic_page.html\n\n{% extends \"base.html\" %}\n{% load static core_tags wagtailcore_tags wagtailimages_tags %}\n{% block body_class %}template-{{ self.get_verbose_name|slugify }}{% endblock %}\n\n{% block content %}\n\n\u003Carticle>\n\u003Ch3>{{ self.title }}\u003C/h3>\n{{ self.body|richtext }}\n\u003C/article>\n{% endblock %}\n```\n\nWe first define our template to extend base.html, which means that every stylesheet, script, that is included in the base.html will be available to your `generic_page.html`. Then we load some tag to be used in our HTML, for example richtext tag from core_tags module. Our content will be rendered inside the `{% raw %}{% block content %}{% endraw %}`. Everything inside that block will be inserted into content block in the base.html. The pattern is: put every static things into base.html, and put dynamic things inside page template. The examples of static element are header, footer, and general style.\n\nWe can access our model’s field, variable, or function in our template by enclosing them using double curly bracket. Every field could also be passed to a function called filter.\n\n```jinja\n# Pass the value of body field into richtext filter\n{{ self.body|richtext }}\n```\n\nNow, if you try to open your page again, it will actually rendered in your browser.\n\n\u003CBlogImage imagePath='/img/developing-wagtail/02.jpg' />\n\nSo to sum up on how to create a page:\n\n1. Create a class extending Page\n2. Make and run the migration\n3. Write the template\n\n## Creating Blog Page\n\nNow that we understand the know-how of creating a page in Wagtail, we will create our blog page. The basics are just the same. However, the page’s fields will be more complex to accommodate our need.\n\nWe want to create a separate app for our blog. The rationale, our project is a website, and a website could consist of some app, for example landing page, blog, etc. So to start thing off, we’ll create our blog app.\n\n```bash\n./manage.py startapp blog\n\n```\n\nThis will create a blog folder in our project, with its respective models and migrations. We will create our BlogPage class inside it.\n\n```python\n# blog/models.py\nfrom django.db import models\n\nfrom wagtail.wagtailcore import blocks\nfrom wagtail.wagtailcore.models import Page\nfrom wagtail.wagtailcore.fields import StreamField\nfrom wagtail.wagtailadmin.edit_handlers import FieldPanel, StreamFieldPanel\nfrom wagtail.wagtailimages.edit_handlers import ImageChooserPanel\nfrom wagtail.wagtailsearch import index\n\nclass BlogPage(Page):\n    intro = models.CharField(max_length=250)\n    main_image = models.ForeignKey(\n        'wagtailimages.Image',\n        null=True,\n        blank=True,\n        on_delete=models.SET_NULL,\n        related_name='+'\n    )\n    body = StreamField([\n        ('rich_text', blocks.RichTextBlock(icon='doc-full', label='Rich Text')),\n        ('html', blocks.RawHTMLBlock(icon='site', label='HTML'))\n    ])\n\n    search_fields = Page.search_fields + (\n        index.SearchField('intro'),\n        index.SearchField('body')\n    )\n\n    content_panels = Page.content_panels + [\n        ImageChooserPanel('main_image'),\n        FieldPanel('intro'),\n        StreamFieldPanel('body')\n    ]\n```\n\nI will introduce you to Wagtail unique feature: StreamField. StreamField is a page component that enables you to build your own page structure, just like LEGO! When we’re creating a StreamField, we will need to specify, which blocks (think about LEGO block) are available for the editor to use. In the example above, we specify that our StreamField could be built with RichText or RawHTML. The we could use arbitrary number of blocks in building our StreamField. Also the structure is free, as long as we only use the blocks that we’ve specified in our model. For more about StreamField, be sure to check Wagtail’s documentation.\n\nFor the main image of the blog post, we could make a reference to Wagtail’s image table. Hence, we specified that our main_image is a foreign key to wagtailimages.Image model.\n\nNow, as always, make the migration script, then migrate it to our DB. After that, let’s create our template.\n\n```jinja\n{% extends \"base.html\" %}\n{% load static core_tags wagtailcore_tags wagtailimages_tags %}\n{% block body_class %}template-{{ self.get_verbose_name|slugify }}{% endblock %}\n\n{% block content %}\n\n\u003Carticle id=\"blog-post\">\n\u003Ch3>{{ self.title }}\u003C/h3>\n\u003Cp class=\"post-meta\">By {{ self.owner }} at {{ self.first_published_at }}\u003C/p>\n{{ self.body }}\n\u003C/article>\n{% endblock %}\n```\n\nThere, it’s super simple to use. It’s very similar with generic_page template. However, when we use StreamField, we don’t have to apply the value to richtext filter. If you want to have greater control over the StreamField, however, you could iterate over it to get each block, like this.\n\n```jinja\n{% for block in self.body %}\n{{ block }}\n{% endfor %}\n```\n\nTo see our blog page model, let’s fire up the Wagtail admin interface.\n\n\u003CBlogImage imagePath='/img/developing-wagtail/03.jpg' />\n\nCheck out the body part. We’ll be given two building block that we’ve specified before. We could add RichText for the first blog, then another RichText for the second blog, or anything we want!\n\n## Extending StreamField Block\n\nThe best part about using StreamField is that we could create our own block by extending the existing block. Because I wanted to make a programming blog, and Wagtail doesn’t provide code or quote block, which are very important in that kind of blog, I’ve to create it myself. It’s relatively easy and painless though!\n\n```python\n# blog/models.py\nclass CodeBlock(blocks.TextBlock):\n    class Meta:\n        template = 'blocks/code.html'\n        icon = 'code'\n        label = 'Code'\n\nclass QuoteBlock(blocks.TextBlock):\n    class Meta:\n        template = 'blocks/quote.html'\n        icon = 'openquote'\n        label = ‘Quote'\n\n        body = StreamField([\n            ('rich_text', blocks.RichTextBlock(icon='doc-full', label='Rich Text')),\n            ('code', CodeBlock(icon='code')),\n            ('quote', QuoteBlock(icon='openquote')),\n            ('html', blocks.RawHTMLBlock(icon='site', label='HTML'))\n        ])\n```\n\nHere, we specified two new blocks, CodeBlock and QuoteBlock. They’re extending Wagtail own TextBlock, because, code and quote are just a text really, with different markup in HTML. Code is enclosed with `\u003Cpre>` and `\u003Ccode>` tags, and quote is enclosed by `\u003Cblockquote>` tag. So, all that matter now is just how we define our block template. Yes, block, like page need template. Because, template is basically is how your model looks like. So let’s make it.\n\n```jinja\n# blog/code.html\n\u003Cpre>\n    \u003Ccode>\n        {{ self }}\n    \u003C/code>\n\u003C/pre>\n\n# blog/quote.html\n\u003Cblockquote>\n    {{ self }}\n\u003C/blockquote>\n```\n\nThere you have it! Really easy right? After you finished creating the custom blocks, we now can use it in our StreamField. Just like any other block, we just add it to list parameter of the StreamField.\n\nNow, in your blog page admin panel, you’ll see code and quote blocks there, which you can use.\n\n\u003CBlogImage imagePath='/img/developing-wagtail/04.jpg' />\n\nSo, there you go, a blog page ready to use! What left to do is to style our blog. But, I will leave it to you and your creative mind! See you later!","src/content/post/developing-wagtail.mdx","00f9785bc7562c00","developing-wagtail.mdx","dropout",{"id":197,"data":199,"body":204,"filePath":205,"digest":206,"legacyId":207,"deferredRender":23},{"title":200,"description":201,"publishDate":202,"draft":15,"tags":203},"Implementing Dropout in Neural Net","Dropout is one simple way to regularize a neural net model. This is one of the recent advancements in Deep Learning that makes training deeper and deeper neural net tractable.",["Date","2016-06-25T14:00:00.000Z"],[17,42,43,44],"import BlogImage from \"@/components/BlogImage.astro\";\n\nDropout is one of the recent advancement in Deep Learning that enables us to train deeper and deeper network. Essentially, Dropout act as a regularization, and what it does is to make the network less prone to overfitting.\n\nAs we already know, the deeper the network is, the more parameter it has. For example, VGGNet from ImageNet competition 2014, has some 148 million parameters. That's a lot. With that many parameters, the network could easily overfit, especially with small dataset.\n\nEnter Dropout.\n\nIn training phase, with Dropout, at each hidden layer, with probability `p`, we kill the neuron. What it means by 'kill' is to set the neuron to 0. As neural net is a collection multiplicative operations, then those 0 neuron won't propagate anything to the rest of the network.\n\n\u003CBlogImage imagePath='/img/dropout/00.png' fullWidth />\n\nLet `n` be the number of neuron in a hidden layer, then the expectation of the number of neuron to be active at each Dropout is `p*n`, as we sample the neurons uniformly with probability `p`. Concretely, if we have 1024 neurons in hidden layer, if we set `p = 0.5`, then we can expect that only half of the neurons (512) would be active at each given time.\n\nBecause we force the network to train with only random `p*n` of neurons, then intuitively, we force it to learn the data with different kind of neurons subset. The only way the network could perform the best is to adapt to that constraint, and learn the more general representation of the data.\n\nIt's easy to remember things when the network has a lot of parameters (overfit), but it's hard to remember things when effectively the network only has so many parameters to work with. Hence, the network must learn to generalize more to get the same performance as remembering things.\n\nSo, that's why Dropout will increase the test time performance: it improves generalization and reduce the risk of overfitting.\n\nLet's see the concrete code for Dropout:\n\n```python\n# Dropout training\nu1 = np.random.binomial(1, p, size=h1.shape)\nh1 *= u1\n```\n\nFirst, we sample an array of independent Bernoulli Distribution, which is just a collection of zero or one to indicate whether we kill the neuron or not. For example, the value of `u1` would be `np.array([1, 0, 0, 1, 1, 0, 1, 0])`. Then, if we multiply our hidden layer with this array, what we get is the originial value of the neuron if the array element is 1, and 0 if the array element is also 0.\n\nFor example, after Dropout, we need to do `h2 = np.dot(h1, W2)`, which is a multiplication operation. What is zero times x? It's zero. Then the subsequent multiplications would be also zero. That's why those 0 neurons won't contribute anything to the rest of the propagation.\n\nNow, because we're only using `p*n` of the neurons, the output then has the expectation of `p*x`, if `x` is the expected output if we use all the neurons (without Dropout).\n\nAs we don't use Dropout in test time, then the expected output of the layer is `x`. That doesn't match with the training phase. What we need to do is to make it matches the training phase expectation, so we scale the layer output with `p`.\n\n```python\n# Test time forward pass\nh1 = X_train @ W1 + b1\nh1[h1 \u003C 0] = 0\n\n# Scale the hidden layer with p\nh1 *= p\n```\n\nIn practice, it's better to simplify things. It's cumbersome to maintain codes in two places. So, we move that scaling into the Dropout training itself.\n\n```python\n# Dropout training, notice the scaling of 1/p\nu1 = np.random.binomial(1, p, size=h1.shape) / p\nh1 *= u1\n```\n\nWith that code, we essentially make the expectation of layer output to be `x` instead of `px`, because we scale it back with `1/p`. Hence in the test time, we don't need to do anything as the expected output of the layer is the same.\n\n## Dropout backprop\n\nDuring the backprop, what we need to do is just to consider the Dropout. The killed neurons don't contribute anything to the network, so we won't flow the gradient through them.\n\n```python\ndh1 *= u1\n```\n\nFor full example, please refer to: https://github.com/wiseodd/hipsternet/blob/master/hipsternet/neuralnet.py.\n\n## Test and Comparison\n\nTest time! But first, let's declare what kind of network we will use for testing.\n\n```python\ndef make_network(D, C, H=100):\n    model = dict(\n        W1=np.random.randn(D, H) / np.sqrt(D / 2.),\n        W2=np.random.randn(H, H) / np.sqrt(H / 2.),\n        W3=np.random.randn(H, C) / np.sqrt(H / 2.),\n        b1=np.zeros((1, H)),\n        b2=np.zeros((1, H)),\n        b3=np.zeros((1, C))\n    )\n\n    return model\n\nmodel = nn.make_network(D, C, H=256)\n```\n\nWe're using three layers network with 256 neurons in each hidden layer. The weights are initialized using Xavier divided by 2, as proposed by He, et al, 2015. The data used are MNIST data with 55000 training data and 10000 test data. The optimization algorithm used is RMSprop with 1000 iterations, repeated 5 times and the test accuracy is averaged.\n\n```\n# Without Dropout\nrmsprop => mean accuracy: 0.9640, std: 0.0071\n\n# With Dropout\nrmsprop => mean accuracy: 0.9692, std: 0.0006\n```\n\nLooking at the result, model which use Dropout yield a better accuracy across the test set. The difference of 0.005 might be negligible, but considering we have 10000 test data, that's quite a bit.\n\nThe standard deviation of the test tells different story though. It seems that the network that uses Dropout for training perform consistently better during test time. Compare it to the non-Dropout network, it's an order of magnitude worse in term of consistency. We can see this when comparing the standard deviation: 0.0006 vs 0.0071.\n\nHowever, when we look at the convergence of the network during training, it seems that non-Dropout network converge better and faster. Here, we could see at the loss at every 100 iterations.\n\n```\n# Without Dropout\nIter-100 loss: 0.7141623845363005\nIter-200 loss: 0.5242217596766273\nIter-300 loss: 0.37112553379849605\nIter-400 loss: 0.38909851968506987\nIter-500 loss: 0.25597567938296006\nIter-600 loss: 0.30120887447912315\nIter-700 loss: 0.24511170871806906\nIter-800 loss: 0.23164132479234184\nIter-900 loss: 0.18410249409092522\nIter-1000 loss: 0.21936521877677104\n\n# With Dropout\nIter-100 loss: 0.8993988029028332\nIter-200 loss: 0.761899148472519\nIter-300 loss: 0.6472785867227253\nIter-400 loss: 0.4277826704557144\nIter-500 loss: 0.48772494633262575\nIter-600 loss: 0.35737694600178316\nIter-700 loss: 0.3650990861796465\nIter-800 loss: 0.30701377662168766\nIter-900 loss: 0.2754936912501326\nIter-1000 loss: 0.3182353552441539\n```\n\nThis indicates that network without Dropout performs better at the training phase while Dropout network perform worse. The table is turned at the test time, Dropout network is not just perform better, but _consistenly better_. One could interpret this as the sign of overfitting. So, really, we could see that Dropout regularize our network and make it more robust to overfitting.\n\n## Conclusion\n\nWe look at one of the driving force of the recent advancement of Deep Learning: Dropout. It's a relatively new technique but already made a very big impact in the field. Dropout act as regularizer by stochastically kill neurons in hidden layers. This in turn force the network to generalize more.\n\nWe also implement Dropout in our model. Implementing Dropout in our neural net model is just a matter of several lines of code. We found that it's a very simple method to implement.\n\nWe then compare the Dropout network with non Dropout network. The result is nice: Dropout network performs consistenly better in test time compared to the non Dropout Network.\n\nTo see more about, check my full example in my Github page: https://github.com/wiseodd/hipsternet.\n\n## References\n\n- http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n- http://cs231n.github.io/neural-networks-2/#reg","src/content/post/dropout.mdx","445dbec266cfa8b1","dropout.mdx","forward-reverse-kl",{"id":208,"data":210,"body":216,"filePath":217,"digest":218,"legacyId":219,"deferredRender":23},{"title":211,"description":212,"publishDate":213,"draft":15,"tags":214},"KL Divergence: Forward vs Reverse?","KL Divergence is a measure of how different two probability distributions are. It is a non-symmetric distance function, and each arrangement has its own interesting property, especially when we use it in optimization settings e.g. Variational Bayes method.",["Date","2016-12-21T05:30:00.000Z"],[42,43,215],"bayes","import BlogImage from \"@/components/BlogImage.astro\";\n\nKullback-Leibler Divergence, or KL Divergence is a measure on how \"off\" two probability distributions $P(X)$ and $Q(X)$ are. It measures the distance between two probability distributions.\n\nFor example, if we have two gaussians, $P(X) = N(0, 2)$ and $Q(X) = N(0, 3)$, how different are those two gaussians?\n\n\u003CBlogImage\n  imagePath='/img/forward-reverse-kl/two_gaussians.png'\n  altText='Two Gaussians.'\n/>\n\nThe KL Divergence could be computed as follows:\n\n$$ D*{KL}[P(X) \\, \\Vert \\, Q(X)] = \\sum*{x \\in X} P(x) \\, \\log \\left( \\frac{P(x)}{Q(x)} \\right) $$\n\nthat is, for all random variable $x \\in X$, KL Divergence calculates the weighted average on the difference between those distributions at $x$.\n\n## KL Divergence in optimization\n\nIn optimization setting, we assume that $P(X)$ as the true distribution we want to approximate and $Q(X)$ as the approximate distribution.\n\nJust like any other distance functions (e.g. euclidean distance), we can use KL Divergence as a loss function in an optimization setting, especially in a probabilistic setting. For example, in Variational Bayes, we are trying to fit an approximate to the true posterior, and the process to make sure that $Q(X)$ fits $P(X)$ is to minimize the KL Divergence between them.\n\nHowever, we have to note this important property about KL Divergence: it is not symmetric. Formally, $D*{KL}[P(X) \\, \\Vert \\, Q(X)] \\neq D*{KL}[Q(X) \\, \\Vert \\, P(X)]$.\n\n$D*{KL}[P(X) \\, \\Vert \\, Q(X)]$ is called forward KL, whereas $D*{KL}[Q(X) \\, \\Vert \\, P(X)]$ is called reverse KL.\n\n## Forward KL\n\nIn forward KL, the difference between $P(x)$ and $Q(x)$ is weighted by $P(x)$. Now let's ponder on that statement for a while.\n\nConsider $P(x) = 0$ for a particular $x$. What does that mean? As $P(x)$ is the weight, then it doesn't really matter what's the value of the other term. In other words, if $P(x) = 0$, there is no consequence at all to have very big difference between $P(x)$ and $Q(x)$. In this case, the total KL Divergence will not be affected when $P(x) = 0$, as the minimum value for KL Divergence is $0$ (no distance at all, i.e. exact match). During the optimization process then, whenever $P(x) = 0$, $Q(x)$ would be ignored.\n\nReversely, if $P(x) > 0$, then the $\\log \\left( \\frac{P(x)}{Q(x)} \\right)$ term will contribute to the overall KL Divergence. This is not good if our objective is to minimize KL Divergence. Hence, during the optimization, the difference between $P(x)$ and $Q(x)$ will be minimized if $P(x) > 0$.\n\nLet's see some visual examples.\n\n\u003CBlogImage\n  imagePath='/img/forward-reverse-kl/forward_kl_bad.png'\n  altText='Bad forward KL.'\n/>\n\nIn the example above, the right hand side mode is not covered by $Q(x)$, but it is obviously the case that $P(x) > 0$! The consequence for this scenario is that the KL Divergence would be big. The optimization algorithm then would force $Q(x)$ to take different form:\n\n\u003CBlogImage\n  imagePath='/img/forward-reverse-kl/forward_kl_good.png'\n  altText='Good forward KL.'\n/>\n\nIn the above example, $Q(x)$ is now more spread out, covering all $P(x) > 0$. Now, there is no $P(x) > 0$ that are not covered by $Q(x)$.\n\nAlthough there are still some area that are wrongly covered by $Q(x)$, this is the desired optimization result as in this form of $Q(x)$, the KL Divergence is low.\n\nThose are the reason why, Forward KL is known as _zero avoiding_, as it is avoiding $Q(x) = 0$ whenever $P(x) > 0$.\n\n## Reverse KL\n\nIn Reverse KL, as we switch the two distributions' position in the equation, now $Q(x)$ is the weight. Still keeping that $Q(x)$ is the approximate and $P(x)$ is the true distribution, let's ponder some scenarios.\n\nFirst, what happen if $Q(x) = 0$ for some $x$, in term of the optimization process? In this case, there is no penalty when we ignore $P(x) > 0$.\n\nSecond, what happen if $Q(x) > 0$? Now the difference between $P(x)$ and $Q(x)$ must be as low as possible, as it now contribute to the overall divergence.\n\nTherefore, the failure case example above for Forward KL, is the desireable outcome for Reverse KL. That is, for Reverse KL, it is better to fit just some portion of $P(X)$, as long as that approximate is good.\n\nConsequently, Reverse KL will try avoid spreading the approximate. Now, there would be some portion of $P(X)$ that will not be approximated by $Q(X)$, i.e. $Q(x) = 0$.\n\nAs those properties suggest, this form of KL Divergence is know as _zero forcing_, as it forces $Q(X)$ to be $0$ on some areas, even if $P(X) > 0$.\n\n## Conclusion\n\nSo, what's the best KL?\n\nAs always, the answer is \"it depends\". As we have seen above, both has its own characteristic. So, depending on what we want to do, we choose which KL Divergence mode that's suitable for our problem.\n\nIn Bayesian Inference, esp. in Variational Bayes, Reverse KL is widely used. As we could see at the derivation of Variational Autoencoder, VAE also uses Reverse KL (as the idea is rooted in Variational Bayes!).\n\n## References\n\n1. Blei, David M. \"Variational Inference.\" Lecture from Princeton, https://www. cs. princeton. edu/courses/archive/fall11/cos597C/lectures/variational-inference-i. pdf (2011).\n2. Fox, Charles W., and Stephen J. Roberts. \"A tutorial on variational Bayesian inference.\" Artificial intelligence review 38.2 (2012): 85-95.","src/content/post/forward-reverse-kl.mdx","5ca5b70a8a4a162f","forward-reverse-kl.mdx","fisher-information",{"id":220,"data":222,"body":227,"filePath":228,"digest":229,"legacyId":230,"deferredRender":23},{"title":223,"description":224,"publishDate":225,"draft":15,"tags":226},"Fisher Information Matrix","An introduction and intuition of Fisher Information Matrix.",["Date","2018-03-11T11:00:00.000Z"],[17],"Suppose we have a model parameterized by parameter vector $ \\theta $ that models a distribution $ p(x \\vert \\theta) $. In frequentist statistics, the way we learn $ \\theta $ is to maximize the likelihood $ p(x \\vert \\theta) $ wrt. parameter $ \\theta $. To assess the goodness of our estimate of $ \\theta $ we define a score function:\n\n$$\ns(\\theta) = \\nabla_{\\theta} \\log p(x \\vert \\theta) \\, ,\n$$\n\nthat is, score function is the gradient of log likelihood function. The result about score function below is important building block on our discussion.\n\n**Claim:**\nThe expected value of score wrt. our model is zero.\n\n_Proof._ &nbsp;&nbsp; Below, the gradient is wrt. $ \\theta $.\n\n$$\n\\begin{align}\n    \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} \\left[ s(\\theta) \\right] &= \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} \\left[ \\nabla \\log p(x \\vert \\theta) \\right] \\\\[5pt]\n    &= \\int \\nabla \\log p(x \\vert \\theta) \\, p(x \\vert \\theta) \\, \\text{d}x \\\\[5pt]\n    &= \\int \\frac{\\nabla p(x \\vert \\theta)}{p(x \\vert \\theta)} p(x \\vert \\theta) \\, \\text{d}x \\\\[5pt]\n    &= \\int \\nabla p(x \\vert \\theta) \\, \\text{d}x \\\\[5pt]\n    &= \\nabla \\int p(x \\vert \\theta) \\, \\text{d}x \\\\[5pt]\n    &= \\nabla 1 \\\\[5pt]\n    &= 0\n\\end{align}\n$$\n\n$$\n\\qed\n$$\n\nBut how certain are we to our estimate? We can define an uncertainty measure around the expected estimate. That is, we look at the covariance of score of our model. Taking the result from above:\n\n$$\n    \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} \\left[ (s(\\theta) - 0) \\, (s(\\theta) - 0)^{\\text{T}} \\right] \\, .\n$$\n\nWe can then see it as an information. The covariance of score function above is the definition of Fisher Information. As we assume $ \\theta $ is a vector, the Fisher Information is in a matrix form, called Fisher Information Matrix:\n\n$$\n    \\text{F} = \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} \\left[ \\nabla \\log p(x \\vert \\theta) \\, \\nabla \\log p(x \\vert \\theta)^{\\text{T}} \\right] \\, .\n$$\n\nHowever, usually our likelihood function is complicated and computing the expectation is intractable. We can approximate the expectation in $ \\text{F} $ using empirical distribution $ \\hat{q}(x) $, which is given by our training data $ X = \\{ x_1, x_2, \\cdots, x_N \\} $. In this form, $ \\text{F} $ is called Empirical Fisher:\n\n$$\n\\begin{align}\n    \\text{F} &= \\frac{1}{N} \\sum_{i=1}^{N} \\nabla \\log p(x_i \\vert \\theta) \\, \\nabla \\log p(x_i \\vert \\theta)^{\\text{T}} \\, .\n\\end{align}\n$$\n\n## Fisher and Hessian\n\nOne property of $ \\text{F} $ that is not obvious is that it has the interpretation of being the negative expected Hessian of our model's log likelihood.\n\n**Claim:**\nThe negative expected Hessian of log likelihood is equal to the Fisher Information Matrix $ \\text{F} $.\n\n_Proof._ &nbsp;&nbsp; The Hessian of the log likelihood is given by the Jacobian of its gradient:\n\n$$\n\\begin{align}\n    \\text{H}_{\\log p(x \\vert \\theta)} &= \\text{J} \\left( \\frac{\\nabla p(x \\vert \\theta)}{p(x \\vert \\theta)} \\right) \\\\[5pt]\n    &= \\frac{ \\text{H}_{p(x \\vert \\theta)} \\, p(x \\vert \\theta) - \\nabla p(x \\vert \\theta) \\, \\nabla p(x \\vert \\theta)^{\\text{T}}}{p(x \\vert \\theta) \\, p(x \\vert \\theta)} \\\\[5pt]\n    &= \\frac{\\text{H}_{p(x \\vert \\theta)} \\, p(x \\vert \\theta)}{p(x \\vert \\theta) \\, p(x \\vert \\theta)} - \\frac{\\nabla p(x \\vert \\theta) \\, \\nabla p(x \\vert \\theta)^{\\text{T}}}{p(x \\vert \\theta) \\, p(x \\vert \\theta)} \\\\[5pt]\n    &= \\frac{\\text{H}_{p(x \\vert \\theta)}}{p(x \\vert \\theta)} - \\left( \\frac{\\nabla p(x \\vert \\theta)}{p(x \\vert \\theta)} \\right) \\left( \\frac{\\nabla p(x \\vert \\theta)}{p(x \\vert \\theta)}\\right)^{\\text{T}} \\, ,\n\\end{align}\n$$\n\nwhere the second line is a result of applying quotient rule of derivative. Taking expectation wrt. our model, we have:\n\n$$\n\\begin{align}\n    \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} \\left[ \\text{H}_{\\log p(x \\vert \\theta)} \\right] &= \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} \\left[ \\frac{\\text{H}_{p(x \\vert \\theta)}}{p(x \\vert \\theta)} - \\left( \\frac{\\nabla p(x \\vert \\theta)}{p(x \\vert \\theta)} \\right) \\left( \\frac{\\nabla p(x \\vert \\theta)}{p(x \\vert \\theta)} \\right)^{\\text{T}} \\right] \\\\[5pt]\n    &= \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} \\left[ \\frac{\\text{H}_{p(x \\vert \\theta)}}{p(x \\vert \\theta)} \\right] - \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} \\left[ \\left( \\frac{\\nabla p(x \\vert \\theta)}{p(x \\vert \\theta)} \\right) \\left( \\frac{\\nabla p(x \\vert \\theta)}{p(x \\vert \\theta)}\\right)^{\\text{T}} \\right] \\\\[5pt]\n    &= \\int \\frac{\\text{H}_{p(x \\vert \\theta)}}{p(x \\vert \\theta)} p(x \\vert \\theta) \\, \\text{d}x \\, - \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} \\left[ \\nabla \\log p(x \\vert \\theta) \\, \\nabla \\log p(x \\vert \\theta)^{\\text{T}} \\right] \\\\[5pt]\n    &= \\text{H}_{\\int p(x \\vert \\theta) \\, \\text{d}x} \\, - \\text{F} \\\\[5pt]\n    &= \\text{H}_{1} - \\text{F} \\\\[5pt]\n    &= -\\text{F} \\, .\n\\end{align}\n$$\n\nThus we have $ \\text{F} = -\\mathop{\\mathbb{E}}\\_{p(x \\vert \\theta)} \\left[ \\text{H}\\_{\\log p(x \\vert \\theta)} \\right] $.\n\n$$\n\\qed\n$$\n\nIndeed knowing this result, we can see the role of $ \\text{F} $ as a measure of curvature of the log likelihood function.\n\n## Conclusion\n\nFisher Information Matrix is defined as the covariance of score function. It is a curvature matrix and has interpretation as the negative expected Hessian of log likelihood function. Thus the immediate application of $ \\text{F} $ is as drop-in replacement of $ \\text{H} $ in second order optimization methods.\n\nOne of the most exciting results of $ \\text{F} $ is that it has connection to KL-divergence. This gives rise to natural gradient method, which we shall discuss further in the next article.\n\n## References\n\n1. Martens, James. \"New insights and perspectives on the natural gradient method.\" arXiv preprint arXiv:1412.1193 (2014).\n2. Ly, Alexander, et al. \"A tutorial on Fisher information.\" Journal of Mathematical Psychology 80 (2017): 40-55.","src/content/post/fisher-information.mdx","f54c59b9b21d4218","fisher-information.mdx","gan-pytorch",{"id":231,"data":233,"body":239,"filePath":240,"digest":241,"legacyId":242,"deferredRender":23},{"title":234,"description":235,"publishDate":236,"draft":15,"tags":237},"Generative Adversarial Networks (GAN) in Pytorch","Pytorch is a new Python Deep Learning library, derived from Torch. Contrary to Theano's and TensorFlow's symbolic operations, Pytorch uses imperative programming style, which makes its implementation more \"Numpy-like\".",["Date","2017-01-20T09:00:00.000Z"],[17,238],"pytorch","This week is a really interesting week in the Deep Learning library front. There are two new Deep Learning libraries being open sourced: Pytorch and Minpy.\n\nThose two libraries are different from the existing libraries like TensorFlow and Theano in the sense of how we do the computation. In TensorFlow and Theano, we have to symbolically construct our computational graph first before running it. In a sense, it is like writing a whole program before running it. Hence, the degree of freedom that we have in those libraries are limited. For example, doing loop, one need to use `tf.while_loop()` function in TensorFlow or `scan()` in Theano. Those approaches are less intuitive compared to imperative programming.\n\nEnter Pytorch. It is a Torch's port for Python. The programming style of Pytorch is imperative, meaning that if we've already familiar using Numpy to code our alogrithm up, then jumping to Pytorch should be a breeze. One does not need to learn symbolic mathematical computation, like in TensorFlow and Theano.\n\nWith that being said, let's try Pytorch by implementing Generative Adversarial Networks (GAN).\n\nLet's start by importing stuffs:\n\n```python\nimport torch\nimport torch.nn.functional as nn\nimport torch.autograd as autograd\nimport torch.optim as optim\nimport numpy as np\nfrom torch.autograd import Variable\n\nmnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\nmb_size = 64\nZ_dim = 100\nX_dim = mnist.train.images.shape[1]\ny_dim = mnist.train.labels.shape[1]\nh_dim = 128\nlr = 1e-3\n```\n\nNow let's construct our Generative Network $G(z)$:\n\n```python\ndef xavier*init(size):\n    in_dim = size[0]\n    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n    return Variable(torch.randn(\\_size) * xavier_stddev, requires_grad=True)\n\nWzh = xavier_init(size=[Z_dim, h_dim])\nbzh = Variable(torch.zeros(h_dim), requires_grad=True)\n\nWhx = xavier_init(size=[h_dim, X_dim])\nbhx = Variable(torch.zeros(X_dim), requires_grad=True)\n\ndef G(z):\n    h = nn.relu(z @ Wzh + bzh.repeat(z.size(0), 1))\n    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n    return X\n```\n\nIt is awfully similar to the TensorFlow version, what is the difference then? It is subtle without more hints, but basically those variables `Wzh, bzh, Whx, bhx` are real tensor/ndarray, just like in Numpy. That means, if we evaluate it with `print(Wzh)` the value is immediately shown. Also, the function `G(z)` is a real function, in the sense that if we input a tensor, we will immediately get the return value back. Try doing those things in TensorFlow or Theano.\n\nNext is the Discriminator Network $D(X)$:\n\n```python\nWxh = xavier_init(size=[X_dim, h_dim])\nbxh = Variable(torch.zeros(h_dim), requires_grad=True)\n\nWhy = xavier_init(size=[h_dim, 1])\nbhy = Variable(torch.zeros(1), requires_grad=True)\n\ndef D(X):\nh = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\ny = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\nreturn y\n```\n\nAttentive readers will notice that unlike in TensorFlow or Numpy implementation, adding bias to the equation is non-trivial in Pytorch. It is a workaround since Pytorch has not implemented Numpy-like broadcasting mechanism yet. If we do not use this workaround, the `X @ W + b` will fail because while `X @ W` is `mb_size x h` dimensional tensor, `b` is only `1 x b` vector!\n\nNow let's define the optimization procedure:\n\n```python\nG_params = [Wzh, bzh, Whx, bhx]\nD_params = [Wxh, bxh, Why, bhy]\nparams = G_params + D_params\n\nG_solver = optim.Adam(G_params, lr=1e-3)\nD_solver = optim.Adam(D_params, lr=1e-3)\n```\n\nWhile at this point, in TensorFlow we just need to run the graph with `G_solver` and `D_solver` as the entry points, in Pytorch we need to tell the program what to do with those instances. So, just like in Numpy, we run the \"forward-loss-backward-update\" loop:\n\n```python\nfor it in range(100000): # Sample data\nz = Variable(torch.randn(mb*size, Z_dim))\nX, * = mnist.train.next_batch(mb_size)\nX = Variable(torch.from_numpy(X))\n\n    # Dicriminator forward-loss-backward-update\n    ## some codes\n\n    # Generator forward-loss-backward-update\n    ## some codes\n\n```\n\nSo first, let's define the $D(X)$'s \"forward-loss-backward-update\" step. First, the forward step:\n\n```python # D(X) forward and loss\nG_sample = G(z)\nD_real = D(X)\nD_fake = D(G_sample)\n\n    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n    D_loss = D_loss_real + D_loss_fake\n\n```\n\nNothing fancy, it is just a Numpy-like operations. Next, the backward and update step:\n\n```python\nD_loss.backward()\nD_solver.step()\n```\n\nThat is it! Notice, when we were constructing all the `W`s and `b`s, we wrapped them with `Variable(..., requires_grad=True)`. That wrapping is basically telling Pytorch that we cares about the gradient of those variables, and consequently `pytorch.autograd` module will calculate their gradients automatically, starting from `D_loss`. We could inspect those gradients by inspecting `grad` instance of the variables, e.g. `Wxh.grad`.\n\nOf course we could code up our own optimizer. But Pytorch has built-in optimizers ready in `pytorch.optim` module. What it does is to abstract the update process and at each iteration, we just need to call `D_solver.step()` to update our variables, now that `grad` instance in those variables has been computed by `backward()` function.\n\nAs we have two different optimizers, we need to clear up the computed gradient in our computational graph as we do not need it anymore. Also, it is necessary so that the gradients won't mix up with the subsequent call of `backward()` as `D_solver` shares some subgraphs with `G_solver`.\n\n```python\ndef reset_grad():\n    for p in params:\n    p.grad.data.zero_()\n```\n\nWe do similar things to implement the \"forward-loss-backward-update\" for $G(z)$:\n\n```python\n# Housekeeping - reset gradient\nreset_grad()\n\n# Generator forward-loss-backward-update\nz = Variable(torch.randn(mb_size, Z_dim))\nG_sample = G(z)\nD_fake = D(G_sample)\n\nG_loss = nn.binary_cross_entropy(D_fake, ones_label)\n\nG_loss.backward()\nG_solver.step()\n\n# Housekeeping - reset gradient\nreset_grad()\n```\n\nAnd that is it, really.\n\nBut we might ask, why do all of those things matter? Why not to just use TensorFlow or Theano? The answer is when we want to inspect or debug inside the computation graph, thing could be hairy in symbolic computation. Think of it like this: we are given a compiled program and what we can do is to run it. How do we debug a specific suboperation inside that program? Granted in TensorFlow we could inspect any variable by returning it once the computation is done, but still, we could only inspect it at the end of the computation not before.\n\nIn contrast, in imperative computation, we could just use `print()` function basically anywhere and anytime we want and immediately it will display the value. Doing other \"non-trivial\" operations like loop and conditional are also become much more easier in Pytorch, just like the good old Python. Hence, one could argue that this way of programming is more \"natural\".\n\nThe full code is available in my Github repo: https://github.com/wiseodd/generative-models.","src/content/post/gan-pytorch.mdx","41e4978042a5960e","gan-pytorch.mdx","gan-tensorflow",{"id":243,"data":245,"body":250,"filePath":251,"digest":252,"legacyId":253,"deferredRender":23},{"title":246,"description":247,"publishDate":248,"draft":15,"tags":249},"Generative Adversarial Nets in TensorFlow","Let's try to implement Generative Adversarial Nets (GAN), first introduced by Goodfellow et al, 2014, with TensorFlow. We'll use MNIST data to train the GAN!",["Date","2016-09-17T15:22:00.000Z"],[17,42,43,44,56],"import BlogImage from \"@/components/BlogImage.astro\";\n\nGenerative Adversarial Nets, or GAN in short, is a quite popular neural net. It was first introduced in a [NIPS 2014 paper by Ian Goodfellow, et al](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf). This paper literally sparked a lot of interest in adversarial training of neural net, proved by the number of citation of the paper. Suddenly, many flavors of GAN came up: DCGAN, Sequence-GAN, LSTM-GAN, etc. In NIPS 2016, there will even be [a whole workshop](https://sites.google.com/site/nips2016adversarial/) dedicated for adversarial training!\n\nNote, the code is available in https://github.com/wiseodd/generative-models.\n\nFirst, let's review the main points about the paper. After that, as always, we will try to implement GAN using TensorFlow, with MNIST data.\n\n## Generative Adversarial Nets\n\nLet's consider the rosy relationship between a money conterfeiting criminal and a cop. What's the objective of the criminal and what's the objective of the cop in term of counterfeited money? Let's enumerate:\n\n- To be a successful money counterfeiter, the criminal wants to fool the cop, so that the cop can't tell the difference between counterfeited money and real money\n- To be a paragon of justice, the cop wants to detect counterfeited money as good as possible\n\nThere, we see we have a clash of interest. This kind of situation could be modeled as a minimax game in Game Theory. And this process is called Adversarial Process.\n\nGenerative Adversarial Nets (GAN), is a special case of Adversarial Process where the components (the cop and the criminal) are neural net. The first net generates data, and the second net tries to tell the difference between the real data and the fake data generated by the first net. The second net will output a scalar `[0, 1]` which represents a probability of real data.\n\nIn GAN, the first net is called Generator Net $G(Z)$ and the second net called Discriminator Net $D(X)$.\n\n\u003CBlogImage imagePath='/img/gan-tensorflow/obj.png' />\n\nAt the equilibrium point, which is the optimal point in minimax game, the first net will models the real data, and the second net will output probability of 0.5 as the output of the first net = real data.\n\n\"BTW why do we interested in training GAN?\" might come in mind. It's because probability distribution of data $P_{data}$ might be a very complicated distribution and very hard and intractable to infer. So, having a generative machine that could generate samples from $P_{data}$ without having to deal with nasty probability distribution is very nice. If we have this, then we could use it for another process that require sample from $P_{data}$ as we could get samples relatively cheaply using the trained Generative Net.\n\n## GAN Implementation\n\nBy the definition of GAN, we need two nets. This could be anything, be it a sophisticated net like convnet or just a two layer neural net. Let's be simple first and use a two layer nets for both of them. We'll use TensorFlow for this purpose.\n\n```python\n# Discriminator Net\nX = tf.placeholder(tf.float32, shape=[None, 784], name='X')\n\nD_W1 = tf.Variable(xavier_init([784, 128]), name='D_W1')\nD_b1 = tf.Variable(tf.zeros(shape=[128]), name='D_b1')\n\nD_W2 = tf.Variable(xavier_init([128, 1]), name='D_W2')\nD_b2 = tf.Variable(tf.zeros(shape=[1]), name='D_b2')\n\ntheta_D = [D_W1, D_W2, D_b1, D_b2]\n\n# Generator Net\nZ = tf.placeholder(tf.float32, shape=[None, 100], name='Z')\n\nG_W1 = tf.Variable(xavier_init([100, 128]), name='G_W1')\nG_b1 = tf.Variable(tf.zeros(shape=[128]), name='G_b1')\n\nG_W2 = tf.Variable(xavier_init([128, 784]), name='G_W2')\nG_b2 = tf.Variable(tf.zeros(shape=[784]), name='G_b2')\n\ntheta_G = [G_W1, G_W2, G_b1, G_b2]\n\ndef generator(z):\n    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n    G_prob = tf.nn.sigmoid(G_log_prob)\n    return G_prob\n\ndef discriminator(x):\n    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n    D_prob = tf.nn.sigmoid(D_logit)\n    return D_prob, D_logit\n```\n\nAbove, `generator(z)` takes 100-dimensional vector and returns 786-dimensional vector, which is MNIST image (28x28). `z` here is the prior for the $G(Z)$. In a way it learns a mapping between the prior space to $P_{data}$.\n\nThe `discriminator(x)` takes MNIST image(s) and return a scalar which represents a probability of real MNIST image.\n\nNow, let's declare the Adversarial Process for training this GAN. Here's the training algorithm from the paper:\n\n\u003CBlogImage imagePath='/img/gan-tensorflow/algorithm.png' fullWidth />\n\n```python\nG_sample = generator(Z)\nD_real, D_logit_real = discriminator(X)\nD_fake, D_logit_fake = discriminator(G_sample)\n\nD_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\nG_loss = -tf.reduce_mean(tf.log(D_fake))\n```\n\nAbove, we use negative sign for the loss functions because they need to be maximized, whereas TensorFlow's optimizer can only do minimization.\n\nAlso, as per the paper's suggestion, it's better to maximize `tf.reduce_mean(tf.log(D_fake))` instead of minimizing `tf.reduce_mean(1 - tf.log(D_fake))` in the algorithm above.\n\nThen we train the networks one by one with those Adversarial Training, represented by those loss functions above.\n\n```python\n# Only update D(X)'s parameters, so var_list = theta_D\nD_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n\n# Only update G(X)'s parameters, so var_list = theta_G\nG_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n\ndef sample_Z(m, n):\n    '''Uniform prior for G(Z)'''\n    return np.random.uniform(-1., 1., size=[m, n])\n\nfor it in range(1000000):\n    X_mb, _ = mnist.train.next_batch(mb_size)\n    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n```\n\nAnd we're done! We can see the training process by sampling $G(Z)$ every now and then:\n\n\u003CBlogImage imagePath='/img/gan-tensorflow/training.gif' />\n\nWe start with random noise and as the training goes on, $G(Z)$ starts going more and more toward $P_{data}$. It's proven by the more and more similar samples generated by $G(Z)$ compared to MNIST data.\n\n## Alternative Loss Formulation\n\nWe could formulate the loss function `D_loss` and `G_loss` using different notion.\n\nLet's follow our intuition. This is inspired by the post about image completion in [Brandon Amos' blog](http://bamos.github.io/2016/08/09/deep-completion/).\n\nIf we think about it, the `discriminator(X)` wants to make all of the outputs to be `1`, as per definition, we want to maximize the probability of real data. The `discriminator(G_sample)` wants to make all of the outputs to be `0`, as again by definition, $D(G(Z))$ wants to minimize the probability of fake data.\n\nWhat about `generator(Z)`? It wants to maximize the probability of fake data! It's the opposite objective of $D(G(Z))$!\n\nHence, we could formulate the loss as follow.\n\n```python\n# Alternative losses:\n# -------------------\nD_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(D_logit_real, tf.ones_like(D_logit_real)))\nD_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(D_logit_fake, tf.zeros_like(D_logit_fake)))\nD_loss = D_loss_real + D_loss_fake\nG_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(D_logit_fake, tf.ones_like(D_logit_fake)))\n```\n\nWe're using the Logistic Loss, following the notion above. Changing the loss functions won't affect the GAN we're training as this is just a different way to think and formulate the problem.\n\n## Conclusion\n\nIn this post, we looked at Generative Adversarial Network (GAN), which was published by Ian Goodfellow, et al. at NIPS 2014. We looked at the formulation of Adversarial Process and the intuition behind it.\n\nNext, we implemented the GAN with two layer neural net for both the Generator and Discriminator Net. We then follow the algorithm presented in Goodfellow, et al, 2014 to train the GAN.\n\nLastly, we thought about the different way to think about GAN loss functions. In the alternative loss functions, we think intuitively about the two networks and used Logistic Loss to model the alternative loss functions.\n\nFor the full code, head to https://github.com/wiseodd/generative-models!\n\n## References\n\n- [Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in Neural Information Processing Systems. 2014.](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)\n- [Image Completion with Deep Learning in TensorFlow](http://bamos.github.io/2016/08/09/deep-completion/)","src/content/post/gan-tensorflow.mdx","c9bdaa25e39dc9b8","gan-tensorflow.mdx","gaussian-anomaly-detection",{"id":254,"data":256,"body":261,"filePath":262,"digest":263,"legacyId":264,"deferredRender":23},{"title":257,"description":258,"publishDate":259,"draft":15,"tags":260},"Gaussian Anomaly Detection","In Frequentist and Bayesian Way",["Date","2016-01-16T16:06:00.000Z"],[],"import BlogImage from \"@/components/BlogImage.astro\";\n\nSuppose you are a teacher in kindergarten. Looking at your class, it seems there are a few children that are out of the ordinary, in term of their height compared to the rest of the class. It seems that they are different, by just looking at them, you're sure of it. You've talked about this to your principle: \"Hey, A and B in my class are way taller than the rest of the class\". The principle replied: \"You sure it's not just an optical illusion? Could you present me with a proof?\". You are then wondering, is there any method that can help you identifying those children?\n\nYes, there is! It's called Anomaly Detection!\n\nSpecifically, what we're going to do is Gaussian Anomaly Detection, because we'll assume that our data are normally distributed, and fit it into a Gaussian distribution. Recall, a Gaussian is a distribution that is paramaterized by two values: mean and standard deviation.\n\nBut, what does it mean to fit data into a Gaussian distribution? Well, it means that we want to find a Gaussian that best represents our data by finding the optimal parameters (mean and standard deviation) under our data. Now, from here, we have two methods for finding the optimal parameters for the Gaussian, by using Frequentist or Bayesian method. Let's go with Frequentist method first.\n\n## Frequentist Method\n\nIn a Frequentist setting, finding the optimal parameter equals to finding the parameter that maximize the likelihood of the data. There are two laws that are supporting this method: the Likelihood Principle, and the Law of Likelihood. Basically, they say that _\"the parameter value which maximizes the likelihood function is the value which is most strongly supported by the evidence\"_.\n\nNow, how do we calculate the maximum likelihood? It turns out, the Maximum Likelihood Estimation (MLE) of mean and standard deviation of a Gaussian are just their sample mean and sample standard deviation. I'm not going to derive it here, but you could read the Bishop's book to understand why it's just sample mean and sample standard deviation.\n\nSo, let's say we have these data:\n\n```python\nX = np.array([10, 14, 12, 1200, 25, 120, 54, 32, 18, 23])\n```\n\nThe best fit Gaussian for those data is `N(mu_MLE, std_MLE)`, that is, Gaussian parameterized by the maximum likelihood estimation of its parameters.\n\n```python\nmu = np.mean(X) # 150.8\nsigma = np.std(X) # 351.1199\n```\n\nSo, that's it! We've fitted our data into `N(150.8, 351.1199)`!\n\n\u003CBlogImage imagePath='/img/gaussian-anomaly-detection/00.png' />\n\nNow, what's left is the evaluation phase. We want to know, based our Gaussian, whether or not a new data is an anomaly. To do that, we need a threshold value that says, if the probability of the new data under our Gaussian is higher or lower than the threshold, it's probably an anomaly.\n\nWe could intuitively define the threshold by looking at the plot of our Gaussian above. The anomalies would be the data that fall under the low probability areas of the Gaussian, because being in the low probability area, that data is _highly unlikely_ to be distributed in our distribution. Those low probability areas are the left and the right tails of the Gaussian. So, we could say that, for example, 25% area under the tails are the anomaly regions.\n\n```python\n# Thresholds: 25 percent of left tail and 25 percent of right tail\nt1, t2 = 25, 75\n\n# Our test data\nX_test = np.array([1500, 10, 35, 400])\n\n# Use CDF to find the position of the data under the curve\ny_test = st.norm.cdf(X_test, mu, sigma)\n\n# Compare the CDFs with the thresholds\nfor y in y_test:\n    if y \u003C t1 or y > t2:\n        print 'Anomaly!'\n```\n\n## Bayesian Method\n\nWe could also use Bayesian method to fit the Gaussian. The obvious advantage of Bayesian method is that it will not just provide a point estimate of the parameters we want to find. Bayesian method will model our parameters as distributions, so we get whole range of all possible values of those parameters, and we can do more with it.\n\nFor this, I will use Python library called PyMC. First of all, we need to specify a prior for all of the parameters we want to find: mean and standard deviation. Pretend that I know nothing about those parameters, so the logical choice of prior will be Uniform prior.\n\n```python\n# Prior\nmu = pymc.Uniform('mu', 0, 1000)\nsigma = pymc.Uniform('sigma', 0, 1000)\n```\n\nNow, we need to express our data in term of likelihood function. Because we're assuming the normality of our data, we will use Gaussian likelihood function. And because we already have our data `X`, we will set this as `observed=True`.\n\n```python\n# Likelihood\nlikelihood = pymc.Normal('likelihood', mu, sigma**-2, observed=True, value=X)\n```\n\nThe only thing left is to infer the posterior distribution of our mean and standard deviation.\n\n```python\nmcmc = pymc.MCMC([mu, sigma, likelihood])\nmcmc.sample(iter=11000, burn=1000)\n```\n\nHere's our posterior distributions obtained after 11000 MCMC iteration:\n\n\u003CBlogImage imagePath='/img/gaussian-anomaly-detection/01.png' />\n\nAs you can see, we get a distribution, not just a point estimate for our parameters. That means, we can do a lot more analysis to these result compared to MLE result. For now though, we will just take a point estimate of mean and standard deviation, just like MLE. Because the distributions are rather skewed, we will use median as our point estimates.\n\n```python\nmu = stats['mu']['quantiles'][50] # 171.0512\nsigma = stats['sigma']['quantiles'][50] # 398.2883\n```\n\nNow, we could just plug those parameters to the Gaussian, and we get our fitted Gaussian distribution. Using the same idea, you could determine whether a new data point is an anomaly or not.","src/content/post/gaussian-anomaly-detection.mdx","3e49e471304d7d01","gaussian-anomaly-detection.mdx","gleam-aoc-2024",{"id":265,"data":267,"body":272,"filePath":273,"digest":274,"legacyId":275,"deferredRender":23},{"title":268,"description":269,"publishDate":270,"draft":15,"tags":271},"Advent of Code 2024: A Gleam Retrospective","Gleam is a rather new functional language. How does it fare in AoC? What about its scientific-computing ecosystem?",["Date","2024-12-28T05:00:00.000Z"],[],"This year (2024), I did the [Advent of Code](https://adventofcode.com) challenges/games.\nIt's essentially a daily programming puzzle challenge, spanning over the advent period (December 1 - 25).[^1]\nSince I have extensively learned [Rust](https://github.com/wiseodd/rustlox) and [Zig](https://github.com/wiseodd/rustlox) early on in the year, and since I write [Python](https://github.com/wiseodd) professionally, I decided to use something new for this.\n\nBecause of Rust, I became to like the [ML family of languages](\u003Chttps://en.wikipedia.org/wiki/ML_(programming_language)>) (think of OCaml, not machine learning).\nMy choice landed upon [Gleam](https://gleam.run/), a functional language that is quite similar in look-and-feel as Rust.\nProgramming in Gleam does feel like programming in Rust but in a strict, functional way:\nThere's no escape hatch to fall back to procedural programming with mutability.\n\nThis post is a \"great-bad\" (👍-👎) evaluation of Gleam for solving programming puzzles.\nAt the end, I will also give my opinion on what Gleam is currently lacking from the perspective of a machine learning scientist.\n\n## 👍 I wish every language had a pipeline operator\n\nSuppose we have $n$ functions $(f_i: \\mathcal{X}_{i-1} \\to \\mathcal{X}_{i})_{i=1}^n$ where each function $f_i$ has domain the previous function's codomain.\nSuppose $x \\in \\mathcal{X}_0$ is the initial input and we would like to apply this sequence of functions to obtain $y \\in \\mathcal{X}_n$.\nThere are two ways of doing this:\n\n- compose the functions, then apply $x$: $(f_n \\cdot \\dots \\cdot f_1)(x)$, or\n- apply the function to the previous function's output: $f_n(f_{n-1}(\\dots f_1(x)))$.\n\nBack to the programming world, usually we do the latter and we do so by calling `f_n(f_n_1(... f_1(x)))`.\nBut it can be cumbersome to match the parens.\nEven more so if $n$ is large.\n\nIn Gleam (and other functional languages), there is a special operator called the _pipeline operator_ `|>`.\nThe above example can then be written as `x |> f_1 |> f_2 |> ... |> f_n`.\nNeat!\n\nHere's an example from my [AoC 2024 solutions](https://github.com/wiseodd/aoc-2024) code:\n\n```gleam\nlet result =\n    x\n    |> string.trim\n    |> string.split(\": \")\n    |> list.last\n    |> result.unwrap(\"\")\n    |> string.split(\",\")\n\n// Compare to: (!!)\nlet result = string.split(result.unwrap(list.last(string.split(string.trim(x), \": \")), \"\"), \",\")\n```\n\nAs we can see, the pipeline operator is a very nice way to \"carry\" an input through the \"pipeline\" to obtain the result at the opposite end.\n\nTraditionally the pipeline operator will put the output of the previous function to the first input of the next function.\nHowever, in Gleam, we can put an output into an arbitrary input slot of the next function:\n\n```gleam\nfn f(first: String, second: Int, third: Float) {\n  ...\n}\n\nlet x: Int = 5\nx |> f(\"first\", _, 7.22)\n```\n\nThat is, `_` marks the \"slot\" we want to put the l.h.s. of the pipeline operator into.\n\n## 👍 The \"let assert\" keyword is great for rapid prototyping\n\nWhen writing a Rust code, I often do so in multiple passes.\nFirst, a prototyping phase where I use the `.unwrap()` and `if-let-else-panic` snippets extensively to validate my algorithms quickly without worrying about safety.\nThen, the second phase is to refactor the above carefully, and handle all cases/errors correclty instead of throwing panic everyime.\n\nHowever, in programming challenges like AoC, we don't really need to robustly handle error --- all that matters is correctness.\nSo it's useful to just stop after the first phase.\n\nHow can we do this in Gleam?\nFor sure we don't really want to handle all pattern matching cases exhaustively for AoC.\nFor instance, if we know the input for the puzzle is a file with two text blocks separated by an empty line, we process this input by:\n\n```gleam\nimport simplifile\n\nfn parse() {\n  case simplifile.read(\"data/day25_input.txt\") {\n    Ok(content) -> {\n      case content |> string.trim |> string.split(\"\\n\\n\") {\n        [first_block, second_block] -> ...\n        _ -> ...\n      }\n    }\n    Error(_) -> ...\n  }\n}\n```\n\nFor the sake of robustness, the code will be quite long and nested.\nThankfully, Gleam has a special keyword `let assert` to alleviate this.\nThink of it as `.unwrap()` and `if-let-else-panic` in Rust:\nIf something doesn't match the asserted pattern, it will immediately panic.\n\nSo, the code above can be written as:\n\n```gleam\nimport simplifile\n\nfn parse() {\n  let assert Ok(content) = simplifile.read(\"data/day25_input.txt\")\n  let assert [first_block, second_block] = content |> string.trim |> string.split(\"\\n\\n\")\n  ...\n}\n```\n\nMuch more concise!\n\n## 👍 The \"use\" expression is a godsend\n\nI wrote about Gleam's `use` expression in the [previous post](/blog/gleam-use), where I approached the discussion from Python's `with`-block perspective.\nHowever, it can also be combined with the stdlib functions `bool.guard` and `bool.lazy_guard` to emulate early returns in procedural languages and to easily emulate Python's decorator.\n\n### Emulating early return\n\nIn Rust, we can do:\n\n```rust\nfn my_function(x: i32) -> bool {\n  if x > 5 {\n    return false;\n  }\n\n  let y = x + 3;\n  let z = y * 3;\n  z == 11\n}\n\nfn my_function_alt(x: i32) -> bool {\n  if x > 5 {\n    return false;\n  } else {\n    let y = x + 3;\n    let z = y * 3;\n    return z == 11;\n  }\n}\n```\n\nNotice that the first function is cleaner.\nThis will be even more apparent if inside the `else` block we have a very long code.\n\nThe standard functional counterpart would be:\n\n```gleam\nfn my_function(x: Int) -> Bool {\n  case x {\n    _ if x > 5 -> False\n    _ -> {\n      let y = y + 3\n      let z = y * 3\n      z == 11\n    }\n  }\n}\n```\n\nImagine if we have nested `if`'s.\nThe nestings, indentations, and curly brackets in the above Gleam code will make the code quite ugly and hard to debug.\nLuckily, the `use` expression, combined with `bool.guard` can rescue us:\n\n```gleam\nimport gleam/bool\n\nfn my_function(x: Int) -> Bool {\n  use \u003C- bool.guard(x > 5, False)\n  let y = y + 3\n  let z = y * 3\n  z == 11\n}\n```\n\nNotice that we have no nesting now!\nTo drive home the point, consider this nested-if code:\n\n```gleam\nfn my_function(x: Int) -> Bool {\n  case x {\n    _ if x > 5 -> False\n    _ -> {\n      let y = y + 3\n      case y {\n        4 -> False\n        _ -> {\n          let z = y * 3\n          case z {\n            _ if z % 2 == 0 -> True\n            _ -> z == 11\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nI'm already tired just looking at that code 😩.\nWith `use` and `bool.guard` we can achieve a cleaner look:\n\n```gleam\nimport gleam/bool\n\nfn my_function(x: Int) -> Bool {\n  use \u003C- bool.guard(x > 5, False)\n\n  let y = y + 3\n  use \u003C- bool.guard(y == 4, False)\n\n  let z = y * 3\n  use \u003C- bool.guard(z % 2 == 0, True)\n\n  z == 11\n}\n```\n\nNo indentation, no nesting 🥳!\n\nAs a final note, be attentive to the return value of your early return.\nIf the return value is a result of a dynamic computation, `bool.lazy_guard` should be used instead.\nThe difference is that the second argument (previously, the return value e.g. `False`) is now a callback function that will be executed lazily when the first argument evaluates to `True`.\n\n### Emulating Python decorator\n\nSince AoC is a programming puzzle, dynamic programming (DP) questions often come up.\nIn functional language, it's natural to write a top-down DP since it amounts to recursion and memoization.\nHowever, memoization is cumbersome in functional language due to immutability.\nFor example, in Python, we can do:\n\n```python\nmemo: Dict[int, int] = {}\n\ndef main():\n    ns: List(int) = [1, 2, 59, 10]\n    fibs: List(int) = [fibo(n) for n in ns]\n\ndef fibo_explicit(n: int) -> int:\n    if n in memo:\n        return memo[n]\n\n    if n \u003C= 1:\n        return n\n\n    res = fibo(n - 1) + fibo(n - 2)\n    memo[n] = res\n\n    return res\n\nimport functools\n\n@functools.cache\ndef fibo_cache(n: int) -> int:\n    if n \u003C= 1:\n        return n\n\n    return fibo_cache(n - 1) + fibo_cache(n - 2)\n```\n\nAs we can see, we can have a clean code by defining `memo` as a global, mutable variable.\nMoreover, this can be simplified further by using `functools.cache` decorator: We write the standard, non-memoized version, and just slap `@functools.cache` on top.\nBut in Gleam, we have to do this:\n\n```gleam\nimport gleam/dict.{type Dict}\nimport gleam/pair\n\nfn main() {\n  let memo: Dict(Int, Int) = dict.new()\n  let ns: List(Int) = [1, 2, 59, 10]\n\n  let fibs: List(Int) = ns |> list.map_fold(memo, fn(memo, n) {\n    let #(res, memo) = fibo(n, memo)\n    #(memo, res)\n  })\n  |> pair.second\n}\n\nfn fibo(n: Int, memo: Dict(Int, Int)) -> #(Int, Dict(Int, Int)) {\n  use \u003C- bool.lazy_guard(memo |> dict.has_key(n), fn() { memo |> dict.get(n) |> result.unwrap(0) })\n  use \u003C- bool.guard(n \u003C= 1, n)\n\n  let #(res, memo) = fibo(n - 1, memo) + fibo(n - 2, memo)\n  let memo = memo |> dict.insert(n, res)\n\n  #(res, memo)\n}\n```\n\nWe use `list.map_fold` since we have to keep track of the state of `memo` and reuse it for the next function calls of `fibo`.\nIt has quite a bit of boilerplate and we must return `memo` together with the result in the `fibo` function.\nThe result is a longer and less clean code.\n\nThankfully, the `use` expression enables us to emulate Python's decorator, which `@functools.cache` is.\nSomeone has thought of this and it's available here, as a [`rememo` library](https://github.com/hunkyjimpjorps/rememo).\nThen we can write this cleaner code:\n\n```gleam\nimport gleam/dict.{type Dict}\nimport rememo/memo\n\nfn main() {\n  let ns: List(Int) = [1, 2, 59, 10]\n\n  use cache \u003C- memo.create()\n\n  let fibs: List(Int) = ns |> list.map(fn(n) {\n    fibo(n, cache)\n  })\n}\n\nfn fibo_cache(n: Int, cache) -> Int {\n  use \u003C- memo.memoize(cache, n)\n  use \u003C- bool.guard(n \u003C= 1, n)\n  fibo_cache(n - 1, cache) + fibo_cache(n - 2, cache)\n}\n```\n\nLooks great 🤩🤩🤩!!!\n\n## 👎 No operator overloading\n\nSuppose I have an integer `x` and a float `y` and I want to add them together.\nIn most contemporary languages, you can just write `x + y`.\nBut this cannot be done in Gleam since integer addition and float addition are two different operators: `+` and `+.`, respectively.\nThere is a good reason for this, just like in [OCaml](https://stackoverflow.com/questions/64244351/why-does-ocaml-have-mandatory-distinct-float-and-int-literal-syntax).\nBut the end result is a more verbose, out-of-the-ordinary (and thus perceived as unnatural) code:\n\n```gleam\nlet x: Int = 5\nlet y: Float = 10.0\n\n// Error!\nlet res: Float = x + y\n\n// Ok!\nimport gleam/int\nlet res: Float = int.to_float(x) +. y\n```\n\nI could imagine that due to this, you cannot have a concise expression like `tensor_1 + tensor_2` for some `ndarray` types defined in a hypothetical Gleam tensor library.\n\nAll in all, I don't think this will help Gleam if it wants to be widely adopted by people from various scientific communities.\n\n## 👎 Barebone ecosystem\n\nOn [Day 13 of AoC 2024](https://adventofcode.com/2024/day/13), the puzzle can be solved by solving a system of linear equations.\nIn Python, there is Numpy, PyTorch, etc., giving rise to a concise solution:\n\n```python\nA = ...\nb = ...\nx = np.linalg.solve(A, b)  # or, np.linalg.inv(A) @ b\n```\n\nBut what about Gleam?\nI don't see any robust, full-featured ndarray/tensor library in Gleam.\nOCaml has [`owl`](https://github.com/owlbarn/owl), Elixir has [`nx`](https://github.com/elixir-nx/nx), Gleam has none.\nNot only that, `pandas`-like and `matplotlib`-like libraries are also missing.\n\nFrom what I've gathered, Gleam is fully interoperable with other languages in the Erlang virtual machine.\nThis means Gleam can actually call Elixir libraries such as `nx` for ndarrays, [`explorer`](https://github.com/elixir-explorer/explorer) for dataframes, and [`VegaLite`](https://github.com/livebook-dev/vega_lite) for plotting.\nHowever, it seems one needs to explicitly \"translate\" every Elixir function into Gleam first before using it.\nFor example: [^2]\n\n```gleam\n@external(erlang, \"Elixir.BasicProject\", \"hello\")\nfn hello_elixir() -> Thing\n```\n\nI can then call `hello_elixir()` in my `.gleam` file and it will, in turn, call the specified Elixir code.\nIt would be great if we could just call Elixir code without translating it first: Just add Elixir dependencies, then call.\nOr, better yet, we need Gleam bindings for all those aforementioned libraries.\nWithout this, I don't think Gleam can sway Python's ML or data science community.\n\n## Conclusion\n\nGleam is great and is a breath of fresh air coming from Python!\nI wish I could just use Gleam for all my research work but alas.\n\nActually, after writing this post, I became even more intrigued with Elixir.\nIt might scratch my itch of moving away from Python due to less \"quirky\" design decisions and due to its more mature, vibrant scientific computing ecosystem.\nIt's a shame that I can't move to Gleam (just yet, hopefully), though.\n\n[^1]: In the Christian tradition, the advent calendar is essentially used to count the days in anticipation of Christmas.\n\n[^2]: Taken from https://github.com/gleam-lang/mix_gleam","src/content/post/gleam-aoc-2024.mdx","c3d4498867dea6a3","gleam-aoc-2024.mdx","gleam-use",{"id":276,"data":278,"body":283,"filePath":284,"digest":285,"legacyId":286,"deferredRender":23},{"title":279,"description":280,"publishDate":281,"draft":15,"tags":282},"The 'use' Expression in Gleam","How can we emulate the behavior of Python's `with` and Rust `?` in Gleam?",["Date","2024-08-03T04:00:00.000Z"],[],"Everybody who knows Python has used the `with` statement, most commonly when opening\na file.[^1]\nIt's quite powerful!\n\nGleam is a radically different type of language from Python.\nIt can be seen as a purely functional version of Rust.\nWe know that things are rather less obvious in purely functional languages.\nSo the question here is, how can we emulate the behavior of Python's `with` statement?\nAnother question is (due to the similarity of Rust and Gleam), how to emulate\nRust's `?` syntax in Gleam?\n\n## 'use' For Emulating the 'with' Statement in Python\n\nIn Python, this code\n\n```python title=\"Python\"\nf = open(\"filename.txt\")\nret = do_something_with_file(f)\nf.close()\n```\n\ncan be rewritten with the `with` statement, resulting in a nice, less verbose\npiece of code:\n\n```python title=\"Python\"\nwith open(\"filename.txt\") as f:\n    ret = do_something_with_file(f)\n```\n\nConceptually, the `with` statement can be thought of as a higher-order function\n(a function that takes other functions) with a callback function as its last parameter.\nThe block inside the statement (`ret = ...` above) is the body of the callback function.\nIn code, this is:\n\n```python title=\"Python\"\ndef file_action(filename: str, callback: Callable[[File], Any]):\n    f = open(filename)\n    ret = callback(f)\n    f.close()\n    return ret\n\nret = file_action(\"filename.txt\", do_something_with_file)\n```\n\nAssuming the functions `open`, `close` are available and behaving similarly to the\nabove, we can rewrite it one-to-one in Gleam.\n(The type `rtype` below is a generic type.)\n\n```gleam title=\"Gleam\"\nfn file_action(filename: String, callback: fn(File) -> rtype) -> rtype {\n  let f: File = open(filename)\n  let ret: rtype = callback(f)\n  close(filename)\n  ret\n}\n```\n\nWhat is the Gleam equivalent to the same code, written with the `with`\nstatement then?\nIt is this:\n\n```gleam title=\"Gleam\"\nuse f \u003C- file_action(\"filename.txt\")\nlet ret: rtype = do_something_with_file(f)\n```\n\nWe can think of the first line `use retval \u003C- func(args)` as the direct counterpart\nof `with func(args) as retval`.\nNote that _all_ code under `use .. \u003C- ..` will be included in the body of `callback`.[^2]\nIf this is an undesirable behavior, we can make the scope similar\nto Python's by simply doing:\n(Note that in Gleam, everything is an expression!)\n\n```gleam title=\"Gleam\"\nlet ret: rtype = {\n  use f \u003C- file_action(\"filename.txt\")\n  do_something_with_file(f)\n}\n\n// Outside of the scope\ndo_something_else(ret)\n```\n\n## 'use' For Emulating '?' in Rust\n\nGleam is like a functional (garbage-collected) variant of Rust---they have many\nsimilarities.\nOne of them is this: errors in Gleam and Rust are _return values_.\nSo, there is no `throw-try-catch` as in Python or C-inspired languages.\n\nThe usual way we handle errors in Rust is by returning the type\n`Result\u003Creturn_type, error_type>`.\nThen, if we don't want to do error handling in a function that calls a function\nwith the `Result` type as the return value, we can simply return that error\nwhen it occurs.\nIn code, this is:\n\n```rust title=\"Rust\"\nfn do_something_and_return_string() -> Result\u003CString, Err> {\n  // ...\n}\n\nfn process_the_retval() -> Result\u003C(), Err> {\n  match do_something_and_return_string() {\n    Ok(ret_val) -> // do something with the string ret_val\n    Err(err_val) -> Err(err_val) // just propagate the err\n  }\n}\n```\n\nNotice that we must do the pattern matching `match` to handle the `Result` type.\nNotice also that if we don't want to handle the error here, we have that line\n`Err(err_val) -> Err(err_val)` which is quite verbose.\n\nIn Rust, this can be concisely rewritten with `?`:[^3]\n\n```rust title=\"Rust\"\nfn process_the_retval() -> Result\u003C(), Err> {\n  let ret_val: String = do_something_and_return_string()?;\n  // do something with the string ret_val\n}\n```\n\nGleam also has the `Result` type and `use` can be used to do the same\nthing as in Rust.\n\n```gleam title=\"Gleam\"\nfn process_the_retval() -> Result(Nil, Err) {\n  case do_something_and_return_string() {\n    Ok(ret_val) -> // do something with the string ret_val\n    Error(err_val) -> Error(err_val)\n  }\n}\n```\n\nIn Gleam, we can use `result.map`\nto circumvent the need for pattern matching.[^4]\n\n```gleam title=\"Gleam\"\nfn callback(s: String) {\n  // do something with the string ret_val\n}\n\nfn process_the_retval() -> Result(Nil, Err) {\n  result.map(do_something_and_return_string(), callback)\n}\n```\n\nBut this is just the higher-order function pattern that we have seen before\nin the previous section.\nSo, we can rewrite it more concisely with `use`:\n\n```gleam title=\"Gleam\"\nfn process_the_retval() -> Result(Nil, Err) {\n  use ret_val \u003C- result.map(do_something_and_return_string())\n  // do something with the string ret_val\n}\n```\n\nNotice the similarity as the Rust code with `?`!\n\n[^1]: https://realpython.com/python-with-statement/\n\n[^2]: https://gleam.run/news/v0.25-introducing-use-expressions/\n\n[^3]: https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html#propagating-errors\n\n[^4]: https://hexdocs.pm/gleam_stdlib/gleam/result.html#map","src/content/post/gleam-use.mdx","1fe85ccefb91c010","gleam-use.mdx","gibbs-sampling",{"id":287,"data":289,"body":294,"filePath":295,"digest":296,"legacyId":297,"deferredRender":23},{"title":290,"description":291,"publishDate":292,"draft":15,"tags":293},"Gibbs Sampling","Example of Gibbs Sampling implementation in Python to sample from a Bivariate Gaussian.",["Date","2015-10-09T17:01:00.000Z"],[17,42,43],"import BlogImage from \"@/components/BlogImage.astro\";\n\nAfter so many months struggling with Gibbs Sampling, now I conquered it! Well, kind of.\n\nThis week's been a renaissance on MCMC. I decided to open up again those Machine Learning Summer School (MLSS) Cambridge 2009, and absent mindedly opened that lecture about MCMC by Iain Murray. Oh boy, did I hit the jackpot? That lecture was really great. Here, help yourself, watch the lecture http://videolectures.net/mlss09uk_murray_mcmc/.\n\nSo, full of confidence and revelation after watching that lecture, I decided to implement the Gibbs Sampler. Now, I won't dive deep on what is Gibbs Sampling and what not, but if you want to know deeper about it, I suggest you to read this tutorial: Gibbs Sampling for the Uninitiated.\n\nGibbs Sampling is a MCMC method to draw samples from a potentially really really complicated, high dimensional distribution, where analytically, it's hard to draw samples from it. The usual suspect would be those nasty integrals when computing the normalizing constant of the distribution, especially in Bayesian inference. Now Gibbs Sampler can draw samples from any distribution, provided you can provide all of the conditional distributions of the joint distribution analitically.\n\nIn this example I will use Gibb Sampler to draw sampler from a Bivariate Gaussian with mu of `[5, 5]` and sigma/covariance matrix of `[[1, 0.9], [0.9, 1]]`. The exact distribution should look like this:\n\n\u003CBlogImage imagePath='/img/gibbs-sampling/00.png' altText='Target distribution.' />\n\nNow, pretend that this distribution is really complicated and very hard to sample (I know, I know, but please bear with me). We don't know how to sample from this directly, and we don't even know the shape of the distribution. However, because of some mathematical convenience, or maybe by just sheer luck, we know the conditional distributions: `P(X|Y)` and `P(Y|X)`. By now, it screams \"Gibbs Sampling!\".\n\nThe derivation of conditional distribution of Multivariate Gaussian could be found here: http://fourier.eng.hmc.edu/e161/lectures/gaussianprocess/node7.html.\n\nLet's inspect the Gibbs Sampler code, shall we.\n\n```python\nimport numpy as np\nimport seaborn as sns\n\ndef p_x_given_y(y, mus, sigmas):\n    mu = mus[0] + sigmas[1, 0] / sigmas[0, 0] * (y - mus[1])\n    sigma = sigmas[0, 0] - sigmas[1, 0] / sigmas[1, 1] * sigmas[1, 0]\n    return np.random.normal(mu, sigma)\n\ndef p_y_given_x(x, mus, sigmas):\n    mu = mus[1] + sigmas[0, 1] / sigmas[1, 1] * (x - mus[0])\n    sigma = sigmas[1, 1] - sigmas[0, 1] / sigmas[0, 0] * sigmas[0, 1]\n    return np.random.normal(mu, sigma)\n\ndef gibbs_sampling(mus, sigmas, iter=10000):\n    samples = np.zeros((iter, 2))\n    y = np.random.rand() * 10\n\n    for i in range(iter):\n        x = p_x_given_y(y, mus, sigmas)\n        y = p_y_given_x(x, mus, sigmas)\n        samples[i, :] = [x, y]\n\n    return samples\n\nif __name__ == '__main__':\n    mus = np.array([5, 5])\n    sigmas = np.array([[1, .9], [.9, 1]])\n\n    samples = gibbs_sampling(mus, sigmas)\n    sns.jointplot(samples[:, 0], samples[:, 1])\n```\n\nReally really really simple. The main algorithm is just what, 10 line of codes? Including whitespaces.\n\nThe potentially complicated thing would be to derive the conditional distribution. For popular distributions, you can find those derivations easily on Google. There, in `p_x_given_y` and `p_y_given_x`, the conditional distribution of a Bivariate Normal is Univariate Gaussian, with mean depends on the conditional.\n\nAfter get a hold on those conditional distributions, the rest is easy. Just plug those conditionals in to the sampler, which iteratively sample from all conditional distributions. At each iteration, Gibbs Sampler will sample from each conditional distribution in turn, and use the new value to sample the other conditional distributions.\n\nAfter a lot of iteration, it will then converge to approximately the exact distribution we're sampling. From there, you could just take the integral (mean, median, etc), or, if you're like me who like pretty things, visualize the samples.\n\nHere's the result of that Gibbs Sampler:\n\n\u003CBlogImage imagePath='/img/gibbs-sampling/01.png' altText='Gibbs samples.' />\n\nPretty good, huh?\n\nGibbs Sampling is one hell of algorithm. It's so simple, yet took me a long time to get the intuition. It's an integral algorithm in Bayesian Inference landscape. One of the popular implementation of Gibbs Sampling would be in Mallet, where David Mimno uses Gibbs Sampler to do inference for LDA. I haven't studied Variational Bayes method, but based on my observation, LDA result using Gibbs Sampling is a lot better than the one using Variational method. I observe this in the case of Mallet vs Gensim implementation of LDA.\n\nAs aclosing note, I really really really suggest you to watch this lecture http://videolectures.net/mlss09uk_murray_mcmc/. What an excellent lecture, that is.","src/content/post/gibbs-sampling.mdx","9dd7cca5d9350950","gibbs-sampling.mdx","hessian-invariance",{"id":298,"data":300,"body":305,"filePath":306,"digest":307,"legacyId":308,"deferredRender":23},{"title":301,"description":302,"publishDate":303,"draft":15,"tags":304},"The Invariance of the Hessian and Its Eigenvalues, Determinant, and Trace","In deep learning, the Hessian and its downstream quantities are observed to be not invariant under reparametrization. This makes the Hessian to be a poor proxy for flatness and makes Newton's method non-invariant. In this post, we shall see that the Hessian and the quantities derived from it are actually invariant under reparametrization.",["Date","2023-02-09T05:00:00.000Z"],[],"import BlogImage from \"@/components/BlogImage.astro\";\n\nLet $f: \\mathcal{X} \\times \\Theta \\to \\R^k$ be a neural network, defined by $(x, \\theta) \\mapsto f(x; \\theta) = f_\\theta(x)$.\nSuppose $\\L: \\Theta \\to \\R$ is a loss function defined on the $d$-dimensional parameter space $\\Theta$ of $f$ and let $\\theta^*$ be a minimum of $\\L$.\nSuppose further $\\varphi: \\Theta \\to \\Psi$ is a **_reparametrization_**, i.e., a differentiable map with a differentiable inverse, mapping $\\theta \\mapsto \\psi$.\n\nSuppose we transform $\\theta^*$ into $\\psi^* = \\varphi(\\theta^*)$.\nThe consensus in the deep learning field regarding the Hessian matrix $H(\\theta^*)$ of $\\L$ at $\\theta^*$ is that:\n\n1. The _eigenvalues_ of $H(\\theta^*)$ are not invariant.\n2. The _determinant_ of $H(\\theta^*)$ is not invariant.\n3. The _trace_ of $H(\\theta^*)$ is not invariant.\n4. Seen as a _bilinear map_, the Hessian is not invariant outside the critical points of $\\L$.\n\nIn this post, we shall see that these quantities are actually invariant under reparametrization!\nAlthough the argument comes from Riemannian geometry, it will also hold even if we use the default assumption found in calculus---the standard setting assumed by deep learning algorithms and practitioners.\n\n**Note.**\nThroughout this post, we use the Einstein summation convention.\nThat is, we sum two variables together if one has an upper index and the other has a lower index, while omitting the summation symbol.\nFor example: $v^i w_i$ corresponds to $\\sum_i v^i w_i$ and $v^i w^j H_{ij} = \\sum_i \\sum_j v^i w^j H_{ij}$, meanwhile the index $i$ in the following partial derivative $\\partial f/\\partial \\theta^i$ counts as a lower index.\n\n## The Hessian as a Bilinear Map\n\nIn calculus, the Hessian matrix $H(\\theta^*)$ at $\\theta^*$ is defined by\n\n$$\n  H_{ij}(\\theta^*) = \\frac{\\partial^2 \\L}{\\partial \\theta^i \\theta^j}(\\theta^*) \\qquad\\qquad \\text{for all} \\qquad i,j = 1, \\dots, d .\n$$\n\nThe Hessian matrix defines a bilinear function, i.e., given arbitrary vectors $v, w$ in $\\R^d$, we can write a function $B(v, w) = v^i w^j H_{ij}(\\theta^*)$.\nFor example, this term comes up in the 2nd-order Taylor expansion of $\\L$ at $\\theta^*$:\n\n$$\n\\begin{align}\n  \\L(\\theta) &\\approx \\L(\\theta^*) + (\\nabla \\L \\vert_{\\theta^*})^\\top d + \\frac{1}{2} \\underbrace{d^\\top H(\\theta^*) d}_{=B(d, d)} ,\n\\end{align}\n$$\n\nwhere we have defined $d = (\\theta - \\theta^*)$.\n\nUnder the reparametrization $\\varphi: \\theta \\mapsto \\psi$ with $\\psi^* = \\varphi(\\theta^*)$, we have $\\L \\mapsto \\varphi^{-1}$.\nThus, by the chain and product rules, the Hessian $H_{ij}$ becomes\n\n$$\n\\begin{align}\n  \\tilde{H}_{ij} &= \\frac{\\partial^2 (\\L \\circ \\varphi^{-1})}{\\partial \\psi^i \\partial \\psi^j} = \\frac{\\partial}{\\partial \\psi^j}\\left( \\frac{\\partial \\L}{\\partial \\theta^m} \\frac{\\partial \\theta^m}{\\partial \\psi^i} \\right) \\\\\n    &= \\frac{\\partial^2 \\L}{\\partial \\theta^m \\partial \\theta^n} \\frac{\\partial \\theta^m}{\\partial \\psi^i} \\frac{\\partial \\theta^n}{\\partial \\psi^j} + \\frac{\\partial \\L}{\\partial \\theta^o} \\frac{\\partial^2 \\theta^o}{\\partial \\psi^i \\partial \\psi^j} .\n\\end{align}\n$$\n\nHowever, notice that if we evaluate $\\tilde{H}_{ij}$ at a minimum $\\psi^* = \\varphi(\\theta^*)$, the second term vanishes.\nAnd so, we have\n\n$$\n  \\tilde{H}_{ij}(\\psi^*) = \\frac{\\partial^2 \\L}{\\partial \\theta^m \\partial \\theta^n}(\\varphi^{-1}(\\psi^*)) \\frac{\\partial \\theta^m}{\\partial \\psi^i}(\\psi^*) \\frac{\\partial \\theta^n}{\\partial \\psi^j}(\\psi^*) .\n$$\n\nMeanwhile, if $v = (v^1, \\dots, v^d)$ and $w = (w^1, \\dots, w^d)$ are vectors at $\\theta^* \\in \\Theta$, their components become\n\n$$\n  \\tilde{v}^i = v^m \\frac{\\partial \\psi^i}{\\partial \\theta^m}(\\theta^*) \\qquad \\text{and} \\qquad \\tilde{w}^j = w^n \\frac{\\partial \\psi^j}{\\partial \\theta^n}(\\theta^*) ,\n$$\n\nbecause the Jacobian of the reparametrization (i.e. change of coordinates) $\\varphi: \\theta \\mapsto \\psi$ defines a change of basis.\n\nNotice that $\\frac{\\partial \\theta^m}{\\partial \\psi^i}(\\psi^*)$ is the inverse of $\\frac{\\partial \\psi^i}{\\partial \\theta^m}(\\theta^*) = \\frac{\\partial \\psi^i}{\\partial \\theta^m}(\\varphi^{-1}(\\psi^*))$.\nConsidering the transformed $H$, $v$, and $w$, the bilinear map $B$ then becomes\n\n$$\n\\begin{align}\n  \\tilde{B}(\\tilde{v}, \\tilde{w}) &= \\tilde{v}^i \\tilde{w}^j \\tilde{H}_{ij}(\\psi^*) \\\\\n    %\n    &= v^m \\cancel{\\frac{\\partial \\psi^i}{\\partial \\theta^m}(\\varphi^{-1}(\\theta^*))} w^n \\cancel{\\frac{\\partial \\psi^j}{\\partial \\theta^n}(\\varphi^{-1}(\\theta^*))} \\frac{\\partial^2 \\L}{\\partial \\theta^m \\partial \\theta^n}(\\varphi^{-1}(\\psi^*)) \\cancel{\\frac{\\partial \\theta^m}{\\partial \\psi^i}(\\psi^*)} \\cancel{\\frac{\\partial \\theta^n}{\\partial \\psi^j}(\\psi^*)} \\\\\n    %\n    &= v^m w^n H_{mn}(\\varphi^{-1}(\\psi^*)) .\n\\end{align}\n$$\n\nunder the reparametization $\\varphi$.\nSince all those indices $m$, $n$ are simply dummy indices, the last expression is equivalent to $v^i w^i H_{ij}(\\theta^*)$.\nSince $v$ and $w$ and $\\varphi$ are arbitrary, this implies that, seen as a bilinear map, the Hessian at a minimum $\\theta^*$ is _invariant_ under reparametrization.\n\n## The Non-Invariance of the Hessian\n\nWhile the Hessian, as a bilinear map at a minimum, is (functionally) invariant, some of its downstream quantities are not.\nLet us illustrate this using the determinant---one can also easily show similar results for trace and eigenvalues.\n\nFirst, recall that the components $H_{ij}(\\theta^*)$ of the Hessian transforms into the following under a reparametrization $\\varphi$:\n\n$$\n  \\tilde{H}_{ij}(\\psi^*) = \\frac{\\partial^2 \\L}{\\partial \\theta^m \\partial \\theta^n}(\\varphi^{-1}(\\psi^*)) \\frac{\\partial \\theta^m}{\\partial \\psi^i}(\\psi^*) \\frac{\\partial \\theta^n}{\\partial \\psi^j}(\\psi^*) .\n$$\n\nIn matrix notation, this is $\\tilde{\\mathbf{H}} = (\\mathbf{J}^{-1})^\\top \\mathbf{H} \\mathbf{J}^{-1}$.\n(The dependency on $\\psi^*$ is omitted for simplicity.)\nThen, the determinant of $\\tilde{\\mathbf{H}}$ is\n\n$$\n  \\det \\tilde{\\mathbf{H}} = (\\det \\mathbf{J}^{-1})^2 \\det \\mathbf{H} .\n$$\n\nThus, in general, $\\det \\tilde{\\mathbf{H}} \\neq \\det \\mathbf{H}$.\nHence the determinant of the Hessian is not invariant.\nThis causes problems in deep learning:\nFor instance, [Dinh et al. 2017](https://arxiv.org/abs/1703.04933) argue that one cannot study the connection between flatness and generalization performance at the minimum of $\\L$.\n\n## The Riemannian Hessian\n\nFrom the Riemannian-geometric perspective, the component $H_{ij}$ of the Hessian of $\\L$ is defined under $\\theta$ coordinates/parametrization as:\n\n$$\n  H_{ij} = \\frac{\\partial^2 \\L}{\\partial \\theta^i \\partial \\theta^j} - \\Gamma^k_{ij} \\frac{\\partial \\L}{\\partial \\theta^k} ,\n$$\n\nwhere $\\Gamma^k_{ij}$ is a three-dimensional array that represent the [Levi-Civita connection (or any connection)](https://en.wikipedia.org/wiki/Affine_connection) on the tangent spaces of $\\Theta$, seen as a Riemannian manifold.\nIn the calculus case, where the Euclidean metric and the Cartesian coordinates are assumed by default, $\\Gamma^k_{ij}$ vanishes identically; hence the previous definition of the Hessian.\nThis also shows that the Riemannian Hessian is a generalization to the standard Hessian.\n\nUnder a reparametrization $\\varphi: \\theta \\to \\psi$, the _connection coefficient_ $\\Gamma$ [transforms as follows](https://en.wikipedia.org/wiki/Christoffel_symbols#Transformation_law_under_change_of_variable):\n\n$$\n  \\tilde\\Gamma_{ij}^k = \\Gamma_{mn}^o \\frac{\\partial \\psi^k}{\\partial \\theta^o} \\frac{\\partial \\theta^m}{\\partial \\psi^i} \\frac{\\partial \\theta^n}{\\partial \\psi^j} + \\frac{\\partial^2 \\theta^o}{\\partial \\psi^i \\partial \\psi^j} \\frac{\\partial \\psi^k}{\\partial \\theta^o} .\n$$\n\nAnd thus, combined with the transformation of the \"calculus Hessian\" (i.e. second partial derivatives) from the previous section, the Riemannian Hessian transform as:\n\n$$\n\\begin{align*}\n  \\tilde{H}_{ij} &= \\frac{\\partial^2 (\\L \\circ \\varphi^{-1})}{\\partial \\psi^i \\partial \\psi^j} - \\tilde\\Gamma^k_{ij} \\frac{\\partial (\\L \\circ \\varphi^{-1})}{\\partial \\psi^k} \\\\\n      %\n      &= \\frac{\\partial^2 \\L}{\\partial \\theta^m \\partial \\theta^n} \\frac{\\partial \\theta^m}{\\partial \\psi^i} \\frac{\\partial \\theta^n}{\\partial \\psi^j} + \\frac{\\partial \\L}{\\partial \\theta^o} \\frac{\\partial^2 \\theta^o}{\\partial \\psi^i \\partial \\psi^j} - \\left( \\Gamma_{mn}^o \\frac{\\partial \\psi^k}{\\partial \\theta^o} \\frac{\\partial \\theta^m}{\\partial \\psi^i} \\frac{\\partial \\theta^n}{\\partial \\psi^j} + \\frac{\\partial^2 \\theta^o}{\\partial \\psi^i \\partial \\psi^j} \\frac{\\partial \\psi^k}{\\partial \\theta^o} \\right) \\frac{\\partial \\L}{\\partial \\theta^o} \\frac{\\partial \\theta^o}{\\partial \\psi^k} \\\\\n      %\n      &= \\frac{\\partial^2 \\L}{\\partial \\theta^m \\partial \\theta^n} \\frac{\\partial \\theta^m}{\\partial \\psi^i} \\frac{\\partial \\theta^n}{\\partial \\psi^j} + \\frac{\\partial \\L}{\\partial \\theta^o} \\frac{\\partial^2 \\theta^o}{\\partial \\psi^i \\partial \\psi^j} - \\Gamma_{mn}^o \\cancel{\\frac{\\partial \\psi^k}{\\partial \\theta^o}} \\frac{\\partial \\theta^m}{\\partial \\psi^i} \\frac{\\partial \\theta^n}{\\partial \\psi^j} \\frac{\\partial \\L}{\\partial \\theta^o} \\cancel{\\frac{\\partial \\theta^o}{\\partial \\psi^k}} - \\frac{\\partial^2 \\theta^o}{\\partial \\psi^i \\partial \\psi^j} \\cancel{\\frac{\\partial \\psi^k}{\\partial \\theta^o}} \\frac{\\partial \\L}{\\partial \\theta^o} \\cancel{\\frac{\\partial \\theta^o}{\\partial \\psi^k}} \\\\\n      %\n      &= \\frac{\\partial^2 \\L}{\\partial \\theta^m \\partial \\theta^n} \\frac{\\partial \\theta^m}{\\partial \\psi^i} \\frac{\\partial \\theta^n}{\\partial \\psi^j} \\cancel{+ \\frac{\\partial \\L}{\\partial \\theta^o} \\frac{\\partial^2 \\theta^o}{\\partial \\psi^i \\partial \\psi^j}} - \\Gamma_{mn}^o \\frac{\\partial \\theta^m}{\\partial \\psi^i} \\frac{\\partial \\theta^n}{\\partial \\psi^j} \\frac{\\partial \\L}{\\partial \\theta^o} \\cancel{- \\frac{\\partial^2 \\theta^o}{\\partial \\psi^i \\partial \\psi^j} \\frac{\\partial \\L}{\\partial \\theta^o}} \\\\\n      %\n      &= \\frac{\\partial \\theta^m}{\\partial \\psi^i} \\frac{\\partial \\theta^n}{\\partial \\psi^j} \\left( \\frac{\\partial^2 \\L}{\\partial \\theta^m \\partial \\theta^n} - \\Gamma_{mn}^o \\frac{\\partial \\L}{\\partial \\theta^o} \\right) \\\\\n      %\n      &= \\frac{\\partial \\theta^m}{\\partial \\psi^i} \\frac{\\partial \\theta^n}{\\partial \\psi^j} H_{mn} .\n\\end{align*}\n$$\n\nNote that while this transformation rule is very similar to the transformation of the \"calculus Hessian\" _at a critical point_, the transformation rule of the Riemannian Hessian applies everywhere on $\\Theta$.\n\n**This means, seen as a bilinear map, the Hessian is invariant _everywhere_ on $\\Theta$**. (Not just at the critical points as before.)\nHow does this discrepancy happen?\nThis is because we ignore $\\Gamma^k_{ij}$ in calculus!\nThis is, of course, justified since $\\Gamma^k_{ij} \\equiv 0$.\nBut as can be seen in its transformation rule, under a reparametrization $\\varphi$, this quantity is non-zero in general in $\\psi$ parametrization---this is already true for a simple, common transformation between the Cartesian and polar coordinates.\n\n## The Invariance of the Hessian Eigenvalues, Determinant, and Trace\n\nLet us focus on the determinant of the Hessian.\nAs discussed above, it is not invariant.\nThis is true even if the Riemannian Hessian above is used.\nHow do we make sense of this?\n\nTo make sense of this, we need to fully understand the object we care about when we talk about the determinant of the Hessian as a measure of the flatness of the loss landscape of $\\L$.\n\nThe loss landscape of $\\L$ is the _graph_ $\\\\{ (\\theta, \\L(\\theta)) \\in \\R^{d+1}: \\theta \\in \\Theta \\\\}$ of $\\L$.\nThis is actually a $d$-dimensional [hypersurface](https://en.wikipedia.org/wiki/Hypersurface) embedded in $\\R^{d+1}$.\nIn particular, a hypersurface is a manifold.\nMeanwhile, the concept of \"sharpness\" or \"flatness\" of the loss landscape of $\\L$ is nothing but the curvatures of the above manifold, particularly the principal curvatures, Gaussian curvature, and mean curvature.\n\nThese curvatures can actually be derived from the Hessian of $\\L$ since this Hessian is the second fundamental form of that manifold. (See that previous post!)\nHowever, to obtain those curvatures, we _must_ first derive the **_shape operator_** with the help of the metric. (The shape operator is a linear operator, mapping a vector to a vector.)\nSuppose the matrix representation of the metric on $\\Theta$ is $\\mathbf{G}$.\nThen, the shape operator $E$ is given by\n\n$$\n  \\mathbf{E} := \\mathbf{G}^{-1} \\mathbf{H} .\n$$\n\nThe principal, Gaussian, and mean curvatures of the loss landscape are then the eigenvalues, determinant, and trace of $\\mathbf{E}$, respectively.\nThe reason why we can simply take eigenvalues or determinant or trace of the Hessian $\\mathbf{H}$ in calculus is because, by default, $\\mathbf{G}$ is assumed to be the $d \\times d$ identity matrix $\\mathbf{I}$, i.e. the Euclidean metric.\nThat is $\\mathbf{E} = \\mathbf{H}$ and we can ignore the $\\mathbf{G}^{-1}$ term above.\n\nBut notice that under a reparametrization $\\varphi: \\theta \\to \\psi$, we have\n\n$$\n  \\mathbf{G} \\mapsto (\\mathbf{J}^{-1})^\\top \\mathbf{G} \\mathbf{J}^{-1} .\n$$\n\nSo, even when $\\mathbf{G} \\equiv \\mathbf{I}$ in the $\\theta$ parametrization, the matrix representation of the metric is different than $\\mathbf{I}$ in the $\\psi$ parametrization!\nThat is, we _must not_ ignore the metric in the shape operator, however trivial it might be, if we care about reparametrization.\n_This is the cause of the non-invariance of the Hessian's eigenvalues, determinant, and trace observed in deep learning!_\n\nFirst, let us see the transformation of the shape operator by combining the transformation rules of $\\mathbf{G}$ and $\\mathbf{H}$:\n\n$$\n\\begin{align}\n  \\tilde{\\mathbf{E}} &= \\tilde{\\mathbf{G}}^{-1} \\tilde{\\mathbf{H}} \\\\\n    %\n    &= ((\\mathbf{J}^{-1})^\\top \\mathbf{G} \\mathbf{J}^{-1})^{-1} (\\mathbf{J}^{-1})^\\top \\mathbf{H} \\mathbf{J}^{-1} \\\\\n    %\n    &= \\mathbf{J} \\mathbf{G}^{-1} \\cancel{\\mathbf{J}^\\top} \\cancel{\\mathbf{J}^{-\\top}} \\mathbf{H} \\mathbf{J}^{-1} \\\\\n    %\n    &= \\mathbf{J} \\mathbf{G}^{-1} \\mathbf{H} \\mathbf{J}^{-1} \\\\\n    %\n    &= \\mathbf{J} \\mathbf{E} \\mathbf{J}^{-1} .\n\\end{align}\n$$\n\nIf we take the determinant of both sides, we have:\n\n$$\n  \\det \\tilde{\\mathbf{E}} = \\cancel{(\\det \\mathbf{J})} \\cancel{(\\det \\mathbf{J})^{-1}} (\\det \\mathbf{E}) = \\det \\mathbf{E} .\n$$\n\nThat is, **the determinant of the Hessian, seen as a shape operator, is invariant!**\n\nWhat about the trace of $\\mathbf{E}$?\nRecall that $\\tr{\\mathbf{A}\\mathbf{B}} = \\tr{\\mathbf{B}\\mathbf{A}}$.\nUsing this property and the transformation of $\\tilde{\\mathbf{E}}$ above, we have:\n\n$$\n\\begin{align}\n  \\mathrm{tr}\\, \\tilde{\\mathbf{E}} &= \\tr{\\mathbf{J} \\mathbf{E} \\mathbf{J}^{-1}} = \\tr{\\mathbf{J} \\mathbf{J}^{-1} \\mathbf{E}} = \\mathrm{tr}\\, \\mathbf{E} ,\n\\end{align}\n$$\n\nand so **the trace is also invariant**.\n\nFinally, we can also show a general invariance result for eigenvalues.\nRecall that $\\lambda$ is an eigenvalue of the linear operator $\\mathbf{E}$ if $\\mathbf{E} \\mathbf{v} = \\lambda \\mathbf{v}$ for an eigenvector $\\mathbf{v}$.\n\nLet $(\\lambda, \\mathbf{v})$ be an eigenpair on the $\\theta$ parametrization and $(\\tilde{\\lambda}, \\tilde{\\mathbf{v}})$ be an eigenpair on the $\\psi$ parametrization.\nWe want to show that $\\lambda = \\tilde{\\lambda}$.\nRecall vectors are transformed by multiplying it with the Jacobian of $\\varphi$.\nSo, $\\tilde{\\mathbf{v}} = \\mathbf{J} \\mathbf{v}$.\nTherefore:\n\n$$\n\\begin{align}\n  \\tilde{\\mathbf{E}} \\tilde{\\mathbf{v}} &= \\tilde{\\lambda} \\tilde{\\mathbf{v}} \\\\\n  %\n  \\mathbf{J} \\mathbf{E} \\cancel{\\mathbf{J}^{-1}} \\cancel{\\mathbf{J}} \\mathbf{v} &= \\tilde{\\lambda} \\mathbf{J} \\mathbf{v} \\\\\n  %\n  \\mathbf{J} \\mathbf{E} &= \\tilde{\\lambda} \\mathbf{J} \\mathbf{v} \\\\\n  %\n  \\mathbf{E} &= \\tilde{\\lambda} \\mathbf{v} ,\n\\end{align}\n$$\n\nwhere the last step is done by multiplying both sides by the inverse of the Jacobian---recall that $\\varphi$ is invertible.\n\nTherefore, we identify that $\\lambda = \\tilde\\lambda$.\nSince $\\lambda$ is an arbitrary eigenvalue, we conclude that **all eigenvalues of $\\mathbf{E}$ are invariant**.\n\n## Non-Invariance from the Tensor Analysis Viewpoint\n\nIn tensor analysis, this issue is very easy to identify.\nFirst, the Hessian represents a bilinear map, so it is a _covariant 2-tensor_.\nMeanwhile, when we talk about eigenvalues, we refer to the [spectral theorem](https://en.wikipedia.org/wiki/Spectral_theorem) and this theorem applies to _linear maps_.\nSo, there is a _type mismatch_ here.\n\nTo apply the spectral theorem on the Hessian, we need to express it as a linear map.\nThis can be done by viewing the Hessian as a linear map on the tangent space onto itself, which is a _1-contravariant 1-covariant tensor_.\nThat is, we need to \"raise\" one of the indices of $H$.\nHow do we do this?\nYou guessed it: Multiply $H$ with the inverse of the metric.\n\n## Conclusion\n\nThe reason why \"flatness measures\" derived from the calculus version of Hessian is not invariant is simply because we measure those \"flatness measures\" from an incorrect object.\nThe correct object we should use is the shape operator, which is obtained with the help of the metric (even when the latter is Euclidean).\n\nMoreover, the reason why Newton's method is not invariant (see Sec. 12 of [Martens, 2020](https://arxiv.org/abs/1412.1193)) is that we ignore the second term involving the connection coefficient $\\Gamma$.\n\nIgnoring those geometric quantities are totally justified in calculus and deep learning since we always assume a Euclidean metric along with the Cartesian coordinates.\nBut this simplification makes us \"forget\" about the correct transformation of the Hessian, giving rise to the pathological non-invariance issues observed in deep learning.","src/content/post/hessian-invariance.mdx","495aac47e6a4ef3c","hessian-invariance.mdx","hessian-curvatures",{"id":309,"data":311,"body":316,"filePath":317,"digest":318,"legacyId":319,"deferredRender":23},{"title":312,"description":313,"publishDate":314,"draft":15,"tags":315},"Hessian and Curvatures in Machine Learning: A Differential-Geometric View","In machine learning, especially in neural networks, the Hessian matrix is often treated synonymously with curvatures. But, from calculus alone, it is not clear why can one say so. Here, we will view the loss landscape of a neural network as a hypersurface and apply a differential-geometric analysis on it.",["Date","2020-11-02T17:00:00.000Z"],[79],"import BlogImage from \"@/components/BlogImage.astro\";\n\nIn machine learning, especially in neural networks, the Hessian matrix is often treated synonymously with curvatures, in the following sense. Suppose $f: \\R^n \\times \\R^d \\to \\R$ defined by $(x, \\theta) \\mapsto f(x; \\theta) =: f_\\theta(x)$ is a (real-valued) neural network, mapping an input $x$ to the output $f(x; \\theta)$ under the parameter $\\theta$. Given a dataset $\\D$, we can define a loss function $\\ell: \\R^d \\to \\R$ by $\\theta \\mapsto \\ell(\\theta)$ such as the mean-squared-error or cross-entropy loss. (We do not explicitly show the dependency of $\\ell$ to $f$ and $\\D$ for brevity.) Assuming the standard basis for $\\R^d$, from calculus we know that the second partial derivatives of $\\ell$ at a point $\\theta \\in \\R^d$ form a matrix called the Hessian matrix at $\\theta$.\n\nOften, one calls the Hessian matrix the \"curvature matrix\" of $L$ at $\\theta$ [1, 2, etc.]. Indeed, it is well-justified since as we have learned in calculus, the eigenspectrum of this Hessian matrix represents the curvatures of the _loss landscape_ of $\\ell$ at $\\theta$. It is, however, not clear from calculus alone what is the precise geometric meaning of these curvatures. In this post, we will use tools from differential geometry---especially the hypersurface theory---to study the geometric interpretation of the Hessian matrix.\n\n## Loss Landscapes as Hypersurfaces\n\nWe begin by formalizing what exactly is a _loss landscape_ via the Euclidean hypersurface theory. We call an $n$-dimensional manifold $M$ a **_(Euclidean) hypersurface_** of $\\R^{n+1}$ if $M$ is a subset of $\\R^{n+1}$ (equipped with the standard basis) and the inclusion $\\iota: M \\hookrightarrow \\R^{n+1}$ is a smooth topological embedding. Since $\\R^{n+1}$ is equipped with a metric in the form of the standard dot product, we can equip $M$ with an induced metric characterized at each point $p \\in M$ by\n\n$$\n    \\langle v, w\\rangle_p = (d\\iota)_p(v) \\cdot (d\\iota)_p(w) ,\n$$\n\nfor all tangent vectors $v, w \\in T_pM$. Here, $\\cdot$ represents the dot product and $(d\\iota)_p: T_pM \\to T_{\\iota(p)}\\R^{n+1} \\simeq \\R^{n+1}$ is the differential of $\\iota$ at $p$ which is represented by the Jacobian matrix of $\\iota$ at $p$. In matrix notation this is\n\n$$\n    \\inner{v, w}_p = (J_p v)^\\top (J_p w) .\n$$\n\nIntuitively, the induced inner product on $M$ at $p$ is obtained by \"pushing forward\" tangent vectors $v$ and $w$ using the Jacobian $J_p$ at $p$ and compute their dot product on $\\R^{n+1}$.\n\n\u003CBlogImage\n  imagePath='/img/hessian-curvatures/pushforward.png'\n  altText='Pushforward.'\n  fullWidth\n/>\n\nLet $g: U \\to \\R$ is a smooth real-valued function over an open subset $U \\subseteq \\R^n$, then the **_graph_** of $g$ is the subset $M := \\\\{ (u, g(u)) : u \\in  U \\\\} \\subseteq \\R^{n+1}$ which is a hypersurface in $\\R^{n+1}$. In this case, we can describe $M$ via the so-called **_graph parametrization_** which is a function $X: U \\to \\R^{n+1}$ defined by $X(u) := (u, g(u))$.\n\nComing back to our neural network setting, assuming that the loss $\\ell$ is smooth, the graph $L := \\\\{ (\\theta, \\ell(\\theta)) : \\theta \\in \\R^d \\\\}$ is a Euclidean hypersurface of $\\R^{d+1}$ with parametrization $Z: \\R^d \\to \\R^{d+1}$ defined by $Z(\\theta) := (\\theta, \\ell(\\theta))$. Furthermore, the metric of $L$ is given by the Jacobian of the parametrization $Z$ and the standard dot product on $\\R^{d+1}$, as before. Thus, the loss landscape of $\\ell$ can indeed be amenable to geometric analysis.\n\n\u003CBlogImage\n  imagePath='/img/hessian-curvatures/graph_hypersurface.png'\n  altText='The graph of a function as a hypersurface.'\n  fullWidth\n/>\n\n## The Second Fundamental Form and Shape Operator\n\nConsider vector fields $X$ and $Y$ on the hypersurface $L \\subseteq \\R^{d+1}$. We can view them as vector fields on $\\R^{d+1}$ and thus the directional derivative $\\nabla_X Y$ on $\\R^{d+1}$ is well-defined at all points in $L$. That is, at every $p \\in L$, $\\nabla_X Y$ is a $(d+1)$-dimensional vector \"rooted\" at $p$. This vector can be decomposed as follows:\n\n$$\n    \\nabla_X Y = (\\nabla_X Y)^\\top + (\\nabla_X Y)^\\perp ,\n$$\n\nwhere $(\\cdot)^\\top$ and $(\\cdot)^\\perp$ are the orthogonal projection operators onto the tangent/normal space of $L$ at $p$. We define the **_second fundamental form_** as the map $\\mathrm{II}$ that takes two vector fields on $L$ and yielding normal vector fields of $L$, as follows:\n\n$$\n    \\mathrm{II}(X,Y) := (\\nabla_X Y)^\\perp .\n$$\n\nSee the following figure for an intuition.\n\n\u003CBlogImage\n  imagePath='/img/hessian-curvatures/II.png'\n  altText='The second fundamental form.'\n/>\n\nSince $L$ is a $d$-dimensional hypersurface of $(d+1)$-dimensional Euclidean space, the normal space $N_pL$ at each point $p$ of $L$ has dimension one, and there exist only two ways of choosing a unit vector field normal to $L$. Any choice of the unit vector field thus automatically gives a basis for $N_pL$ for all $p \\in L$. One of the choices is the following normal vector field which is oriented _outward_ relative to $L$.\n\n\u003CBlogImage\n  imagePath='/img/hessian-curvatures/unit_normal.png'\n  altText='A unit normal vector field.'\n/>\n\nAnother choice is the same unit normal field but oriented _inward_ relative to $L$.\n\nFix a unit normal field $N$. We can replace the vector-valued second fundamental form $\\mathrm{II}$ with a simpler scalar-valued form. We define the **_scalar second fundamental form_** of $M$ to be\n\n$$\n    h(X, Y) := \\inner{N, \\mathrm{II}(X,Y)} .\n$$\n\nFurthermore, we define the **_shape operator_** of $L$ as the map $s$, mapping a vector field to another vector field on $L$, characterized by\n\n$$\n    \\inner{s(X), Y} = h(X,Y) .\n$$\n\nBased on the characterization above, we can alternatively view $s$ as an operator obtained by raising an index of $h$, i.e. multiplying the matrix of $h$ with the inverse-metric.\n\nNote that, at each point $p \\in L$, the shape operator at $p$ is a linear endomorphism of $T_p L$, i.e. it defines a map from the tangent space to itself. Furthermore, we can show that $\\mathrm{II}(X,Y) = \\mathrm{II}(Y,X)$ and thus $h(X,Y)$ is symmetric. This implies that $s$ is self-adjoint since we can write\n\n$$\n    \\inner{s(X), Y} = h(X,Y) = h(Y,X) = \\inner{s(Y), X} = \\inner{X, s(Y)} .\n$$\n\nAltogether, this means that at each $p \\in L$, the shape operator at $p$ can be represented by a symmetric $d \\times d$ matrix.\n\n## Principal Curvatures\n\nThe previous fact about the matrix of $s$ says that we can apply eigendecomposition on $s$ and obtain $n$ real eigenvalues $\\kappa_1, \\dots, \\kappa_n$ and an orthonormal basis for $T_p L$ formed by the eigenvectors $(b_1, \\dots, b_n)$ corresponding to these eigenvalues. We call these eigenvalues the **_principal curvatures_** of $L$ at $p$ and the corresponding eigenvectors the **_principal directions_**. Moreover, we also define the **_Gaussian curvature_** as $\\det s = \\prod_{i=1}^d \\kappa_i$ and the **_mean curvature_** as $\\frac{1}{d} \\mathrm{tr}\\,s = \\frac{1}{d} \\sum_{i=1}^d \\kappa_i$.\n\n\u003CBlogImage\n  imagePath='/img/hessian-curvatures/curvature_plane_curv.png'\n  altText='Principal curvatures.'\n/>\n\nThe intuition of the principal curvatures and directions in $\\R^3$ is shown in the preceding figure. Suppose $M$ is a surface in $\\R^3$. Choose a tangent vector $v \\in T_pM$. Together with the choice of our unit normal vector $N_p$ at $p$, we obtain a plane $\\varPi$ passing through $p$. The intersection of $\\varPi$ and the neighborhood of $p$ in $M$ is a plane curve $\\gamma \\subseteq \\varPi$ containing $p$. We can now compute the curvature of this curve at $p$ as usual, in the calculus sense (the reciprocal of the radius of the osculating circle at $p$). Then, the principal curvatures of $M$ at $p$ are the minimum and maximum curvatures obtained this way. The corresponding vectors in $T_p M$ that attain the minimum and maximum are the principal directions.\n\nPrincipal and mean curvatures are not intrinsic to a hypersurface. There are two hypersurfaces that are isometric, but have different principal curvatures and hence different mean curvatures. Consider the following two surfaces.\n\n\u003CBlogImage\n  imagePath='/img/hessian-curvatures/principal_curvatures_extrinsic.png'\n  altText='Principal curvatures are extrinsic.'\n  fullWidth\n/>\n\nThe first (left) surface is the plane described by the parametrization $(x,y) \\mapsto \\\\{ x, y, 0 \\\\}$ for $0 \u003C x \u003C \\pi$ and $0 \u003C y \u003C \\pi$. The second one is the half cylinder $(x,y) \\mapsto \\\\{ x, y, \\sqrt{1-y^2} \\\\}$ for $0 \u003C x \u003C \\pi$ and $\\abs{y} \u003C 1$. It is clear that they have different principal curvatures since the plane is a flat while the half-cylinder is \"curvy\". Indeed, assuming a downward pointing normal, we can see that $\\kappa_1 = \\kappa_2 = 0$ for the plane and $\\kappa_1 = 0, \\kappa_2 = 1$ for the half-cylinder and thus their mean curvatures differ. However, they are actually isometric to each other---from the point of view of Riemannian geometry, they are the same. Thus, both principal and mean curvatures depend on the choice of the parametrization and not intrinsic.\n\nRemarkably, the Gaussian curvature is intrinsic: All isometric hypersurfaces of dimension $\\geq 2$ have the same Gaussian curvature (up to sign). Using the previous example: the plane and half-cylinder have the same Gaussian curvature of $0$. In 2D surfaces, this is a classic result which Gauss named _Theorema Egregium_. For hypersurfaces with dimension $> 2$, it can be shown that the Gaussian curvature is intrinsic up to sign [5, Ch. 7, Cor. 23].\n\n## The Loss Landscape's Hessian\n\nNow we are ready to draw a geometric connection between principal curvatures and the Hessian of $\\ell$. Let $Z: \\R^d \\to \\R^{d+1}$ be graph parametrization of the loss landscape $L$. The coordinates $(\\theta^1, \\dots, \\theta^d) \\in \\R^d$ thus give local coordinates for $L$. The coordinate vector field $\\partial/\\partial \\theta^1, \\dots, \\partial/\\partial \\theta^d$, push forward to vector fields $dZ(\\partial/\\partial \\theta^1), \\dots, dZ(\\partial/\\partial \\theta^d)$ on $\\R^{d+1}$, via the Jacobian of $Z$. At each $p \\in L$, these vector fields form a basis for $T_p L$, viewed as a collection of $d$ vectors in $\\R^{d+1}$.\n\nIf we think of $Z(\\theta) = (Z^1(\\theta), \\dots, Z^{d+1}(\\theta))$ as a vector-valued function of $\\theta$, then by definition of Jacobian, these push-forwarded coordinate vector fields can be written for every $\\theta \\in \\R^d$ as\n\n$$\n    dZ_\\theta \\left( \\frac{\\partial}{\\partial \\theta^i} \\right) = \\frac{\\partial Z}{\\partial \\theta^i} (\\theta) =: \\partial_i Z(\\theta) ,\n$$\n\nfor each $i = 1, \\dots, d$.\n\nLet us suppose we are given a unit normal field to $L$. Then we have the following result.\n\n**Proposition 1.** _Suppose $L \\subseteq \\R^{d+1}$ is the loss landscape of $\\ell$, $Z: \\R^d \\to \\R^{d+1}$ is the graph parametrization of $L$. Suppose further that $\\partial_1 Z, \\dots, \\partial_d Z$ are the vector fields determined by $Z$ which restriction at each $p \\in L$ is a basis for $T_pL$, and suppose $N$ is a unit normal field on $L$. Then the scalar second fundamental form is given by_\n\n$$\n    h(\\partial_i Z, \\partial_j Z) = \\left\\langle \\frac{\\partial^2 Z}{\\partial \\theta^i \\partial \\theta^d} , N \\right\\rangle = N^{d+1} \\frac{\\partial^2 \\ell}{\\partial \\theta^i \\partial \\theta^j}.\n$$\n\n_Where $N^{d+1}$ is the last component of the unit normal field._\n\n_Proof._ To show the first equality, one can refer to Proposition 8.23 in [1], which works for any parametrization and not just the graph parametrization. Now recall that $Z(\\theta) = (\\theta^1, \\dots, \\theta^d, \\ell(\\theta^1, \\dots, \\theta^d))$. Therefore for each $i = 1, \\dots, d$:\n\n$$\n    \\frac{\\partial Z}{\\partial \\theta^i} = \\left( 0, \\dots, 1, \\dots, \\frac{\\partial \\ell}{\\partial \\theta^i} \\right) ,\n$$\n\nand thus\n\n$$\n    \\frac{\\partial^2 Z}{\\partial \\theta^i \\partial \\theta^j} = \\left( 0, \\dots, 0, \\frac{\\partial^2 \\ell}{\\partial \\theta^i \\partial \\theta^j} \\right) .\n$$\n\nTaking the inner product with the unit normal field $N$, we obtain\n\n$$\n    h(\\partial_i Z, \\partial_j Z) = 0 + \\dots + 0 + N^{d+1} \\frac{\\partial^2 \\ell}{\\partial \\theta^i \\partial \\theta^j} = N^{d+1} \\frac{\\partial^2 \\ell}{\\partial \\theta^i \\partial \\theta^j} ,\n$$\n\nwhere $N^{d+1}$ is the $(d+1)$-st component function (it is a function $\\R^{d+1} \\to \\R$) of the normal field $N$. At each $p \\in L$, the matrix of $h$ is therefore $N^{d+1}(p)$ times the Hessian matrix of $\\ell$ at $p$.\n\n$$\n\\qed\n$$\n\nFinally, we show the connection between the principal curvatures with the scalar second fundamental form, and hence the principal curvatures with the Hessian. The following proposition says that at a critical point, the unit normal vector can be chosen as $(0, \\dots, 0, 1)$ and thus the scalar second fundamental form coincides with the Hessian of $\\ell$. Furthermore, by orthonormalizing the basis for the tangent space at that point, we can show that the matrix of the scalar second fundamental form in this case is exactly the matrix of the shape operator at $p$ and thus the Hessian encodes the principal curvatures at that point.\n\n**Proposition 2.** \\_Suppose $L \\subseteq \\R^{d+1}$ is a loss landscape with its graph parametrization and let $\\theta__ \\in \\R^d$ be a critical point of $\\ell$ and $p\\__ := (\\theta*\\*^1, \\dots, \\theta*_^d, \\ell(\\theta\\__)) \\in L$. Then the matrix of the shape operator $s$ of $L$ at $p_*$ is equal to the Hessian matrix of $\\ell$ at $\\theta_*$.\\_\n\n_Proof._ We can assume w.l.o.g. that the basis $(E_1, \\dots, E_d)$ for $T_{p_\\*} L$ is orthonormal by applying the Gram-Schmidt algorithm on $d$ linearly independent tangent vectors in $T_{p_\\*} L$. Furthermore pick $(0, \\dots, 0, 1) \\in \\R^{d+1}$ as the choice of the unit normal $N$ at $p_\\*$. We can do so since by hypothesis $p_\\*$ is a critical point and therefore $(0, \\dots, 0, 1)$ is perpendicular to $T_{p_\\*} L$.\n\nIt follows by Proposition 1 that the matrix of the scalar second fundamental form $h$ of $L$ at $p_\\*$ is equal to the Hessian matrix of $\\ell$ at $\\theta_\\*$. Moreover, since we have an orthonormal basis for $T_{p_\\*} L$, the metric of $L$ at $p_\\*$ is represented by the $d \\times d$ diagonal matrix. This implies that the matrix of the shape operator at $p_\\*$ is equal to the matrix of the second fundamental form and the claim follows directly.\n\n$$\n\\qed\n$$\n\nAs a side note, we can actually have a more general statement: At any point in a hypersurface with any parametrization, the principal curvatures give a concise description of the local shape of the hypersurface by approximating it with the graph of a quadratic function. See Prop. 8.24 in [3] for a detailed discussion.\n\n## Flatness and Generalization\n\nIn deep learning, there have been interesting works connecting the \"flatness\" of the loss landscape's local minima with the generalization performance of an NN. The conjecture is that the flatter a minimum is, the better the network generalizes. \"Flatness\" here often refers to the eigenvalues or trace of the Hessian matrix at the minima. However, this has been disputed by e.g. [4] and rightly so.\n\nAs we have seen previously, at a minimum, the principal and mean curvature (the eigenvalues and trace of the Hessian of $\\ell$, resp.) are not intrinsic. Different parametrization of $L$ can yield different principal and mean curvatures. Just like the illustration with the plane and the half-cylinder above, [4] illustrates this directly in the loss landscape. In particular, we can apply a bijective transformation $\\varphi$ to the original parameter space $\\R^d$ s.t. the resulting loss landscape is isometric to the original loss landscape and the particular minimum $\\theta_\\*$ does not change, i.e. $\\varphi(\\theta_\\*) = \\theta_\\*$. See the following figure for an illustration (we assume that the length of the red curves below is the same).\n\n\u003CBlogImage\n  imagePath='/img/hessian-curvatures/reparametrization_curvatures.png'\n  altText='Principal curvatures are not invariant under reparametrization.'\n  fullWidth\n/>\n\nIt is clear that the principal curvature changes even though functionally, the NN still represents the same function. Thus, we cannot actually connect the notion of \"flatness\" that are common in literature to the generalization ability of the NN. A definitive connection between them must start with some intrinsic notion of flatness---for starter, the Gaussian curvature, which can be easily computed since it is just the determinant of the Hessian at the minima.\n\n## References\n\n1. Martens, James. \"New Insights and Perspectives on the Natural Gradient Method.\" arXiv preprint arXiv:1412.1193 (2014).\n2. Dangel, Felix, Stefan Harmeling, and Philipp Hennig. \"Modular Block-diagonal Curvature Approximations for Feedforward Architectures.\" AISTATS. 2020.\n3. Lee, John M. Riemannian manifolds: an introduction to curvature. Vol. 176. Springer Science & Business Media, 2006.\n4. Dinh, Laurent, et al. \"Sharp Minima can Generalize for Deep Nets.\" ICML, 2017.\n5. Spivak, Michael D. A comprehensive introduction to differential geometry. Publish or perish, 1970.","src/content/post/hessian-curvatures.mdx","29c4c9edb1ce064a","hessian-curvatures.mdx","infogan",{"id":320,"data":322,"body":327,"filePath":328,"digest":329,"legacyId":330,"deferredRender":23},{"title":323,"description":324,"publishDate":325,"draft":15,"tags":326},"InfoGAN: unsupervised conditional GAN in TensorFlow and Pytorch","Adding Mutual Information regularization to a GAN turns out gives us a very nice effect: learning data representation and its properties in unsupervised manner.",["Date","2017-01-29T12:46:00.000Z"],[17,56],"import BlogImage from \"@/components/BlogImage.astro\";\n\nGenerative Adversarial Networks (GAN) is one of the most exciting generative models in recent years. The idea behind it is to learn generative distribution of data through two-player minimax game, i.e. the objective is to find the Nash Equilibrium. For more about the intuition and implementation of GAN, please see my previous post about GAN and CGAN.\n\nNote, the TensorFlow and Pytorch code could be found here: https://github.com/wiseodd/generative-models.\n\nOne natural extension of GAN is to learn a conditional generative distribution. The conditional could be anything, e.g. class label or even another image.\n\nHowever, we need to provide those conditionals manually, somewhat similar to supervised learning. InfoGAN, therefore, attempted to make this conditional learned automatically, instead of telling GAN what that is.\n\n## InfoGAN intuition\n\nRecall, in CGAN, the generator network has an additional parameter: $c$, i.e. $G(z, c)$, where $c$ is a conditional variable. During training, $G$ will learn the conditional distribution of data $P(X \\vert z, c)$. Although principally what CGAN and InfoGAN learn is the same distribution: $P(X \\vert z, c)$, what different is how they see $c$.\n\nIn CGAN, $c$ is assumed to be semantically known, e.g. labels, so during training we have to supply it. In InfoGAN we assume $c$ to be unknown, so what we do instead is to put a prior for $c$ and infer it based on the data, i.e. we want to find posterior $P(c \\vert X)$.\n\nAs $c$ in InfoGAN is inferred automatically, InfoGAN could assign it to anything related to the distribution of data, depending to the choice of the prior. For example, although we could not specify what $c$ should encodes, we could hope that InfoGAN captures label information into it by assigning a Categorical prior. Another example, if we assign a Gaussian prior for $c$, InfoGAN might assign a continuous propery for $c$, e.g. rotation angle.\n\nSo how does InfoGAN do that? This is when information theory takes part.\n\nIn information theory, if we want to express the knowledge about something if we know something else, we could use mutual information. So, if we maximize mutual information, we could find something that could contribute to the knowledge of another something the most. In our case, we want to maximize the knowledge about our conditional variable $c$, if we know $X$.\n\nThe InfoGAN mutual information loss is formulated as follows:\n\n$$\n    I(c, G(z, c)) = \\mathbb{E}_{c \\sim P(c), x \\sim G(z, c)} \\left[ \\log Q(c \\vert X) \\right] + H(c)\n$$\n\nwhere $H(c)$ is the entropy of the prior $P(c)$, $G(z, c)$ is the generator net, and $Q(c \\vert X)$ is a neural net that takes image input and producing the conditional $c$. $Q(c \\vert X)$ is a variational distribution to model the posterior $P(c \\vert X)$, which we do not know and as in any Bayesian inference, it is often hard to compute.\n\nThis mutual information term fits in the overall GAN loss as a regularization:\n\n$$\n    \\min*{G} \\max*{D} \\, V(D, G) - \\lambda I(c, G(z, c))\n$$\n\nwhere $V(D, G)$ is GAN loss.\n\n## InfoGAN training\n\nDuring training, we provide a prior $P(c)$, which could be any distribution. In fact, we could add as many priors as we want, and InfoGAN might assign different properties to them. The author of InfoGAN called this as \"disentangled representations\", as it kind of breaking down the properties of data into several conditional parameters.\n\nThe training process is similar for discriminator net $D(X)$ and generator net $G(z, c)$ is quite similar to CGAN, which could be read further here. The differences, however, are:\n\n- instead of $D(X, c)$, we use discriminator as in vanilla GAN: $D(X)$, i.e. unconditional discriminator,\n- instead of feeding observed data for the $c$, e.g. labels, into $G(z, c)$, we sample $c$ from prior $P(c)$.\n\nIn addition to $D(X)$ and $G(z, c)$, we also train $Q(c \\vert X)$ so that we could compute the mutual information. What we do is to sample $c \\sim P(c)$ and use it to sample $X \\sim G(z, c)$ and finally pass it to $Q(c \\vert X)$. The result, along with prior $P(c)$ are used to compute the mutual information. The mutual information is then backpropagated to both $G$ and $Q$ to update both networks so that we could maximize the mutual information.\n\n## InfoGAN implementation in TensorFlow\n\nThe implementation for vanilla and conditional GAN could be found here: GAN, CGAN. We will focus on the additional implementation for InfoGAN in this section.\n\nWe will implement InfoGAN for MNIST data, with $c$ categorically distributed, i.e. one-hot vector with ten elements.\n\nAs seen in the loss function of InfoGAN, we need one additional network, $Q(c \\vert X)$:\n\n```python\nQ_W1 = tf.Variable(xavier_init([784, 128]))\nQ_b1 = tf.Variable(tf.zeros(shape=[128]))\n\nQ_W2 = tf.Variable(xavier_init([128, 10]))\nQ_b2 = tf.Variable(tf.zeros(shape=[10]))\n\ntheta_Q = [Q_W1, Q_W2, Q_b1, Q_b2]\n\ndef Q(x):\n    Q_h1 = tf.nn.relu(tf.matmul(x, Q_W1) + Q_b1)\n    Q_prob = tf.nn.softmax(tf.matmul(Q_h1, Q_W2) + Q_b2)\n    return Q_prob\n```\n\nthat is, we model $Q(c \\vert X)$ as a two-layer net with softmax on top. The choice of softmax is because $c$ is categorically distributed, and softmax could pose as its parameter. If we choose $c$ to be Gaussian, then we could design the network so that the outputs are mean and variance.\n\nNext, we specify our prior:\n\n```python\ndef sample_c(m):\n    return np.random.multinomial(1, 10*[0.1], size=m)\n```\n\nwhich is a categorical distribution, with equal probability for each of the ten elements.\n\nAs training $D$ and $G$ is not different than vanila GAN and CGAN, we will omit it from this section. To train $Q$, as seen in the regularization term above, we first sample $c$ from $P(c)$, and use it to sample $X$ from $Q(X \\vert z, c)$:\n\n```python\nG_sample = generator(Z, c)\nQ_c_given_x = Q(G_sample)\n```\n\nduring runtime, we will populate $c$ with values from `sample_c()`.\n\nHaving all ingredients in hands, we could compute the mutual information term, which is the conditional entropy of the prior and our variational distribution, plus the entropy of our prior. Observe however, our prior is a fixed distribution, thus the entropy will be constant and can be left out.\n\n```python\ncond_ent = tf.reduce_mean(-tf.reduce_sum(tf.log(Q_c_given_x + 1e-8) \\* c, 1))\nQ_loss = cond_ent\n```\n\nThen, we optimize both $G$ and $Q$, based on that:\n\n```python\nQ_solver = tf.train.AdamOptimizer().minimize(Q_loss, var_list=theta_G + theta_Q)\n```\n\nWe initialized the training as follows:\n\n```python\nfor it in range(1000000):\n    \"\"\" Sample X_real, z, and c from priors \"\"\"\n    X_mb, _ = mnist.train.next_batch(mb_size)\n    Z_noise = sample_Z(mb_size, Z_dim)\n    c_noise = sample_c(mb_size)\n\n    \"\"\" Optimize D \"\"\"\n    _, D_loss_curr = sess.run([D_solver, D_loss],\n                              feed_dict={X: X_mb, Z: Z_noise, c: c_noise})\n\n    \"\"\" Optimize G \"\"\"\n    _, G_loss_curr = sess.run([G_solver, G_loss],\n                              feed_dict={Z: Z_noise, c: c_noise})\n\n    \"\"\" Optimize Q \"\"\"\n    sess.run([Q_solver], feed_dict={Z: Z_noise, c: c_noise})\n```\n\nAfter training, we could see what property our prior $c$ encodes. In this experiment, our $c$ will encode label property nicely, i.e. if we pass `c = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]`, we might get this:\n\n\u003CBlogImage imagePath='/img/infogan/02.png' altText='InfoGAN samples.' />\n\nNote, naturally, there is no guarantee on the ordering of $c$.\n\nWe could try different values for $c$:\n\n\u003CBlogImage imagePath='/img/infogan/03.png' altText='InfoGAN samples.' />\n\n\u003CBlogImage imagePath='/img/infogan/01.png' altText='InfoGAN samples.' />\n\nWe could see that our implementation of InfoGAN could capture the conditional variable, which in this case is the labels, in unsupervised manner.\n\n## Conclusion\n\nIn this post we learned the intuition of InfoGAN: a conditional GAN trained in unsupervised manner.\n\nWe saw that InfoGAN learns to map the prior $P(c)$, together with noise prior $P(z)$ into data distribution $P(X \\vert z, c)$ by adding maximization of the mutual information between $c$ and $X$ into GAN training. The rationale is that at the maximum mutual information between those two, they can explain each other well, e.g. $c$ could explain why $X \\sim P(X \\vert z, c=c)$ are a images of the same digit.\n\nWe also implemented InfoGAN in TensorFlow, which as we saw, it is a simple modification from the original GAN and CGAN.\n\nThe full code, both TensorFlow and Pytorch implementations are available in: https://github.com/wiseodd/generative-models.\n\n## References\n\n1. Chen, Xi, et al. \"Infogan: Interpretable representation learning by information maximizing generative adversarial nets.\" Advances in Neural Information Processing Systems. 2016.","src/content/post/infogan.mdx","5848998e46462cc1","infogan.mdx","jekyll-fb-share",{"id":331,"data":333,"body":339,"filePath":340,"digest":341,"legacyId":342,"deferredRender":23},{"title":334,"description":335,"publishDate":336,"draft":15,"tags":337},"How to Use Specific Image and Description when Sharing Jekyll Post to Facebook","Normally, random subset of pictures and the site's description will be picked when we shared our Jekyll blog post URL to Facebook. This is how to force Facebook to use the specific image and description for our blog post!",["Date","2016-08-15T17:52:00.000Z"],[42,338],"jekyll","import BlogImage from \"@/components/BlogImage.astro\";\n\nOften times, when we share our blog post to Facebook, disappointments will arise. Our hard work reduced to zero just because Facebook picks undesireable image and description. Our perfectly written and beautifully arranged blog post looks really bad in the Facebook post... The horror!\n\nHowever, there is a quick remedy for that, and if you're using Jekyll, you just need to modify once, and forget about it for the rest of your life. Here's how!\n\nFirst, check the front matter format of your blog post's markdown. Here's mine for example:\n\n```\n---\nlayout:     post\ntitle:      \"Deriving LSTM Gradient for Backpropagation\"\nsubtitle:   \"Deriving neuralnet gradient is an absolutely great exercise to understand backpropagation and computational graph better. In this post we will walk through the process of deriving LSTM net gradient so that we can use it in backpropagation.\"\ndate:       2016-08-12 12:34\ntags:       [machine learning, programming, python, neural networks, rnn, lstm]\n---\n```\n\nSo, in my case, I want the image and the description that are displayed in my Facebook post to be the value of `header-img` and `subtitle` variables respectively. You can use any variable though, as this is just an example.\n\nNow, open your `_include/head.html` template, and this piece of code inside the `\u003Chead>` tag.\n\n```html\n{% if page.header-img %}\n\u003Cmeta property=\"og:image\" content=\"{{ site.url }}/{{ page.header-img }}\" />\n{% endif %}\n\n\u003Cmeta property=\"og:description\" content=\"{{ page.subtitle }}\" />\n```\n\nAnd voila! The next time you share your blog post to Facebook, the post's `header-img` and `subtitle` will be used!\n\n\u003CBlogImage imagePath='/img/jekyll-fb-share/share.png' />\n\nTo update the already shared posts so that they follow the above schema, you should head to https://developers.facebook.com/tools/debug/, enter your URL, click \"Debug\", and click \"Scrape Again\".\n\nNow, try to re-share the post. It should be updated now.\n\nWe have to do this because Facebook caches the scraped URL. It is especially true if the URL is somewhat recently shared.\n\nCredit due to front-end ninja http://timotiusnc.github.io!","src/content/post/jekyll-fb-share.mdx","918bd3350c1af6ee","jekyll-fb-share.mdx","kl-mle",{"id":343,"data":345,"body":351,"filePath":352,"digest":353,"legacyId":354,"deferredRender":23},{"title":346,"description":347,"publishDate":348,"draft":15,"tags":349},"Maximizing likelihood is equivalent to minimizing KL-Divergence","We will show that doing MLE is equivalent to minimizing the KL-Divergence between the estimator and the true distribution.",["Date","2017-01-26T08:53:00.000Z"],[17,350],"probability","When reading Kevin Murphy's book, I came across this statement:\n\n> ... maximizing likelihood is equivalent to minimizing $D_{KL}[P(. \\vert \\theta^*) \\, \\Vert \\, P(. \\vert \\theta)]$, where $P(. \\vert \\theta^*)$ is the true distribution and $P(. \\vert \\theta)$ is our estimate ...\n\nSo here is an attempt to prove that.\n\n$$\n\\begin{align}\n\nD_{KL}[P(x \\vert \\theta^*) \\, \\Vert \\, P(x \\vert \\theta)] &= \\mathbb{E}_{x \\sim P(x \\vert \\theta^*)}\\left[\\log \\frac{P(x \\vert \\theta^*)}{P(x \\vert \\theta)} \\right] \\\\[10pt]\n        &= \\mathbb{E}_{x \\sim P(x \\vert \\theta^*)}\\left[\\log \\, P(x \\vert \\theta^*) - \\log \\, P(x \\vert \\theta) \\right] \\\\[10pt]\n        &= \\mathbb{E}_{x \\sim P(x \\vert \\theta^*)}\\left[\\log \\, P(x \\vert \\theta^*) \\right] - \\mathbb{E}_{x \\sim P(x \\vert \\theta^*)}\\left[\\log \\, P(x \\vert \\theta) \\right] \\\\[10pt]\n\n\\end{align}\n$$\n\nIf it looks familiar, the left term is the entropy of $P(x \\vert \\theta^*)$. However it does not depend on the estimated parameter $\\theta$, so we will ignore that.\n\nSuppose we sample $N$ of these $x \\sim P(x \\vert \\theta^*)$. Then, the [Law of Large Number](https://en.wikipedia.org/wiki/Law_of_large_numbers) says that as $N$ goes to infinity:\n\n$$\n-\\frac{1}{N} \\sum_i^N \\log \\, P(x_i \\vert \\theta) = -\\mathbb{E}_{x \\sim P(x \\vert \\theta^*)}\\left[\\log \\, P(x \\vert \\theta) \\right]\n$$\n\nwhich is the right term of the above KL-Divergence. Notice that:\n\n$$\n\\begin{align}\n\n-\\frac{1}{N} \\sum_i^N \\log \\, P(x_i \\vert \\theta) &= \\frac{1}{N} \\, \\text{NLL} \\\\[10pt]\n                                                  &= c \\, \\text{NLL} \\\\[10pt]\n\n\\end{align}\n$$\n\nwhere NLL is the negative log-likelihood and $c$ is a constant.\n\nThen, if we minimize $D_{KL}[P(x \\vert \\theta^*) \\, \\Vert \\, P(x \\vert \\theta)]$, it is equivalent to minimizing the NLL. In other words, it is equivalent to maximizing the log-likelihood.\n\nWhy does this matter, though? Because this gives MLE a nice interpretation: maximizing the likelihood of data under our estimate is equal to minimizing the difference between our estimate and the real data distribution. We can see MLE as a proxy for fitting our estimate to the real distribution, which cannot be done directly as the real distribution is unknown to us.","src/content/post/kl-mle.mdx","9447282395f66a43","kl-mle.mdx","laplace",{"id":355,"data":357,"body":362,"filePath":363,"digest":364,"legacyId":365,"deferredRender":23},{"title":358,"description":359,"publishDate":360,"draft":15,"tags":361},"Modern Arts of Laplace Approximations","The Laplace approximation (LA) is a simple yet powerful class of methods for approximating intractable posteriors. Yet, it is largely forgotten in the Bayesian deep learning community. Here, we review the LA, and highlight a recent software library for applying LA to deep nets.",["Date","2021-10-27T04:00:00.000Z"],[215],"import BlogImage from \"@/components/BlogImage.astro\";\n\nLet $f: X \\times \\Theta \\to Y$ defined by $(x, \\theta) \\mapsto f_\\theta(x)$ be a neural network, where $X \\subseteq \\R^n$, $\\Theta \\subseteq \\R^d$, and $Y \\subseteq \\R^c$ be the input, parameter, and output spaces, respectively.\nGiven a dataset $\\D := \\\\{ (x_i, y_i) : x_i \\in X, y_i \\in Y \\\\}_{i=1}^m$, we define the likelihood $p(\\D \\mid \\theta) := \\prod_{i=1}^m p(y_i \\mid f_\\theta(x_i))$.\nThen, given a prior $p(\\theta)$, we can obtain the posterior via an application of Bayes' rule: $p(\\theta \\mid \\D) = 1/Z \\,\\, p(\\D \\mid \\theta) p(\\theta)$.\nBut, the exact computation of $p(\\theta \\mid \\D)$ is intractable in general due to the need of computing the normalization constant\n\n$$\n    Z = \\int_\\Theta p(\\D \\mid \\theta) p(\\theta) \\,d\\theta .\n$$\n\nWe must then approximate $p(\\theta \\mid \\D)$.\nOne simple way to do this is by simply finding one single likeliest point under the posterior, i.e. the mode of $p(\\theta \\mid \\D)$.\nThis can be done via optimization, instead of integration:\n\n$$\n    \\theta_\\map := \\argmax_\\theta \\sum_{i=1}^m \\log p(y_i \\mid f_\\theta(x_i)) + \\log p(\\theta) =: \\argmax_\\theta \\L(\\theta; \\D) .\n$$\n\nThe estimate $\\theta_\\map$ is referred to as the _maximum a posteriori_ (MAP) estimate.\nHowever, the MAP estimate does not capture the uncertainty around $\\theta$.\nThus, it often (and in some cases, e.g. [1], almost always) leads to overconfidence.\n\nIn the context of Bayesian neural networks, the Laplace approximation (LA) is a family of methods for obtaining a Gaussian approximate posterior distribution of networks' parameters.\nThe fact that it produces a Gaussian approximation is a step up from the MAP estimation: particularly, it conveys some notion of uncertainty in $\\theta$.\nLA stems from the early work of Pierre-Simon Laplace in 1774 [2] and it was first adapted for Bayesian neural networks (BNNs) by David MacKay in 1992 [3].\nThe method goes as follows.\n\nGiven the MAP estimate $\\theta_\\map$, let us Taylor-expand $\\L$ around $\\theta_\\map$ up to the second-order:\n\n$$\n    \\L(\\theta; \\D) \\approx \\L(\\theta_\\map; \\D) + \\frac{1}{2} (\\theta - \\theta_\\map)^\\top \\left(\\nabla^2_\\theta \\L\\vert_{\\theta_\\map}\\right) (\\theta - \\theta_\\map) .\n$$\n\n(Note that the gradient $\\nabla_\\theta \\L$ is zero at $\\theta_\\map$ since $\\theta_\\map$ is a critical point of $\\L$ and thus the first-order term in the above is also zero.)\nNow, recall that $\\L$ is the log-numerator of the posterior $p(\\theta \\mid \\D)$.\nThus, the r.h.s. of the above can be used to approximate the true numerator, by simply exponentiating it:\n\n$$\n\\begin{align*}\n    p(\\D \\mid \\theta)p(\\theta) &\\approx \\exp\\left( \\L(\\theta_\\map; \\D) + \\frac{1}{2} (\\theta - \\theta_\\map)^\\top \\left(\\nabla^2_\\theta \\L\\vert_{\\theta_\\map}\\right) (\\theta - \\theta_\\map) \\right) \\\\[5pt]\n        %\n        &= \\exp(\\L(\\theta_\\map; \\D)) \\exp\\left(\\frac{1}{2} (\\theta - \\theta_\\map)^\\top \\left(\\nabla^2_\\theta \\L\\vert_{\\theta_\\map}\\right) (\\theta - \\theta_\\map) \\right) .\n\\end{align*}\n$$\n\nFor simplicity, let $\\varSigma := -\\left(\\nabla^2_\\theta \\L\\vert_{\\theta_\\map}\\right)^{-1}$. Then, using this approximation, we can also obtain an approximation of $Z$:\n\n$$\n\\begin{align*}\n    Z &\\approx \\exp(\\L(\\theta_\\map; \\D)) \\int_\\theta  \\exp\\left(-\\frac{1}{2} (\\theta - \\theta_\\map)^\\top \\varSigma^{-1} (\\theta - \\theta_\\map) \\right) \\,d\\theta \\\\[5pt]\n        %\n        &= \\exp(\\L(\\theta_\\map; \\D)) (2\\pi)^{d/2} (\\det \\varSigma)^{1/2} ,\n\\end{align*}\n$$\n\nwhere the equality follows from the fact the integral above is the famous, tractable [Gaussian integral](https://en.wikipedia.org/wiki/Gaussian_integral).\nCombining both approximations, we obtain\n\n$$\n\\begin{align*}\n    p(\\theta \\mid \\D) &\\approx \\frac{1}{(2\\pi)^{d/2} (\\det \\varSigma)^{1/2}} \\exp\\left(-\\frac{1}{2} (\\theta - \\theta_\\map)^\\top \\varSigma^{-1} (\\theta - \\theta_\\map) \\right) \\\\[5pt]\n        %\n        &= \\N(\\theta \\mid \\theta_\\map, \\varSigma) .\n\\end{align*}\n$$\n\nThat is, we obtain a tractable, easy-to-work-with Gaussian approximation to the intractable posterior via a simple second-order Taylor expansion!\nMoreover, this is not just any Gaussian approximation: Notice that this Gaussian is fully determined once we have the MAP estimate $\\theta_\\map$.\nConsidering that the MAP estimation is _the_ standard procedure for training NNs, the LA is nothing but a simple post-training step on top of it.\nThis means the LA, unlike other approximate inference methods, is a _post-hoc_ method that can be applied to virtually any pre-trained NN, without the need of re-training!\n\nGiven this approximation, we can then use it as a proxy to the true posterior.\nFor instance, we can use it to obtain the predictive distribution\n\n$$\n\\begin{align*}\n    p(y \\mid x, \\D) &\\approx \\int_\\theta p(y \\mid f_\\theta(x)) \\, \\N(\\theta \\mid \\theta_\\map, \\varSigma) \\,d\\theta \\\\\n        %\n        &\\approx \\frac{1}{s} \\sum_{i=1}^s p(y \\mid f_\\theta(x)) \\qquad \\text{where} \\enspace \\theta_s \\sim \\N(\\theta \\mid \\theta_\\map, \\varSigma) ,\n\\end{align*}\n$$\n\nwhich in general is less overconfident compared to the MAP-estimate-induced predictive distribution [3].\n\nWhat we have seen is the most general framework of the LA.\nOne can make a specific design decision, such as by imposing a special structure to the Hessian $\\nabla^2_\\theta \\L$, and thus the covariance $\\varSigma$.\n\n## The laplace-torch library\n\nThe simplicity of the LA is not without a drawback.\nRecall that the parameter $\\theta$ is in $\\Theta \\subseteq \\R^d$.\nIn neural networks (NNs), $d$ is often in the order of millions or even billions.\nNaively computing the Hessian $\\nabla^2_\\theta \\L$ is thus often infeasible since it scales like $O(d^2)$.\nTogether with the fact that the LA is an old method (and thus not \"trendy\" in the (Bayesian) deep learning community), this might be the reason why the LA is not as popular as other BNN posterior approximation methods such as variational Bayes (VB) and Markov Chain Monte Carlo (MCMC).\n\nMotivated by this observation, in our NeurIPS 2021 paper titled [\"Laplace Redux -- Effortless Bayesian Deep Learning\"](https://arxiv.org/abs/2106.14806), we showcase that (i) the Hessian can be obtained cheaply, thanks to recent advances in second-order optimization, and (ii) even the simplest LA can be competitive to more sophisticated VB and MCMC methods, while only being much cheaper than them.\nOf course, numbers alone are not sufficient to promote the goodness of the LA.\nSo, in that paper, we also propose an extendible, easy-to-use software library for PyTorch called `laplace-torch`, which is available at [this Github repo](https://github.com/AlexImmer/Laplace).\n\nThe `laplace-torch` is a simple library for, essentially, \"turning standard NNs into BNNs\".\nThe main class of this library is the class `Laplace`, which can be used to transform a standard PyTorch model into a Laplace-approximated BNN.\nHere is an example.\n\n```python title=\"try_laplace.py\"\nfrom laplace import Laplace\n\nmodel = load_pretrained_model()\nla = Laplace(model, 'regression')\n\n# Compute the Hessian\nla.fit(train_loader)\n\n# Hyperparameter tuning\nla.optimize_prior_precision()\n\n# Make prediction\npred_mean, pred_var = la(x_test)\n```\n\nThe resulting object, `la` is a fully-functioning BNN, yielding the following prediction.\n(Notice the identical regression curves---the LA essentially imbues MAP predictions with uncertainty estimates.)\n\n\u003CBlogImage\n  imagePath='/img/laplace/regression_example.png'\n  altText='Laplace for regression.'\n/>\n\nOf course, `laplace-torch` is flexible: the `Laplace` class has almost all state-of-the-art features in Laplace approximations.\nThose features, along with the corresponding options in `laplace-torch`, are summarized in the following flowchart.\n(The options `'subnetwork'` for `subset_of_weights` and `'lowrank'` for `hessian_structure` are in the work, by the time this post is first published.)\n\n\u003CBlogImage\n  imagePath='/img/laplace/flowchart.png'\n  altText='Modern arts of Laplace approximations.'\n  fullWidth\n/>\n\nThe `laplace-torch` library uses a very cheap yet highly-performant flavor of LA by default, based on [4]:\n\n```python\ndef Laplace(model, likelihood, subset_of_weights='last_layer', hessian_structure='kron', ...)\n```\n\nThat is, by default the `Laplace` class will fit a last-layer Laplace with a Kronecker-factored Hessian for approximating the covariance.\nLet us see how this default flavor of LA performs compared to the more sophisticated, recent (all-layer) Bayesian baselines in classification.\n\n\u003CBlogImage\n  imagePath='/img/laplace/classification.png'\n  altText='Laplace for classification.'\n  fullWidth\n/>\n\nHere we can see that `Laplace`, with default options, improves the calibration (in terms of expected calibration error (ECE)) of the MAP model.\nMoreover, it is guaranteed to preserve the accuracy of the MAP model---something that cannot be said for other baselines.\nUltimately, this improvement is cheap: `laplace-torch` only incurs little overhead relative to the MAP model---far cheaper than other Bayesian baselines.\n\n## Hyperparameter Tuning\n\nHyperparameter tuning, especially for the prior variance/precision, is crucial in modern Laplace approximations for BNNs.\n`laplace-torch` provides several options: (i) cross-validation and (ii) marginal-likelihood maximization (MLM, also known as empirical Bayes and type-II maximum likelihood).\n\nCross-validation is simple but needs a validation dataset.\nIn `laplace-torch`, this can be done via the following.\n\n```python\nla.optimize_prior_precision(method='CV', val_loader=val_loader)\n```\n\nA more sophisticated and interesting tuning method is MLM.\nRecall that by taking the second-order Taylor expansion over the log-posterior, we obtain an approximate normalization constant $Z$ of the Gaussian approximate posterior.\nThis object is called the marginal likelihood: it is a probability over the dataset $\\D$ and crucially, it is a function of the hyperparameter since the parameter $\\theta$ is marginalized out.\nThus, we can find the best values for our hyperparameters by maximizing this function.\n\nIn `laplace-torch`, the marginal likelihood can be accessed via\n\n```python\nml = la.log_marginal_likelihood(prior_precision)\n```\n\nThis function is compatible with PyTorch's autograd, so we can backpropagate through it to obtain the gradient of $Z$ w.r.t. the prior precision hyperparameter:\n\n```python\nml.backward()  # Works!\n```\n\nThus, MLM can easily be done in `laplace-torch`.\nBy extension, recent methods such as online MLM [5], can also easily be applied using `laplace-torch`.\n\n## Outlooks\n\nThe `laplace-torch` library is continuously developed.\nSupport for more likelihood functions and priors, subnetwork Laplace, etc. are on the way.\n\nIn any case, we hope to see the revival of the LA in the Bayesian deep learning community.\nSo, please try out our library at [https://github.com/AlexImmer/Laplace](https://github.com/AlexImmer/Laplace)!\n\n## References\n\n1. Hein, Matthias, Maksym Andriushchenko, and Julian Bitterwolf. \"Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem.\" CVPR 2019.\n2. Laplace, Pierre Simon. \"Mémoires de Mathématique et de Physique, Tome Sixieme\" 1774.\n3. MacKay, David JC. \"The evidence framework applied to classification networks.\" Neural computation 4.5 (1992).\n4. Kristiadi, Agustinus, Matthias Hein, and Philipp Hennig. \"Being Bayesian, even just a bit, fixes overconfidence in ReLU networks.\" ICML 2020.\n5. Immer, Alexander, Matthias Bauer, Vincent Fortuin, Gunnar Rätsch, and Mohammad Emtiyaz Khan. \"Scalable marginal likelihood estimation for model selection in deep learning.\" ICML, 2021.","src/content/post/laplace.mdx","8527d92e97de62e0","laplace.mdx","lda-gibbs",{"id":366,"data":368,"body":373,"filePath":374,"digest":375,"legacyId":376,"deferredRender":23},{"title":369,"description":370,"publishDate":371,"draft":15,"tags":372},"Gibbs Sampler for LDA","Implementation of Gibbs Sampler for the inference of Latent Dirichlet Allocation (LDA)",["Date","2017-09-07T15:56:00.000Z"],[17,18],"Latent Dirichlet Allocation (LDA) [1] is a mixed membership model for topic modeling. Given a set of documents in bag of words representation, we want to infer the underlying topics those documents represent. To get a better intuition, we shall look at LDA's generative story. Note, the full code is available at https://github.com/wiseodd/mixture-models.\n\nGiven $i = \\\\{1, \\dots, N_D\\\\}$ the document index, $v = \\\\{1, \\dots, N_W\\\\}$ the word index, $k = \\\\{1, \\dots, N_K\\\\}$ the topic index, LDA assumes:\n\n$$\n\\begin{align}\n\n\\mathbf{\\pi}_i &\\sim \\text{Dir}(\\mathbf{\\pi}_i \\, \\vert \\, \\alpha) \\\\[10pt]\nz_{iv} &\\sim \\text{Cat}(z_{iv} \\, \\vert \\, \\mathbf{\\pi}_i) \\\\[10pt]\n\\mathbf{b}_k &\\sim \\text{Dir}(\\mathbf{b}_k \\, \\vert \\, \\gamma) \\\\[10pt]\ny_{iv} &\\sim \\text{Cat}(y_{iv} \\, \\vert \\, z_{iv} = k, \\mathbf{B})\n\n\\end {align}\n$$\n\nwhere $\\alpha$ and $\\gamma$ are the parameters for the Dirichlet priors. They tell us how narrow or spread the document topic and topic word distributions are.\n\nDetails for the above generative process above in words:\n\n1. Assume each document generated by selecting the topic first. Thus, sample $\\mathbf{\\pi}_i$, the topic distribution for $i$-th document.\n2. Assume each words in $i$-th document comes from one of the topics. Therefore, we sample $z_{iv}$, the topic for each word $v$ in document $i$.\n3. Assume each topic is composed of words, e.g. topic \"computer\" consits of words \"cpu\", \"gpu\", etc. Therefore, we sample $\\mathbf{b}_k$, the distribution those words for particular topic $k$.\n4. Finally, to actually generate the word, given that we already know it comes from topic $k$, we sample the word $y_{iv}$ given the $k$-th topic word distribution.\n\n## Inference\n\nThe goal of inference in LDA is that given a corpus, we infer the underlying topics that explain those documents, according to the generative process above. Essentially, given $y_{iv}$, we are inverting the above process to find $z_{iv}$, $\\mathbf{\\pi}_i$, and $\\mathbf{b}_k$.\n\nWe will infer those variables using Gibbs Sampling algorithm. In short, it works by sampling each of those variables given the other variables (full conditional distribution). Because of the conjugacy, the full conditionals are as follows:\n\n$$\n\\begin{align}\n\np(z_{iv} = k \\, \\vert \\, \\mathbf{\\pi}_i, \\mathbf{b}_k) &\\propto \\exp(\\log \\pi_{ik} + \\log b_{k, y_{iv}}) \\\\[10pt]\np(\\mathbf{\\pi}_i \\, \\vert \\, z_{iv} = k, \\mathbf{b}_k) &= \\text{Dir}(\\alpha + \\sum_l \\mathbb{I}(z_{il} = k)) \\\\[3pt]\np(\\mathbf{b}_k \\, \\vert \\, z_{iv} = k, \\mathbf{\\pi}_i) &= \\text{Dir}(\\gamma + \\sum_i \\sum_l \\mathbb{I}(y_{il} = v, z_{il} = k))\n\n\\end {align}\n$$\n\nEssentially, what we are doing is to count the assignment of words and documents to particular topics. Those are the sufficient statistics for the full conditionals\n\nGiven those full conditionals, the rest is as easy as plugging those into the Gibbs Sampling framework, as we shall discuss in the next section.\n\n## Implementation\n\nWe begin with randomly initializing topic assignment matrix $\\mathbf{Z}_{N_D \\times N_W}$. We also sample the initial values of $\\mathbf{\\Pi}_{N_D \\times N_K}$ and $\\mathbf{B}_{N_K \\times N_W}$.\n\n```python\n# Dirichlet priors\nalpha = 1\ngamma = 1\n\n# Z := word topic assignment\nZ = np.zeros(shape=[N_D, N_W])\n\nfor i in range(N_D):\n    for l in range(N_W):\n        Z[i, l] = np.random.randint(N_K) # randomly assign word's topic\n\n# Pi := document topic distribution\nPi = np.zeros([N_D, N_K])\nfor i in range(N_D):\n    Pi[i] = np.random.dirichlet(alpha*np.ones(N_K))\n\n# B := word topic distribution\nB = np.zeros([N_K, N_W])\nfor k in range(N_K):\n    B[k] = np.random.dirichlet(gamma*np.ones(N_W))\n```\n\nThen we sample the new values for each of those variables from the full conditionals in the previous section, and iterate:\n\n```python\nfor it in range(1000):\n    # Sample from full conditional of Z\n    # ---------------------------------\n    for i in range(N_D):\n        for v in range(N_W):\n            # Calculate params for Z\n            p_iv = np.exp(np.log(Pi[i]) + np.log(B[:, X[i, v]]))\n            p_iv /= np.sum(p_iv)\n\n            # Resample word topic assignment Z\n            Z[i, v] = np.random.multinomial(1, p_iv).argmax()\n\n    # Sample from full conditional of Pi\n    # ----------------------------------\n    for i in range(N_D):\n        m = np.zeros(N_K)\n\n        # Gather sufficient statistics\n        for k in range(N_K):\n            m[k] = np.sum(Z[i] == k)\n\n        # Resample doc topic dist.\n        Pi[i, :] = np.random.dirichlet(alpha + m)\n\n    # Sample from full conditional of B\n    # ---------------------------------\n    for k in range(N_K):\n        n = np.zeros(N_W)\n\n        # Gather sufficient statistics\n        for v in range(N_W):\n            for i in range(N_D):\n                for l in range(N_W):\n                    n[v] += (X[i, l] == v) and (Z[i, l] == k)\n\n        # Resample word topic dist.\n        B[k, :] = np.random.dirichlet(gamma + n)\n\n```\n\nAnd basically we are done. We could inspect the result by looking at those variables after some iterations of the algorithm.\n\n## Example\n\nLet's say we have these data:\n\n```python\n# Words\nW = np.array([0, 1, 2, 3, 4])\n\n# D := document words\nX = np.array([\n    [0, 0, 1, 2, 2],\n    [0, 0, 1, 1, 1],\n    [0, 1, 2, 2, 2],\n    [4, 4, 4, 4, 4],\n    [3, 3, 4, 4, 4],\n    [3, 4, 4, 4, 4]\n])\n\nN_D = X.shape[0] # num of docs\nN_W = W.shape[0] # num of words\nN_K = 2 # num of topics\n```\n\nThose data are already in bag of words representation, so it is a little abstract at a glance. However if we look at it, we could see two big clusters of documents based on their words: $\\\\{ 1, 2, 3 \\\\}$ and $\\\\{ 4, 5, 6 \\\\}$. Therefore, we expect after our sampler converges to the posterior, the topic distribution for those documents will follow our intuition.\n\nHere is the result:\n\n```\nDocument topic distribution:\n----------------------------\n[[ 0.81960751  0.18039249]\n [ 0.8458758   0.1541242 ]\n [ 0.78974177  0.21025823]\n [ 0.20697807  0.79302193]\n [ 0.05665149  0.94334851]\n [ 0.15477016  0.84522984]]\n```\n\nAs we can see, indeed document 1, 2, and 3 tend to be in the same cluster. The same could be said for document 4, 5, 6.\n\n## References\n\n1. Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3.Jan (2003): 993-1022.\n2. Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.","src/content/post/lda-gibbs.mdx","91f678fe04f55c87","lda-gibbs.mdx","least-squares-gan",{"id":377,"data":379,"body":384,"filePath":385,"digest":386,"legacyId":387,"deferredRender":23},{"title":380,"description":381,"publishDate":382,"draft":15,"tags":383},"Least Squares GAN","2017 is the year GAN loss its logarithm. First, it was Wasserstein GAN, and now, it's LSGAN's turn.",["Date","2017-03-02T10:50:00.000Z"],[17,56],"import BlogImage from \"@/components/BlogImage.astro\";\n\nThanks to F-GAN, which established the general framework of GAN training, recently we saw modifications of GAN which unlike the original GAN, learn other metrics other than Jensen-Shannon divergence (JSD).\n\nOne of those modifications are Wasserstein GAN (WGAN), which replaces JSD with Wasserstein distance. It works wonderfully well and even the authors claimed that it cures the mode collapse problem and providing GAN with meaningful loss function. Although the implementation is quite straightforward, the theory behind WGAN is heavy and requires some \"hack\" e.g. weight clipping. Moreover, the training process and the convergence are slower than the original GAN.\n\nNow, the question is: could we design GAN that works well, fast, simpler, and more intuitive compared to WGAN?\n\nThe answer yes. What we need is to back to basic.\n\n## Least Squares GAN\n\nThe main idea of LSGAN is to use loss function that provides smooth and non-saturating gradient in discriminator $D$. We want $D$ to \"pull\" data generated by generator $G$ towards the real data manifold $P_{data}(X)$, so that $G$ generates data that are similar to $P_{data}(X)$.\n\nAs we know in original GAN, $D$ uses log loss. The decision boundary is something like this:\n\n\u003CBlogImage\n  imagePath='/img/least-squares-gan/00.png'\n  altText='Log-loss decision boundary.'\n/>\n\nAs $D$ uses sigmoid function, and as it is saturating very quickly, even for somewhat-still-small data point $x$, it will quickly ignore the distance of $x$ to the decision boundary $w$. What it means is that it essentially won't penalize $x$ that is far away from $w$ in the manifold. That is, as long as $x$ is correctly labeled, we're happy. Consequently, as $x$ becoming bigger and bigger, the gradient of $D$ quickly goes down to $0$, as log loss doesn't care about the distance, only the sign.\n\nFor learning the manifold of $P_{data}(X)$, then log loss is not effective. Generator $G$ is trained using the gradient of $D$. If the gradient of $D$ is saturating to $0$, then $G$ won't have the necessary information for learning $P_{data}(X)$.\n\nEnter ${L_2}$ loss:\n\n\u003CBlogImage imagePath='/img/least-squares-gan/01.png' altText='L2 decision boundary.' />\n\nIn $L2$ loss, data that are quite far away from $w$ (in this context, the regression line of $P_{data}(X)$) will be penalized proportional to the distance. The gradient therefore will only become $0$ when $w$ perfectly captures all of $x$. This will guarantee $D$ to yield informative gradients if $G$ has not captured the data manifold.\n\nDuring the optimization process, the only way for $L2$ loss of $D$ to be small is to make $G$ generating $x$ that are close to $w$. This way, $G$ will actually learn to match $P_{data}(X)$!\n\nThe overall training objective of LSGAN then could be stated as follows:\n\n\u003CBlogImage imagePath='/img/least-squares-gan/02.png' altText='LSGAN loss.' />\n\nAbove, we choose $b = 1$ to state that it's the real data. Conversely, we choose $a = 0$ as it the fake data. Finally $c = 1$, as we want to fool $D$.\n\nThose values is not the only valid values, though. The authors of LSGAN provides some theory that optimizing the above loss is the same as minimizing Pearson $\\chi^2$ divergence, if $b - c = 1$ and $b - a = 2$. Hence, choosing $a = -1, b = 1, c = 0$ is equally valid.\n\nOur final loss is as follows:\n\n\u003CBlogImage imagePath='/img/least-squares-gan/03.png' altText='LSGAN loss' />\n\n## LSGAN implementation in Pytorch\n\nLet's outline the modifications done by LSGAN to the original GAN:\n\n1. Remove $\\log$ from $D$\n2. Use $L2$ loss instead of log loss\n\nSo let's begin by doing the the first checklist:\n\n```python\nG = torch.nn.Sequential(\n    torch.nn.Linear(z_dim, h_dim),\n    torch.nn.ReLU(),\n    torch.nn.Linear(h_dim, X_dim),\n    torch.nn.Sigmoid()\n)\n\nD = torch.nn.Sequential(\n    torch.nn.Linear(X_dim, h_dim),\n    torch.nn.ReLU(), # No sigmoid\n    torch.nn.Linear(h_dim, 1),\n)\n\nG_solver = optim.Adam(G.parameters(), lr=lr)\nD_solver = optim.Adam(D.parameters(), lr=lr)\n```\n\nThe rest is straightforward, following the loss function above.\n\n```python\nfor it in range(1000000):\n    # Sample data\n    z = Variable(torch.randn(mb*size, z_dim))\n    X, * = mnist.train.next_batch(mb_size)\n    X = Variable(torch.from_numpy(X))\n\n    # Dicriminator\n    G_sample = G(z)\n    D_real = D(X)\n    D_fake = D(G_sample)\n\n    # Discriminator loss\n    D_loss = 0.5 * (torch.mean((D_real - 1)**2) + torch.mean(D_fake**2))\n\n    D_loss.backward()\n    D_solver.step()\n    reset_grad()\n\n    # Generator\n    G_sample = G(z)\n    D_fake = D(G_sample)\n\n    # Generator loss\n    G_loss = 0.5 * torch.mean((D_fake - 1)**2)\n\n    G_loss.backward()\n    G_solver.step()\n    reset_grad()\n```\n\nThe full code is available at https://github.com/wiseodd/generative-models.\n\n## Conclusion\n\nIn this post we looked at LSGAN, which modifies the original GAN by using $L2$ loss instead of log loss.\n\nWe looked at the intuition why $L2$ loss could help GAN learning the data manifold. We also looked at the intuition on why GAN could not learn effectively using log loss.\n\nFinally, we implemented LSGAN in Pytorch. We found that the implementation of LSGAN is very simple, amounting to just two line changes.\n\n## References\n\n1. Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. \"f-GAN: Training generative neural samplers using variational divergence minimization.\" Advances in Neural Information Processing Systems. 2016. [arxiv](https://arxiv.org/abs/1606.00709)\n2. Mao, Xudong, et al. \"Multi-class Generative Adversarial Networks with the L2 Loss Function.\" arXiv preprint arXiv:1611.04076 (2016). [arxiv](https://arxiv.org/abs/1611.04076v2)","src/content/post/least-squares-gan.mdx","5357ecdb900fb628","least-squares-gan.mdx","levelset-segmentation",{"id":388,"data":390,"body":396,"filePath":397,"digest":398,"legacyId":399,"deferredRender":23},{"title":391,"description":392,"publishDate":393,"draft":15,"tags":394},"Level Set Method Part II: Image Segmentation","Level Set Method is an interesting classical (pre deep learning) Computer Vision method based on Partial Differential Equation (PDE) for image segmentation. In this post, we will look at its application in image segmentation.",["Date","2016-11-20T05:36:00.000Z"],[42,43,395],"computer vision","import BlogImage from \"@/components/BlogImage.astro\";\n\nLast post, we looked at the intuition and the formulation of Level Set Method. It's useful to create a physical simulation like front propagation, e.g. wave simulation, wildfire simulation, or gas simulation. In this post, we are going to see into Level Set Method application in Computer Vision, to solve image segmentation problem.\n\n## Image Segmentation with Level Set Method\n\nRecall that the Level Set PDE that we have derived in the last post is as follows:\n\n$$\n    \\phi' = \\phi + \\Delta t F {\\lVert \\nabla \\phi \\rVert}\n$$\n\nHere, $F$ would need our special attention, as the key to adapting Level Set Method into new problem is by formulating this term. $F$ is intuitively a force that drive curve propagation. In other words, we could think of $F$ as a velocity field, i.e. $F$ is a vector field where at every point it tells us the direction and magnitude of movement of our surface $\\phi$.\n\nAs we want to segment an image, then we need to derive $F$ from the image. Let's say we have this image:\n\n\u003CBlogImage imagePath='/img/levelset-segmentation/lsm_ori.png' />\n\nAs $F$ is a velocity field and consider the Level Set PDE above, we want $F$ to be high at all region that are not the border of the object we want to segment, and low otherwise. Intuitively, we want the curve to propagate quickly in the background of the image, and we want the curve to slowly propagate or even stop the propagation at the border of the object.\n\nOne way to do it is obviously derive our $F$ from edge detector. Simplest way to do edge detection is to take the gradients of the image:\n\n$$\n    g(I) = \\frac{1}{1 + {\\lVert \\nabla I \\rVert}^2}\n$$\n\nSo, we could implement it with the code below:\n\n```python\nimport numpy as np\nimport scipy.ndimage\nimport scipy.signal\nimport matplotlib.pyplot as plt\nfrom skimage import color, io\n\ndef grad(x):\n    return np.array(np.gradient(x))\n\ndef norm(x, axis=0):\n    return np.sqrt(np.sum(np.square(x), axis=axis))\n\ndef stopping_fun(x):\n    return 1. / (1. + norm(grad(x))**2)\n\nimg = io.imread('twoObj.bmp')\nimg = color.rgb2gray(img)\nimg = img - np.mean(img)\n\n# Smooth the image to reduce noise and separation between noise and edge becomes clear\nimg_smooth = scipy.ndimage.filters.gaussian_filter(img, sigma)\n\nF = stopping_fun(img_smooth)\n```\n\n\u003CBlogImage imagePath='/img/levelset-segmentation/lsm_g.png' />\n\nSo that's it, we have our $F$. We could plug it into the PDE directly then:\n\n```python\ndef default_phi(x):\n    # Initialize surface phi at the border (5px from the border) of the image # i.e. 1 outside the curve, and -1 inside the curve\n    phi = np.ones(x.shape[:2])\n    phi[5:-5, 5:-5] = -1.\n    return phi\n\ndt = 1.\n\nfor i in range(n_iter):\n    dphi = grad(phi)\n    dphi_norm = norm(dphi)\n    dphi_t = F * dphi_norm\n    phi = phi + dt * dphi_t\n```\n\nAnd here's the segmentation result after several iteration:\n\n\u003CBlogImage imagePath='/img/levelset-segmentation/lsm_naive.png' />\n\nThis is the naive method for Level Set image segmentation. We could do better by using more complicated formulation for $F$.\n\n## Geodesic Active Contour\n\nIn Geodesic Active Contour (GAC) formulation of Level Set Method, we define the force as:\n\n$$\n    \\frac{\\partial \\phi}{\\partial t} = g(I) {\\lVert \\nabla \\phi \\rVert} div \\left( \\frac{\\nabla \\phi}{\\lVert \\nabla \\phi \\rVert} \\right) + g(I) {\\lVert \\nabla \\phi \\rVert} v + \\nabla g(I) \\cdot \\nabla \\phi\n$$\n\nThe first term is the smoothing term, it moves the curve into the direction of its curvature. The second term is the balloon term, controlling the speed of the curve propagation with parameter $v$. Lastly, the third term is the image attachment term that helps the curve to converge.\n\n```python\ndef curvature(f):\n    fy, fx = grad(f)\n    norm = np.sqrt(fx**2 + fy**2)\n    Nx = fx / (norm + 1e-8)\n    Ny = fy / (norm + 1e-8)\n    return div(Nx, Ny)\n\ndef div(fx, fy):\n    fyy, fyx = grad(fy)\n    fxy, fxx = grad(fx)\n    return fxx + fyy\n\ndef dot(x, y, axis=0):\n    return np.sum(x * y, axis=axis)\n\nv = 1.\ndt = 1.\n\ng = stopping_fun(img_smooth, alpha)\ndg = grad(g)\n\nfor i in range(n_iter):\n    dphi = grad(phi)\n    dphi_norm = norm(dphi)\n    kappa = curvature(phi)\n\n    smoothing = g * kappa * dphi_norm\n    balloon = g * dphi_norm * v\n    attachment = dot(dphi, dg)\n\n    dphi_t = smoothing + balloon + attachment\n\n    phi = phi + dt * dphi_t\n```\n\nFinally, here's the result of using GAC formulation of Level Set Method for segmenting the same image above:\n\n\u003CBlogImage imagePath='/img/levelset-segmentation/lsm_gac.png' />\n\nNotice that qualitatively the segmentation result is better than before, i.e. smoother and better fit.\n\n## Conclusion\n\nIn this post we looked into the application of Level Set Method on Computer Vision problem, that is image segmentation. We saw the intuition on applying it for image segmentation. We also saw the example of implementation of it. Finally we saw the more complicated formulation, i.e. GAC, for better segmentation.\n\n## References\n\n1. Richard Szeliski. 2010. Computer Vision: Algorithms and Applications (1st ed.). Springer-Verlag New York, Inc., New York, NY, USA.","src/content/post/levelset-segmentation.mdx","6ec72bf27ad43484","levelset-segmentation.mdx","lstm-backprop",{"id":400,"data":402,"body":409,"filePath":410,"digest":411,"legacyId":412,"deferredRender":23},{"title":403,"description":404,"publishDate":405,"draft":15,"tags":406},"Deriving LSTM Gradient for Backpropagation","Deriving neuralnet gradient is an absolutely great exercise to understand backpropagation and computational graph better. In this post we will walk through the process of deriving LSTM net gradient so that we can use it in backpropagation.",["Date","2016-08-12T16:34:00.000Z"],[17,42,43,44,407,408],"rnn","lstm","import BlogImage from \"@/components/BlogImage.astro\";\n\nRecurrent Neural Network (RNN) is hot in these past years, especially with the boom of Deep Learning. Just like any deep neural network, RNN can be seen as a (very) deep neural network if we \"unroll\" the network with respect of the time step. Hence, with all the things that enable vanilla deep network, training RNN become more and more feasible too.\n\nThe most popular model for RNN right now is the LSTM (Long Short-Term Memory) network. For the background theory, there are a lot of amazing resources available in [Andrej Karpathy's blog]() and [Chris Olah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n\nUsing modern Deep Learning libraries like TensorFlow, Torch, or Theano nowadays, building an LSTM model would be a breeze as we don't need to analytically derive the backpropagation step. However to understand the model better, it's absolutely a good thing, albeit optional, to try to derive the LSTM net gradient and implement the backpropagation \"manually\".\n\nSo, here, we will try to first implement the forward computation step according to the LSTM net formula, then we will try to derive the network gradient analytically. Finally, we will implement it using numpy.\n\n## LSTM Forward\n\nWe will follow this model for a single LSTM cell:\n\n\u003CBlogImage imagePath='/img/lstm-backprop/formula.png' />\n\nLet's implement it!\n\n```python\nimport numpy as np\n\nH = 128 # Number of LSTM layer's neurons\nD = ... # Number of input dimension == number of items in vocabulary\nZ = H + D # Because we will concatenate LSTM state with the input\n\nmodel = dict(\n    Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n    Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n    Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n    Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n    Wy=np.random.randn(H, D) / np.sqrt(D / 2.),\n    bf=np.zeros((1, H)),\n    bi=np.zeros((1, H)),\n    bc=np.zeros((1, H)),\n    bo=np.zeros((1, H)),\n    by=np.zeros((1, D))\n)\n```\n\nAbove, we're declaring our LSTM net model. Notice that from the formula above, we're concatenating the old hidden state `h` with current input `x`, hence the input for our LSTM net would be `Z = H + D`. And because our LSTM layer wants to output `H` neurons, each weight matrices' size would be `ZxH` and each bias vectors' size would be `1xH`.\n\nOne difference is for `Wy` and `by`. This weight and bias would be used for fully connected layer, which would be fed to a softmax layer. The resulting output should be a probability distribution over all possible items in vocabulary, which would be the size of `1xD`. Hence, `Wy`'s size must be `HxD` and `by`'s size must be `1xD`.\n\n```python\ndef lstm_forward(X, state):\n    m = model\n    Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n    bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n\n    h_old, c_old = state\n\n    # One-hot encode\n    X_one_hot = np.zeros(D)\n    X_one_hot[X] = 1.\n    X_one_hot = X_one_hot.reshape(1, -1)\n\n    # Concatenate old state with current input\n    X = np.column_stack((h_old, X_one_hot))\n\n    hf = sigmoid(X @ Wf + bf)\n    hi = sigmoid(X @ Wi + bi)\n    ho = sigmoid(X @ Wo + bo)\n    hc = tanh(X @ Wc + bc)\n\n    c = hf * c_old + hi * hc\n    h = ho * tanh(c)\n\n    y = h @ Wy + by\n    prob = softmax(y)\n\n    state = (h, c) # Cache the states of current h & c for next iter\n    cache = ... # Add all intermediate variables to this cache\n\n    return prob, state, cache\n```\n\nThe above code is for the forward step for a single LSTM cell, which identically follows the formula above. The only additions are the one-hot encoding and the hidden-input concatenation process.\n\n## LSTM Backward\n\nNow, we will dive into the main point of this post: LSTM backward computation. We will assume that derivative function for `sigmoid` and `tanh` are already known.\n\n```python\ndef lstm_backward(prob, y_train, d_next, cache):\n    # Unpack the cache variable to get the intermediate variables used in forward step\n    ... = cache\n    dh_next, dc_next = d_next\n\n    # Softmax loss gradient\n    dy = prob.copy()\n    dy[1, y_train] -= 1.\n\n    # Hidden to output gradient\n    dWy = h.T @ dy\n    dby = dy\n    # Note we're adding dh_next here\n    dh = dy @ Wy.T + dh_next\n\n    # Gradient for ho in h = ho * tanh(c)\n    dho = tanh(c) * dh\n    dho = dsigmoid(ho) * dho\n\n    # Gradient for c in h = ho * tanh(c), note we're adding dc_next here\n    dc = ho * dh * dtanh(c)\n    dc = dc + dc_next\n\n    # Gradient for hf in c = hf * c_old + hi * hc\n    dhf = c_old * dc\n    dhf = dsigmoid(hf) * dhf\n\n    # Gradient for hi in c = hf * c_old + hi * hc\n    dhi = hc * dc\n    dhi = dsigmoid(hi) * dhi\n\n    # Gradient for hc in c = hf * c_old + hi * hc\n    dhc = hi * dc\n    dhc = dtanh(hc) * dhc\n\n    # Gate gradients, just a normal fully connected layer gradient\n    dWf = X.T @ dhf\n    dbf = dhf\n    dXf = dhf @ Wf.T\n\n    dWi = X.T @ dhi\n    dbi = dhi\n    dXi = dhi @ Wi.T\n\n    dWo = X.T @ dho\n    dbo = dho\n    dXo = dho @ Wo.T\n\n    dWc = X.T @ dhc\n    dbc = dhc\n    dXc = dhc @ Wc.T\n\n    # As X was used in multiple gates, the gradient must be accumulated here\n    dX = dXo + dXc + dXi + dXf\n    # Split the concatenated X, so that we get our gradient of h_old\n    dh_next = dX[:, :H]\n    # Gradient for c_old in c = hf * c_old + hi * hc\n    dc_next = hf * dc\n\n    grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n    state = (dh_next, dc_next)\n\n    return grad, state\n```\n\nA bit long isn't it? However, actually it's easy enough to derive the LSTM gradients if you understand how to take a partial derivative of a function and how to do chain rule, albeit some tricky stuffs are going on here. For this, I would recommend [CS231n](http://cs231n.github.io/optimization-2/).\n\nThings that are tricky and not-so-obvious when deriving the LSTM gradients are:\n\n1. Adding `dh_next` to `dh`, because `h` is branched in forward propagation: it was used in `y = h @ Wy + by` and the next time step, concatenated with `x`. Hence the gradient is split and has to be added here.\n2. Adding `dc_next` to `dc`. Identical reason with above.\n3. Adding `dX = dXo + dXc + dXi + dXf`. Similar reason with above: X is used in many places so the gradient is split and need to be accumulated back.\n4. Getting `dh_next` which is the gradient of `h_old`. As `X = [h_old, x]`, then `dh_next` is just a reverse concatenation: split operation on `dX`.\n\nWith the forward and backward computation implementations in hands, we could stitch them together to get a full training step that would be useful for optimization algorithms.\n\n## LSTM Training Step\n\nThis training step consists of three steps: forward computation, loss calculation, and backward computation.\n\n```python\ndef train_step(X_train, y_train, state):\n    probs = []\n    caches = []\n    loss = 0.\n    h, c = state\n\n    # Forward Step\n    for x, y_true in zip(X_train, y_train):\n        prob, state, cache = lstm_forward(x, state, train=True)\n        loss += cross_entropy(prob, y_true)\n\n        # Store forward step result to be used in backward step\n        probs.append(prob)\n        caches.append(cache)\n\n    # The loss is the average cross entropy\n    loss /= X_train.shape[0]\n\n    # Backward Step\n\n    # Gradient for dh_next and dc_next is zero for the last timestep\n    d_next = (np.zeros_like(h), np.zeros_like(c))\n    grads = {k: np.zeros_like(v) for k, v in model.items()}\n\n    # Go backward from the last timestep to the first\n    for prob, y_true, cache in reversed(list(zip(probs, y_train, caches))):\n        grad, d_next = lstm_backward(prob, y_true, d_next, cache)\n\n        # Accumulate gradients from all timesteps\n        for k in grads.keys():\n            grads[k] += grad[k]\n\n    return grads, loss, state\n```\n\nIn the full training step, first we're do full forward propagation on all items in training set, then store the results which are the softmax probabilities and cache of each timestep into a list, because we are going to use it in backward step.\n\nNext, at each timestep, we can calculate the cross entropy loss (because we're using softmax). We then accumulate all of those loss in every timestep, then average them.\n\nLastly, we do backpropagation based on the forward step results. Notice while we're iterating the data forward in forward step, we're going the reverse direction here.\n\nAlso notice that `dh_next` and `dc_next` for the first timestep in backward step is zero. Why? This is because at the last timestep in forward propagation, `h` and `c` won't be used in the next timestep, as there are no more timestep! So, the gradient of `h` and `c` in the last timestep are not split and could be derived directly without `dh_next` and `dc_next`.\n\nWith this function in hands, we could plug this to any optimization algorithm like RMSProp, Adam, etc with some modification. Namely, we have to take account on the state of the network. So, the state for the current timestep need to be passed to the next timestep.\n\nAnd, that's it. We can train our LSTM net now!\n\n## Test Result\n\nUsing Adam to optimize the network, here's the result when I feed a copy-pasted text about Japan from Wikipedia. Each data is a character in the text. The target is the next character.\n\nAfter each 100 iterations, the network are sampled.\n\nIt works like this:\n\n1. Do forward propagation and get the softmax distribution\n2. Sample the distribution\n3. Feed the sampled character as the input of next time step\n4. Repeat\n\nAnd here's the snippet of the results:\n\n```\n=========================================================================\nIter-100 loss: 4.2125\n=========================================================================\nbest c ehpnpgteHihcpf,M tt\" ao tpo Teoe ep S4 Tt5.8\"i neai   neyoserpiila o  rha aapkhMpl rlp pclf5i\n=========================================================================\n\n...\n\n=========================================================================\nIter-52800 loss: 0.1233\n=========================================================================\ntary shoguns who ruled in the name of the Uprea wal motrko, the copulation of Japan is a sour the wa\n=========================================================================\n```\n\nOur network definitely learned something here!\n\n## Conclusion\n\nHere, we looked at the general formula for LSTM and implement the forward propagation step based on it, which is very straightforward to do.\n\nThen, we derived the backward computation step. This step was also straightforward, but there were some tricky stuffs that we had to ponder about, especially the recurrency step in `h` and `c`.\n\nWe then stitched the forward and backward step together to build the full training step that can be used with any optimization algorithm.\n\nLastly, we tried to run the network using some test data and showed that the network was learning by looking at the loss value and the sample of text that are produced by the network.\n\n## References\n\n- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n- http://cs231n.github.io/optimization-2/\n- https://gist.github.com/karpathy/d4dee566867f8291f086","src/content/post/lstm-backprop.mdx","3ef9fcb8e3a69909","lstm-backprop.mdx","mayer-vietoris-sphere",{"id":413,"data":415,"body":420,"filePath":421,"digest":422,"legacyId":423,"deferredRender":23},{"title":416,"description":417,"publishDate":418,"draft":15,"tags":419},"Reduced Betti number of sphere: Mayer-Vietoris Theorem","A proof of reduced homology of sphere with Mayer-Vietoris sequence.",["Date","2018-07-23T14:00:00.000Z"],[79],"import BlogImage from \"@/components/BlogImage.astro\";\n\nIn the previous post about Brouwer's Fixed Point Theorem, we used two black boxes. In this post we will prove the slight variation of those black boxes. We will start with the simplest lemma first: the reduced homology of balls.\n\n**Lemma 2 (Reduced homology of balls)**  \nGiven a $ d $-ball $ \\mathbb{B}^d $, then its reduced $ p $-th homology space is trivial, i.e. $\\tilde{H}\\_p(\\mathbb{B}^d) = 0 $, for any $ d $ and $ p $.\n\n_Proof._ &nbsp;&nbsp; Observe that $ \\mathbb{B}^d $ is contractible, i.e. homotopy equivalent to a point. Assuming we use coefficient $ \\mathbb{Q} $, we know the zero-th homology space of point is $ H_0(\\, \\cdot \\,, \\mathbb{Q}) = \\mathbb{Q} $, and trivial otherwise, i.e. $ H_p (\\, \\cdot \\,, \\mathbb{Q}) = 0 \\enspace \\forall p \\geq 1 $.\n\nIn the reduced homology, therefore $ \\tilde{H}\\_0(\\, \\cdot \\,, \\mathbb{Q}) = 0 $. Thus the reduced homology of balls is trivial for all $ d, p $.\n\n$$\n\\qed\n$$\n\n**Corollary 1 (Reduced Betti numbers of balls)**  \nThe $ p $-th reduced Betti numbers of $ \\mathbb{B}^d $ is zero for all $d, p$.\n\n$$\n\\qed\n$$\n\nNow, we are ready to prove the main theme of this post.\n\n**Lemma 1 (Reduced Betti numbers of spheres)**  \nGiven a $ d $-sphere $ \\mathbb{S}^d $, then its $ p $-th reduced Betti number is:\n\n$$\n\\tilde{\\beta}_p(\\mathbb{S}^d) = \\begin{cases} 1, & \\text{if } p = d \\\\ 0, & \\text{otherwise} \\enspace . \\end{cases}\n$$\n\n_Proof._ &nbsp;&nbsp; We use \"divide-and-conquer\" approach to apply Mayer-Vietoris Theorem. We cut the sphere along the equator and note that the upper and lower portion of the sphere is just a disk, and the intersection between those two parts is a circle (sphere one dimension down), as shown in the figure below.\n\n\u003CBlogImage\n  imagePath='/img/mayer-vietoris-sphere/sphere.svg'\n  altText='Mayer-Vietoris on a sphere.'\n/>\n\nBy Mayer-Vietoris Theorem, we have a long exact sequence in the form of:\n\n$$\n\\dots \\longrightarrow \\tilde{H}_p(\\mathbb{S}^{d-1}) \\longrightarrow \\tilde{H}_p(\\mathbb{B}^d) \\oplus \\tilde{H}_p(\\mathbb{B}^d) \\longrightarrow \\tilde{H}_p(\\mathbb{S}^d) \\longrightarrow \\tilde{H}_{p-1}(\\mathbb{S}^{d-1}) \\longrightarrow \\dots \\enspace .\n$$\n\nBy Corollary 1, $ \\tilde{H}_p(\\mathbb{B}^d) \\oplus \\tilde{H}\\_p(\\mathbb{B}^d) = \\tilde{H}_{p-1}(\\mathbb{B}^d) \\oplus \\tilde{H}_{p-1}(\\mathbb{B}^d) = 0 $. As the sequence is exact, therefore $ \\tilde{H}\\_p(\\mathbb{S}^d) \\longrightarrow \\tilde{H}_{p-1}(\\mathbb{S}^{d-1}) $ is a bijection, and thus an isomorphism. Then by induction with base case of $ \\mathbb{S}^0 $, we conclude that the claim holds.\n\n$$\n\\qed\n$$\n\n## References\n\n1. Hatcher, Allen. \"Algebraic topology.\" (2001).","src/content/post/mayer-vietoris-sphere.mdx","c71edc8d86aad7aa","mayer-vietoris-sphere.mdx","metropolis-hastings",{"id":424,"data":426,"body":431,"filePath":432,"digest":433,"legacyId":434,"deferredRender":23},{"title":427,"description":428,"publishDate":429,"draft":15,"tags":430},"Metropolis-Hastings","An implementation example of Metropolis-Hastings algorithm in Python.",["Date","2015-10-17T01:14:00.000Z"],[17,42,43],"import BlogImage from \"@/components/BlogImage.astro\";\n\nMetropolis-Hastings algorithm is another sampling algorithm to sample from high dimensional, difficult to sample directly (due to intractable integrals) distributions or functions.\n\nIt's an MCMC algorithm, just like Gibbs Sampling. It's MC (Markov Chain) because to get the next sample, one only need to consider the current sample. It's MC (Monte Carlo) because it generates random sample which we could use to compute integrals or numerical results, for example in the probability distribution setting, the integrals we may want to compute are the expected value, mode, median, etc.\n\nThe core of the algorithm lies in the distribution `Q(x -> x')`, which is used to suggest the next candidate of the Markov Chain given the current state/sample, and the acceptance probability alpha which is used to decide whether we accept the new sample, or stay with the current sample.\n\nThe acceptance probability alpha is found by this equation:\n\n```python\nalpha = min(1, P(x')/P(x) * Q(x' -> x)/Q(x -> x'))\n```\n\nWhere `P(x)` is the target distribution, namely the distribution we want to sample from.\n\nWhen transition distribution `Q(x -> x')` is symmetric, `Q(x -> x') = Q(x' -> x)`, then the ratio will become 1, and the alpha will be just:\n\n```python\nalpha = min(1, P(x')/P(x))\n```\n\nIn this case, the Metropolis-Hastings would become just Metropolis. So, Metropolis algorithm is the special case of Metropolis-Hastings algorithm where the transition distribution is symmetric. Take Gaussian for example.\n\nLet's devour the code.\n\n```python\nimport numpy as np\nimport scipy.stats as st\nimport seaborn as sns\n\nmus = np.array([5, 5])\nsigmas = np.array([[1, .9], [.9, 1]])\n\ndef circle(x, y):\n    return (x-1)**2 + (y-2)**2 - 3**2\n\ndef pgauss(x, y):\n    return st.multivariate_normal.pdf([x, y], mean=mus, cov=sigmas)\n\ndef metropolis_hastings(p, iter=1000):\n    x, y = 0., 0.\n    samples = np.zeros((iter, 2))\n\n    for i in range(iter):\n        x_star, y_star = np.array([x, y]) + np.random.normal(size=2)\n        if np.random.rand() \u003C p(x_star, y_star) / p(x, y):\n            x, y = x_star, y_star\n        samples[i] = np.array([x, y])\n\n    return samples\n\nif __name__ == '__main__':\n    samples = metropolis_hastings(circle, iter=10000)\n    sns.jointplot(samples[:, 0], samples[:, 1])\n\n    samples = metropolis_hastings(pgauss, iter=10000)\n    sns.jointplot(samples[:, 0], samples[:, 1])\n```\n\nIn the code, I ignore the min(1, x) term for the alpha calculation, and just calculate `P(x') / P(x),` because we only care about whether or not the ratio is bigger than some uniform random number `[0, 1]`, so when `P(x') / P(x) > 1`, it will then always satisfy the test just as `P(x') / P(x) = 1`, calculated from the `min(1, x)` term.\n\nThe first function I'm sampling for is a circle centered in `(1, 2)` with radius of `3`. Let's see the result.\n\n\u003CBlogImage imagePath='/img/rejection-sampling/01.png' />\n\nIt indeed looks like a circle, doesn't it?\n\nFor the second run of Metropolis-Hastings, I'm trying to sample from a Bivariate Normal distribution, exactly the same one I sampled in my Gibbs Sampling post.\n\n\u003CBlogImage imagePath='/img/rejection-sampling/01.png' />\n\nThe tail of the distribution looks off because the starting point of the Markov Chain I've chosen is `(0, 0)`. It should be discarded if we use burn-in (discard a first few samples) in our Metropolis-Hastings. Other than that, it yielded the same distribution shape as Gibbs Sampling. This is because Gibbs Sampling is derived from Metropolis-Hastings, with special case, i.e. the acceptance probability of Gibbs Sampling is 1, so it will always accept the sample drawn from the conditional distributions.\n\nI'm curious about the performance of Metropolis-Hastings compared to Gibbs Sampling. So I profiled the two algorithms.\n\n```bash\n> python -m cProfile gibbs_sampling.py | grep gibbs_sampling\n> ncalls tottime percall cumtime percall\n> 10000 0.059 0.000 0.062 0.000 gibbs_sampling.py:5(p_x_given_y)\n> 10000 0.057 0.000 0.059 0.000 gibbs_sampling.py:11(p_y_given_x)\n\n    1    0.022    0.022    0.143    0.143 gibbs_sampling.py:17(gibbs_sampling)\n\n> python -m cProfile metropolis_hastings.py | grep metropolis_hastings\n> ncalls tottime percall cumtime percall\n> 20000 0.100 0.000 3.449 0.000 metropolis_hastings.py:10(pgauss)\n\n    1    0.121    0.121    3.658    3.658 metropolis_hastings.py:16(metropolis_hasting)\n```\n\nAs we can see, Gibbs Sampling is really fast as it only need around 0.14 seconds, compared to Metropolis-Hastings 3.6 seconds. Sampling from conditional distribution is really fast, whereas sampling from full joint distribution is slow, as we can observe there when comparing call time of `p_x_given_y`, `p_y_given_x` and `pgauss`.\n\nHowever, the main advantage of Metropolis-Hastings over Gibbs Sampling is that we don't have to derive the conditional distributions analytically. We just need to know the joint distribution, and no need to derive anything analytically.","src/content/post/metropolis-hastings.mdx","4df3278d578bf9f9","metropolis-hastings.mdx","levelset-method",{"id":435,"data":437,"body":442,"filePath":443,"digest":444,"legacyId":445,"deferredRender":23},{"title":438,"description":439,"publishDate":440,"draft":15,"tags":441},"Level Set Method Part I: Introduction","Level Set Method is an interesting classical (pre deep learning) Computer Vision method based on Partial Differential Equation (PDE) for image segmentation. In this post, we will look at the intuition behind it.",["Date","2016-11-05T15:24:00.000Z"],[42,43,395],"import BlogImage from \"@/components/BlogImage.astro\";\n\nLooking at the trend in Computer Vision, people steadily abandon the classical methods and just throw everything into Deep Neural Network. It is, however, very useful to study the classical CV method as it is still the key foundation, regardless whether we plan to use DNN or not.\n\nWe will look at one of the classic algorithms in Computer Vision: the Level Set Method (LSM). In this post we will see the motivation behind it, the intuition, formulation and finally the implementation of LSM. In the next post, we will apply this method for image segmentation.\n\n## Intuition\n\nLet's say we throw a stone into the middle of a pond. What would happen? There would be a ripple of water (for simplicity let's just pick one wave), moving from the epicenter, going wide until it dissipates or hit the pond's edge. How do we model and simulate that phenomenon?\n\nWe could do this: model the curve of that ripple and track its movement. Let's say at time $t = 1$, this is how the ripple looks like:\n\n\u003CBlogImage imagePath='/img/levelset-method/0.png' />\n\nNow, we want to model the movement as time goes. To do that, we have to track the movement of the curve above. One way to do it is to sample sufficient points in that curve and move it to the direction normal to the curve.\n\n\u003CBlogImage imagePath='/img/levelset-method/1.png' />\n\nThis is a good solution for a very simple simulation (like this one). However, consider what happen to this case:\n\n\u003CBlogImage imagePath='/img/levelset-method/2.png' />\n\nAssuming there's no external force, those two curve will merge together into a single curve. Also, we have to consider the case when a curve splitting into two or more curves. How do we model that?\n\nThis is where LSM shines. Instead of modeling the curve explicitly, LSM will model it implicitly. But how can it help us to model the split, merge, and the movement of the curve, we would ask. Let's see how it works.\n\nSuppose we have this 3D curve (surface):\n\n\u003CBlogImage imagePath='/img/levelset-method/3.png' />\n\nWe can model the above curve (circle) with this curve by exploiting the relation between surface, plane, and curve. What we're going to do is to adjust our surface so that it intersect with a plane in a certain height. Like this:\n\n\u003CBlogImage imagePath='/img/levelset-method/4.png' />\n\nTake at a closer look at the intersection. What is it? It is none other than a curve, specifically, a circle! The curve is the level curve, i.e., a curve of a level set. This is the idea behind LSM. We implicitly modify our curve by transforming the surface then intersecting it to a plane and evaluate the resulting level curve.\n\nBut it's still not clear how do LSM could model the merge and split operation of curves. Let's do something to our surface and see what does it do to our level curve.\n\n\u003CBlogImage imagePath='/img/levelset-method/5.png' />\n\nIn the above graph, we transform the surface into a surface with two minima. We can see the implication at the level curve, instead of a single circle, now it becomes two. Similarly with merge operation:\n\n\u003CBlogImage imagePath='/img/levelset-method/6.png' />\n\nEffortlessly, the level curve captures it!\n\nThis is a powerful insight and we're going to formulate this.\n\n## Level Set Method Formulation\n\nSuppose we have surface $\\phi(x)$. The c-level set of this surface is given by:\n\n$$\n    \\{x | \\phi(x) = c\\}\n$$\n\nFormally, we want to track the level curve at $c = 0$, which is the zero level set of $\\phi$.\n\n$$\n    \\{x | \\phi(x) = 0\\}\n$$\n\nAs we're dealing with curve and surface evolution, we will parameterized our surface with a temporal variable $t$ such that:\n\n$$\n    \\phi(x(t), t) = 0\n$$\n\nWe could think of that as the surface at time $t$, given the variable $x$ at time $t$.\n\nNext, as we want to track the movement of the zero level curve of $\\phi$, we will derive it with respect to $t$ i.e. we derive the equation of motion of $\\phi$. Remember, the derivation of position is speed, and knowing the speed, we could model the movement of the surface.\n\n$$\n    \\frac{\\partial \\phi(x(t), t)}{\\partial t} = 0\n$$\n\nUsing chain rule, we get:\n\n$$\n    \\frac{\\partial \\phi}{\\partial x(t)} \\frac{\\partial x(t)}{\\partial t} + \\frac{\\partial \\phi}{\\partial t} = 0\n$$\n\nRemember, by definition, the left most partial derivate is the gradient of our surface. And also, for clearer reading we will switch the Leibniz into more compact notation.\n\n$$\n    \\nabla\\phi X_t + \\phi_t = 0\n$$\n\nAs we state above, the direction of the curve's movement is normal, which is $\\frac{\\nabla \\phi}{\\lVert \\nabla \\phi \\rVert}$. Of course there would also be a force that move the curve, we call it as $F$. Hence, the speed vector is given by $x_t = F \\frac{\\nabla \\phi}{\\lVert \\nabla \\phi \\rVert}$.\n\n$$\n\\begin{align}\n\n\\nabla \\phi F \\frac{\\nabla \\phi}{\\lVert \\nabla \\phi \\rVert} + \\phi_t & = 0 \\\\[10pt]\nF \\lVert \\nabla \\phi \\rVert + \\phi_t & = 0\n\n\\end{align}\n$$\n\nFinally, organizing things a little bit, we have our level set equation:\n\n$$\n    \\phi_t = -F {\\lVert \\nabla \\phi \\rVert}\n$$\n\nThis gives us the speed of the surface evolution of $\\phi$.\n\n## Solving the PDE\n\nKnowing the initial value of $\\phi$ (let's say 0 everywhere) and the speed of evolution, we can solve the equation of motion. That is, we want to know surface $\\phi$ at time $t$. This is a Partial Differential Equation (PDE).\n\nThe simplest way to solve this would be to use Finite Difference Method. Let's consider forward difference scheme.\n\n$$\n    f'(x) = \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}\n$$\n\nPlugging in our $\\phi$ we have:\n\n$$\n\\begin{align}\n\n\\frac{\\partial \\phi(x(t), t)}{\\partial t} & = \\frac{\\phi(x(t), t + \\Delta t) - \\phi(x(t), t)}{\\Delta t} \\\\[10pt]\n\\Delta t \\phi_t & = \\phi(x(t), t + \\Delta t) - \\phi(x(t), t) \\\\[10pt]\n\\phi(x(t), t + \\Delta t) & = \\phi(x(t), t) + \\Delta t \\phi_t \\\\[10pt]\n\\phi(x(t), t + \\Delta t) & = \\phi(x(t), t) + \\Delta t F {\\lVert \\nabla \\phi \\rVert}\n\n\\end{align}\n$$\n\nTo make it clearer for those who has a background in Machine Learning, recall gradient descent. The update rule is in the form of:\n\n$$\n    x' = x + \\alpha \\nabla x\n$$\n\nWhich is analogous to the finite difference scheme above. So we're practically doing gradient descent on $\\phi$ with respect to $t$!\n\n$$\n    \\phi' = \\phi + \\Delta t F {\\lVert \\nabla \\phi \\rVert}\n$$\n\nSo that's it. We just need to provide initial value for $\\phi$ and figure out the equation of force $F$, which depend on the system we're going to model.\n\n## Implementation\n\nGiven the finite difference formulation to solve the LSM's PDE, we could now implement it.\n\nAs we see, first we need to provide initial values. Then at every iteration, we look at the zero level set of $\\phi$, and we get our curve evolution! For example in Python using matplotlib, it would be something like this:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nphi = np.random.randn(20, 20) # initial value for phi\nF = ... # some function\ndt = 1\nit = 100\n\nfor i in range(it):\n    dphi = np.gradient(phi)\n    dphi_norm = np.sqrt(np.sum(dphi**2, axis=0))\n\n    phi = phi + dt * F * dphi_norm\n\n    # plot the zero level curve of phi\n    plt.contour(phi, 0)\n    plt.show()\n```\n\nAs we can see, the core of LSM could be implemented with just a few lines of code. Bear in mind, this is the simplest formulation of LSM. There are many sophisticated variations of LSM which modifies $\\frac{\\phi(x(t), t)}{\\partial t}$.\n\n## Conclusion\n\nIn this post, we looked at Level Set Method (LSM) which is a method to model curve evolution using implicit contour. LSM is powerful because we don't have to explicitly model difficult curve evolution like merge and split directly.\n\nThen, we looked at the LSM formulation and how to solve the LSM as PDE using Finite Difference Method.\n\nFinally, we implemented the simplest formulation of LSM in Python.\n\n## References\n\n1. Richard Szeliski. 2010. Computer Vision: Algorithms and Applications (1st ed.). Springer-Verlag New York, Inc., New York, NY, USA.\n2. http://step.polymtl.ca/~rv101/levelset/","src/content/post/levelset-method.mdx","fbee4d4bf5957d4b","levelset-method.mdx","manifold-gaussians",{"id":446,"data":448,"body":453,"filePath":454,"digest":455,"legacyId":456,"deferredRender":23},{"title":449,"description":450,"publishDate":451,"draft":15,"tags":452},"The Curvature of the Manifold of Gaussian Distributions","The Gaussian probability distribution is central in statistics and machine learning. As it turns out, by equipping the set of all Gaussians p.d.f. with a Riemannian metric given by the Fisher information, we can see it as a Riemannian manifold. In this post, we will prove that this manifold can be covered by a single coordinate chart and has a constant negative curvature.",["Date","2021-06-21T12:00:00.000Z"],[79],"import BlogImage from \"@/components/BlogImage.astro\";\n\nThe (univariate) Gaussian distribution is defined by the following p.d.f.:\n\n$$\n    \\N(x \\mid \\mu, \\sigma) := \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left( - \\frac{(x-\\mu)^2}{2 \\sigma^2} \\right) .\n$$\n\nLet $M := \\\\{ \\N(x \\mid \\mu, \\sigma) : (\\mu, \\sigma) \\in \\R \\times \\R_{> 0} \\\\}$ be the set of all Gaussian p.d.f.s. We would like to treat this set as a smooth manifold and then, additionally, as a Riemannian manifold.\n\nFirst, let's define a coordinate chart for $M$. Let $\\theta : M \\to \\R \\times \\R_{>0}$, defined by $\\N(x \\mid \\mu, \\sigma) \\mapsto (\\mu, \\sigma)$ be such a chart. That is, the coordinate chart $\\theta$ maps $M$ to the open Euclidean upper half-plane $\\\\{ (x, y) : y > 0 \\\\}$. Note that $\\theta$ is a _global_ chart since the Gaussian distribution is uniquely identified by its location and scale (i.e. its mean and standard-deviation). Thus, we can interchangeably write $p \\in M$ or $\\theta := (\\mu, \\sigma) \\in \\R \\times \\R_{>0}$ with a slight abuse of notation. From here, it is clear that $M$ is of dimension $2$ because $\\theta$ gives a homeomorphism from $M$ to $\\R \\times \\R_{>0} \\simeq \\R^2$.\n\nNow let us equip the smooth manifold $M$ with a Riemannian metric, say $g$. The standard choice for $g$ for probability distributions is the Fisher information metric. I.e., in coordinates, it is defined by\n\n$$\n\\begin{align}\n    g_{ij} &= g_{ij}(\\theta) := \\E_{\\N(x \\mid \\mu, \\sigma)} \\left( \\frac{\\partial \\log \\N(x \\mid \\mu, \\sigma)}{\\partial \\theta^i} \\, \\frac{\\partial \\log \\N(x \\mid \\mu, \\sigma)}{\\partial \\theta^j} \\right) \\\\\n        %\n        &= -\\E_{\\N(x \\mid \\mu, \\sigma)} \\left( \\frac{\\partial^2 \\log \\N(x \\mid \\mu, \\sigma)}{\\partial \\theta^i \\, \\partial \\theta^j} \\right) .\n\\end{align}\n$$\n\nIn a matrix form, it is (see [here](https://en.wikipedia.org/wiki/Normal_distribution))\n\n$$\n    G := (g_{ij}) = \\begin{pmatrix}\n            \\frac{1}{\\sigma^2} & 0 \\\\\n            0 & \\frac{2}{\\sigma^2}\n        \\end{pmatrix} .\n$$\n\nIts inverse, denoted by upper indices, $(g^{ij}) = G^{-1}$ is given by\n\n$$\n    g^{ij} = \\begin{pmatrix}\n            \\sigma^2 & 0 \\\\\n            0 & \\frac{\\sigma^2}{2}\n        \\end{pmatrix} .\n$$\n\nNote in particular that the matrix $G$ is positive definite for any $(\\mu, \\sigma)$ and thus gives a notion of inner product in the tangent bundle of $M$. Therefore, the tuple $(M, g)$ is a Riemannian manifold.\n\nOne more structure is needed for computing the curvature(s) of $M$. We need to equip $(M, g)$ with an affine connection. Here, we will use the Levi-Civita connection $\\nabla$ of $g$.\n\n**Note.** _We will use the Einstein summation convention from now on. For example, $\\Gamma^k_{ij} \\Gamma^l*{km} = \\sum_k \\Gamma^k*{ij} \\Gamma^l*{km}$.*\n\n## Christoffel Symbols\n\nThe first order of business is to determine the connection coefficients of $\\nabla$---the Christoffel symbols of the second kind. In coordinates, it is represented by the $3$-dimensional array $(\\Gamma^k_{ij}) \\in \\R^{2 \\times 2 \\times 2}$, and is given by the following formula\n\n$$\n    \\Gamma^k_{ij} := \\frac{1}{2} g^{kl} \\left( \\frac{\\partial g_{jl}}{\\partial \\theta^i} + \\frac{\\partial g_{il}}{\\partial \\theta^j} - \\frac{\\partial g_{ij}}{\\partial \\theta^l} \\right) .\n$$\n\nMoreover, due to the symmetric property of the Levi-Civita connection, the lower indices of $\\Gamma$ is symmetric, i.e. $\\Gamma^k_{ij} = \\Gamma^k_{ji}$ for all $i, j, k = 1, 2$.\n\nLet us begin with $k = 1$. For $i,j = 1$, we have\n\n$$\n\\begin{align}\n    \\Gamma^1_{11} &= \\frac{1}{2} g^{11} \\left( \\frac{\\partial g_{11}}{\\partial \\theta^1} + \\frac{\\partial g_{11}}{\\partial \\theta^1} - \\frac{\\partial g_{11}}{\\partial \\theta^1} \\right) + \\frac{1}{2} \\underbrace{g^{12}}_{=0} \\left( \\frac{\\partial g_{12}}{\\partial \\theta^1} + \\frac{\\partial g_{12}}{\\partial \\theta^1} - \\frac{\\partial g_{11}}{\\partial \\theta^2} \\right) \\\\\n        %\n        &= \\frac{1}{2} \\sigma^2 \\frac{\\partial}{\\partial \\mu} \\left( \\frac{1}{\\sigma^2} \\right) = 0 .\n\\end{align}\n$$\n\nSimilarly, we have $\\Gamma^1_{22} = 0$. For $\\Gamma^1_{12} = \\Gamma^1_{21}$, we have\n\n$$\n\\begin{align}\n    \\Gamma^1_{12} = \\Gamma^1_{21} &= \\frac{1}{2} g^{11} \\left( \\cancel{\\frac{\\partial g_{21}}{\\partial \\theta^1}} + \\frac{\\partial g_{11}}{\\partial \\theta^2} - \\cancel{\\frac{\\partial g_{12}}{\\partial \\theta^1}} \\right) + \\frac{1}{2} \\underbrace{g^{12}}_{=0} \\dots  \\\\\n        %\n        &= \\frac{1}{2} \\sigma^2 \\frac{\\partial}{\\partial \\sigma} \\left( \\frac{1}{\\sigma^2} \\right) \\\\\n        %\n        &= -\\frac{1}{\\sigma} .\n\\end{align}\n$$\n\nNote that in the above, we can immediately cross out partial derivatives that depend on $\\theta^1 = \\mu$ since we know that $g_{ij}$ does not depend on $\\mu$ for all $i, j = 1, 2$. Meanwhile, we know immediately that the second term is zero because $g$ is diagonal---in particular $g^{ij} = 0$ for $i \\neq j$.\n\nNow, for $k=2$, we can easily show (the hardest part is to keep track the indices) that $\\Gamma^2_{12} = \\Gamma^2_{21}=$. Meanwhile,\n\n$$\n\\begin{align}\n    \\Gamma^2_{11} &= \\frac{1}{2} \\underbrace{g^{21}}_{0} \\dots + \\frac{1}{2} g^{22} \\left( \\underbrace{\\frac{\\partial g_{12}}{\\partial \\theta^1}}_{0} + \\underbrace{\\frac{\\partial g_{12}}{\\partial \\theta^1}}_{0} - \\frac{\\partial g_{11}}{\\partial \\theta^2} \\right) \\\\\n        %\n        &= -\\frac{1}{2} \\frac{\\sigma^2}{2} \\frac{\\partial}{\\partial \\sigma} \\left( \\frac{1}{\\sigma^2} \\right) \\\\\n        %\n        &= -\\frac{1}{\\cancel{2}} \\frac{\\cancel{\\sigma^2}}{2} \\left(-\\cancel{2} \\frac{1}{\\sigma^{\\cancel{3}}}\\right) \\\\\n        %\n        &= \\frac{1}{2\\sigma} ,\n\\end{align}\n$$\n\nand similar computation gives $\\Gamma^2_{22} = -\\frac{1}{\\sigma}$.\n\nSo, all in all, $\\Gamma$ is given by\n\n$$\n    \\Gamma^k = \\begin{cases}\n        \\begin{pmatrix}\n            0 & -\\frac{1}{\\sigma} \\\\\n            -\\frac{1}{\\sigma} & 0\n        \\end{pmatrix} & \\text{if } k = 1 \\\\[3pt]\n        %\n        \\begin{pmatrix}\n            \\frac{1}{2\\sigma} & 0 \\\\\n            0 & -\\frac{1}{\\sigma}\n        \\end{pmatrix} & \\text{if } k = 2  .\n    \\end{cases}\n$$\n\n## Sectional Curvature\n\nNow we are ready to compute the curvature of $M$. There are different notions of curvatures, e.g. the Riemann, Ricci curvature tensor, or the scalar curvature. In this post, we focus on the sectional curvature, which is a generalization of the Gaussian curvature in classical surface geometry (i.e. the study of embedded $2$-dimensional surfaces in $\\R^3$).\n\nLet $v, w$ in $T_pM$ be two basis vectors for $T_pM$. The formula of the sectional curvature $\\text{sec}(v, w)$ under $v, w$ is as follows:\n\n$$\n    \\text{sec}(v, w) := \\frac{Rm(v, w, w, v)}{\\inner{v, v} \\inner{w, w} - \\inner{v, w}^2} ,\n$$\n\nwhere $Rm$ is the Riemann curvature tensor, and $\\inner{\\cdot, \\cdot}$ denotes the inner product w.r.t. $g$. Note that $\\text{sec}(v, w)$ is independent of the choice of $(v,w)$, i.e. given another pair of basis vectors $(v_0, w_0)$ of $T_pM$, we have that $\\text{sec}(v_0, w_0) = \\text{sec}(v, w)$.\n\nThe partial derivative operators $\\frac{\\partial}{\\partial \\theta^1} =: \\partial_1$ and $\\frac{\\partial}{\\partial \\theta^2} =: \\partial_2$ under the coordinates $\\theta$ form a basis for $T_pM$. So, let us use them to compute the sectional curvature of $M$. In this case, the formula reads as\n\n$$\n    \\text{sec}(\\partial_1, \\partial_2) = \\frac{Rm(\\partial_1, \\partial_2, \\partial_2, \\partial_1)}{\\inner{\\partial_1, \\partial_1} \\inner{\\partial_2, \\partial_2} - \\inner{\\partial_1, \\partial_2}^2} .\n$$\n\nBut the definition of $Rm$ implies that $Rm(\\partial_1, \\partial_2, \\partial_2, \\partial_1) = R_{1221}$, i.e. the element $1,2,2,1$ of the multidimensional array representation of $Rm$ in coordinates. Moreover, by definition, $g_{ij} = \\inner{\\partial_1, \\partial_2}$. And so:\n\n$$\n    \\text{sec}(\\partial_1, \\partial_2) = \\frac{R_{1221}}{g_{11} g_{22} - (g_{12})^2} = \\frac{R_{1221}}{\\det g} ,\n$$\n\nsince $g$ is symmetric. Note that this is but the definition of the Gaussian curvature---indeed, in dimension $2$, the sectional and the Gaussian curvatures coincide.\n\nWe are now ready to compute $\\text{sec}(\\partial_1, \\partial_2)$. The denominator is easy from our definition of $G$ at the beginning of this post:\n\n$$\n    \\det g = \\frac{1}{\\sigma^2} \\frac{2}{\\sigma^2} = \\frac{2}{\\sigma^4} .\n$$\n\nFor the numerator, we can compute $R_{ijkl}$ via the metric and the Christoffel symbols:\n\n$$\n    R_{ijkl} = g_{lm} \\left( \\frac{\\partial \\Gamma^m_{jk}}{\\partial \\theta^i} - \\frac{\\partial \\Gamma^m_{ik}}{\\partial \\theta^j} + \\Gamma^p_{jk} \\Gamma^m_{ip} - \\Gamma^p_{ik} \\Gamma^m_{jp} \\right) .\n$$\n\nSo, we have\n\n$$\n\\begin{align}\n    R_{1221} &= g_{1m} \\left( \\frac{\\partial \\Gamma^m_{22}}{\\partial \\theta^1} - \\frac{\\partial \\Gamma^m_{12}}{\\partial \\theta^2} + \\Gamma^p_{22} \\Gamma^m_{1p} - \\Gamma^p_{12} \\Gamma^m_{2p} \\right) \\\\\n        %\n        &= g_{1m} \\left( \\frac{\\partial \\Gamma^m_{22}}{\\partial \\mu} - \\frac{\\partial \\Gamma^m_{12}}{\\partial \\sigma} + \\left( \\Gamma^1_{22} \\Gamma^m_{11} + \\Gamma^2_{22} \\Gamma^m_{12} \\right) - \\left( \\Gamma^1_{12} \\Gamma^m_{21} + \\Gamma^2_{12} \\Gamma^m_{22} \\right) \\right) \\\\\n        %\n        &= g_{11} \\left( \\frac{\\partial \\Gamma^1_{22}}{\\partial \\mu} - \\frac{\\partial \\Gamma^1_{12}}{\\partial \\sigma} + \\left( \\Gamma^1_{22} \\Gamma^1_{11} + \\Gamma^2_{22} \\Gamma^1_{12} \\right) - \\left( \\Gamma^1_{12} \\Gamma^1_{21} + \\Gamma^2_{12} \\Gamma^1_{22} \\right) \\right) + \\underbrace{g_{12}}_{=0} \\dots .\n\\end{align}\n$$\n\nNow, we can cross out the partial derivative term w.r.t. $\\mu$ since we know already that none of the $\\Gamma^k_{ij}$ depend on $\\mu$. Moreover, recall that the Christoffel symbols are given by $\\Gamma^1_{12} = \\Gamma^1_{21} = -\\frac{1}{\\sigma}$ and $\\Gamma^2_{11} = \\frac{1}{2\\sigma}$, and $0$ otherwise. Hence,\n\n$$\n\\begin{align}\n    R_{1221} &= g_{11} \\left( - \\frac{\\partial \\Gamma^1_{12}}{\\partial \\sigma} + \\Gamma^2_{22} \\Gamma^1_{12} - \\Gamma^1_{12} \\Gamma^1_{21} \\right) \\\\\n        %\n        &= \\frac{1}{\\sigma^2} \\left( -\\frac{\\partial}{\\partial \\sigma} \\left( -\\frac{1}{\\sigma} \\right) + \\cancel{\\left( -\\frac{1}{\\sigma} \\right)^2} - \\cancel{\\left( -\\frac{1}{\\sigma} \\right)^2} \\right) \\\\\n        %\n        &= \\frac{1}{\\sigma^2} \\left( -\\frac{1}{\\sigma^2} \\right) \\\\\n        %\n        &= -\\frac{1}{\\sigma^4} .\n\\end{align}\n$$\n\nThus, the sectional curvature is given by\n\n$$\n    \\text{sec}(\\partial_1, \\partial_2) = \\frac{-\\frac{1}{\\sigma^4}}{\\frac{2}{\\sigma^4}} = -\\frac{1}{2} .\n$$\n\nNote in particular that this sectional curvature does not depend on both $\\mu$ and $\\sigma$, i.e. it is constant. Hence, $M$ is a manifold of constant negative curvature. I.e., we can think of $M$ as a saddle surface.\n\n## Visualization\n\nThanks to the amazing [`geomstats`](https://github.com/geomstats/geomstats) package, we can visualize $M$ in coordinates easily. The idea is by visualizing the contours of the distances from points in $\\R \\times \\R_{>0}$ to $(0, 1)$, i.e. corresponding to $\\N(x \\mid 0, 1)$---the standard normal.\n\n\u003CBlogImage\n  imagePath='/img/manifold-gaussians/gaussians_geo.png'\n  altText='Geodesic paths of Gaussians.'\n/>\n\nAbove, red points are the discretized steps of geodesics from $\\N(x \\mid 0, 1)$ to other Gaussians with different mean and variance. Indeed, geodesics of $M$ behave similarly like in the Poincaré half-space model---one of the poster children of the hyperbolic geometry.","src/content/post/manifold-gaussians.mdx","912f2d233919aee9","manifold-gaussians.mdx","minkowski-dirichlet",{"id":457,"data":459,"body":464,"filePath":465,"digest":466,"legacyId":467,"deferredRender":23},{"title":460,"description":461,"publishDate":462,"draft":15,"tags":463},"Minkowski's, Dirichlet's, and Two Squares Theorem","Application of Minkowski's Theorem in geometry problems, Dirichlet's Approximation Theorem, and Two Squares Theorem.",["Date","2018-07-25T00:30:00.000Z"],[79],"import BlogImage from \"@/components/BlogImage.astro\";\n\n\u003CBlogImage imagePath='/img/minkowski-dirichlet/forest.svg' altText='A \"forest\"' />\n\nSuppose we are standing at the origin of bounded regular forest in $ \\mathbb{R}^2 $, with diameter of $26$m, and all the trees inside have diameter of $0.16$m. Can we see outside this forest? This problem can be solved using Minkowski's Theorem. We will see the theorem itself first, and we shall see how can we answer that question. Furthermore, Minkowski's Theorem can also be applied to answer two other famous theorems, Dirichlet's Approximation Theorem, and Two Squares Theorem.\n\n**Theorem 1 (Minkowski's Theorem)**  \nLet $ C \\subseteq \\mathbb{R}^d $ be symmetric around the origin, convex, and bounded set. If $ \\text{vol}(C) > 2^d $ then $ C $ contains at least one lattice point different from the origin.\n\n_Proof._ &nbsp;&nbsp; Let $ C' := \\frac{1}{2} C = \\\\{ \\frac{1}{2} c \\, \\vert \\, c \\in C \\\\} $. Assume that there exists non-zero integer $ v \\in \\mathbb{Z}^d \\setminus \\\\{ 0 \\\\} $, such that the intersection between $ C' $ and its translation wrt. $ v $ is non-empty.\n\nPick arbitrary $ x \\in C' \\cap (C' + v) $. Then $ x - v \\in C' $ by construction. By symmetry, $ v - x \\in C' $. As $ C' $ is convex, then line segment between $ x $ and $ v - x $ is in $ C' $. We particularly consider the midpoint of the line segment: $ \\frac{1}{2}x + \\frac{1}{2} (v - x) = \\frac{1}{2} v \\in C' $. This immediately implies that $ v \\in C $ by the definition of $ C' $, which proves the theorem.\n\n$$\n\\qed\n$$\n\nThe claim that there exists non-zero integer $ v \\in \\mathbb{Z}^d \\setminus \\\\{ 0 \\\\} $, such that $ C' \\cap (C' + v) \\neq \\emptyset $ is not proven in this post. One can refer to Matoušek's book for the proof.\n\n\u003CBlogImage\n  imagePath='/img/minkowski-dirichlet/forest_minkowski.svg'\n  altText=\"The Minkowski's theorem.\"\n/>\n\nGiven Minkowski's Theorem, now we can answer our original question. We assume the trees are just lattice points, and our visibility line is now a visibility strip, which has wide of $ 0.16 $m and length of $ 26 $m. We note that the preconditions of Minkowski's Theorem are satisfied by this visibility strip, which has the volume of $ \\approx 4.16 > 4 = 2^d $. Therefore, there exists a lattice point other than the origin inside our visibility strip. Thus our vision outside is blocked by the tree.\n\nNow we look at two theorems that can be proven using Minkowski's Theorem. The first one is about approximation of real number with a rational.\n\n**Theorem 2 (Dirichlet's Approximation Theorem)**  \nLet $ \\alpha \\in \\mathbb{R} $. Then for all $ N \\in \\mathbb{N} $, there exists $ m \\in \\mathbb{Z}, n \\in \\mathbb{N} $ with $ n \\leq N $ such that:\n\n$$\n\\left \\vert \\, \\alpha - \\frac{m}{n} \\right \\vert \\lt \\frac{1}{nN} \\enspace .\n$$\n\n_Proof._ &nbsp;&nbsp; Consider $ C := \\\\{ (x, y) \\in \\mathbb{R}^2 \\, \\vert \\, -N-\\frac{1}{2} \\leq x \\leq N+\\frac{1}{2}, \\vert \\alpha x - y \\vert \\lt \\frac{1}{N} \\\\} $. By inspection on the figure below, we can observe that $ C $ is convex, bounded, and symmetric around the origin.\n\n\u003CBlogImage\n  imagePath='/img/minkowski-dirichlet/dirichlet.svg'\n  altText=\"The Dirichlet's approximation theorem.\"\n/>\n\nObserve also that the area of $ C $ is $ \\text{vol}(C) = \\frac{2}{N} (2N + 1) = 4 + \\frac{2}{N} \\gt 4 = 2^d $. Thus this construction satisfied the Minkowski's Theorem's preconditions. Therefore there exists lattice point $ (n, m) \\neq (0, 0) $. As $ C $ is symmetric, we can always assume $ n \\gt 0 $ thus $ n \\in \\mathbb{N} $. By definition of $ C $, $ n \\leq N+\\frac{1}{2} \\implies n \\leq N $ as $ N \\in \\mathbb{N} $. Futhermore, we have $ \\vert \\alpha n - m \\vert \\lt \\frac{1}{N} $. This implies $ \\left\\vert \\alpha - \\frac{m}{n} \\right\\vert \\lt \\frac{1}{nN} $ which conclude the proof.\n\n$$\n\\qed\n$$\n\nOur second application is the theorem saying that prime number $ p \\equiv 1 \\, (\\text{mod } 4) $ can be written as a sum of two squares. For this we need the General Minkowski's Theorem, which allows us to use arbitrary basis for our lattice.\n\n**Theorem 3 (General Minkowski's Theorem)**\nLet $ C \\subseteq \\mathbb{R}^d $ be symmetric around the origin, convex, and bounded set. Let $ \\Gamma $ be the lattice in $ \\mathbb{R}^d $. If $ \\text{vol}(C) > 2^d \\,\\text{vol}(\\Gamma) = 2^d \\det \\Gamma $, then $ C $ contains at least one lattice point in $ \\Gamma $ different from the origin.\n\n$$\n\\qed\n$$\n\n**Theorem 4 (Two Squares Theorem)**  \nEvery prime number $ p \\equiv 1 \\, (\\text{mod } 4) $ can be written by the sum of two squares $ p = a^2 + b^2 $ where $ a, b \\in \\mathbb{Z} $.\n\n_Proof._ &nbsp;&nbsp; We need intermediate result which will not be proven here (refer to [1] for the proof): $ -1 $ is a quadratic residue modulo $ p $, that is, there exists $ q \\lt p $ such that $ q^2 \\equiv -1 \\, (\\text{mod } p) $.\n\nFix $ q $ and take the following basis for our lattice: $ z_1 := (1, q), \\, z_2 := (0, p) $. The volume of this lattice is: $ \\det \\Gamma = \\det \\begin{bmatrix} 1 & 0 \\\\ q & p \\end{bmatrix} = p $.\n\nDefine a convex, symmetric, and bounded body $ C := \\\\{ (x, y) \\in \\mathbb{R}^2 \\, \\vert \\, x^2 + y^2 \\lt 2p \\\\} $, i.e. $ C $ is an open ball around the origin with radius $ \\sqrt{2p} $. Note:\n\n$$\n\\text{vol}(C) = \\pi r^2 \\approx 6.28p \\gt 4p = 2^2 p = 2^d \\det \\Gamma \\enspace ,\n$$\n\nthus General Minkowski's Theorem applies and there exists a lattice point $ (a, b) = i z_1 + j z_2 = (i, iq + jp) \\neq (0, 0) $. Notice:\n\n$$\n\\begin{align}\n\na^2 + b^2 &= i^2 + i^2 q^2 + 2ijpq + j^2 p^2 \\\\\n          &\\equiv i^2 + i^2q^2 \\, (\\text{mod } p) \\\\\n          &\\equiv i^2(1+q^2) \\, (\\text{mod } p) \\\\\n          &\\equiv i^2(1-1) \\, (\\text{mod } p) \\\\\n          &\\equiv 0 \\, (\\text{mod } p) \\enspace .\n\n\\end{align}\n$$\n\nTo go from 3rd to 4th line, we use our very first assumption, i.e. $ q^2 \\equiv -1 \\, (\\text{mod } p) $. Therefore $ a^2 + b^2 $ has to be divisible by $ p $. Also, as $ (a, b) $ is in $ C $ this implies $ a^2 + b^2 \\lt 2p $ by definition. Thus the only choice is $ a^2 + b^2 = p $. This proves the theorem.\n\n$$\n\\qed\n$$\n\n## References\n\n1. Matoušek, Jiří. Lectures on discrete geometry. Vol. 212. New York: Springer, 2002.","src/content/post/minkowski-dirichlet.mdx","6f327755e821ca7f","minkowski-dirichlet.mdx","mle-vs-map",{"id":468,"data":470,"body":475,"filePath":476,"digest":477,"legacyId":478,"deferredRender":23},{"title":471,"description":472,"publishDate":473,"draft":15,"tags":474},"MLE vs MAP: the connection between Maximum Likelihood and Maximum A Posteriori Estimation","In this post, we will see what is the difference between Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP).",["Date","2017-01-01T10:40:00.000Z"],[17,18],"Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP), are both a method for estimating some variable in the setting of probability distributions or graphical models. They are similar, as they compute a single estimate, instead of a full distribution.\n\nMLE, as we, who have already indulge ourselves in Machine Learning, would be familiar with this method. Sometimes, we even use it without knowing it. Take for example, when fitting a Gaussian to our dataset, we immediately take the sample mean and sample variance, and use it as the parameter of our Gaussian. This is MLE, as, if we take the derivative of the Gaussian function with respect to the mean and variance, and maximizing it (i.e. setting the derivative to zero), what we get is functions that are calculating sample mean and sample variance. Another example, most of the optimization in Machine Learning and Deep Learning (neural net, etc), could be interpreted as MLE.\n\nSpeaking in more abstract term, let's say we have a likelihood function $P(X \\vert \\theta)$. Then, the MLE for $\\theta$, the parameter we want to infer, is:\n\n$$\n\\begin{align}\n\n\\theta_{MLE} &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} P(X \\vert \\theta) \\\\[10pt]\n             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\prod_i P(x_i \\vert \\theta)\n\n\\end{align}\n$$\n\nAs taking a product of some numbers less than 1 would approaching 0 as the number of those numbers goes to infinity, it would be not practical to compute, because of computation underflow. Hence, we will instead work in the log space, as logarithm is monotonically increasing, so maximizing a function is equal to maximizing the log of that function.\n\n$$\n\\begin{align}\n\n\\theta_{MLE} &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\log P(X \\vert \\theta) \\\\[10pt]\n             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\log \\prod_i P(x_i \\vert \\theta) \\\\[10pt]\n             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\sum_i \\log P(x_i \\vert \\theta)\n\n\\end{align}\n$$\n\nTo use this framework, we just need to derive the log likelihood of our model, then maximizing it with regard of $\\theta$ using our favorite optimization algorithm like Gradient Descent.\n\nUp to this point, we now understand what does MLE do. From here, we could draw a parallel line with MAP estimation.\n\nMAP usually comes up in Bayesian setting. Because, as the name suggests, it works on a posterior distribution, not only the likelihood.\n\nRecall, with Bayes' rule, we could get the posterior as a product of likelihood and prior:\n\n$$\n\\begin{align}\n\nP(\\theta \\vert X) &= \\frac{P(X \\vert \\theta) P(\\theta)}{P(X)} \\\\[10pt]\n                  &\\propto P(X \\vert \\theta) P(\\theta)\n\n\\end{align}\n$$\n\nWe are ignoring the normalizing constant as we are strictly speaking about optimization here, so proportionality is sufficient.\n\nIf we replace the likelihood in the MLE formula above with the posterior, we get:\n\n$$\n\\begin{align}\n\n\\theta_{MAP} &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} P(X \\vert \\theta) P(\\theta) \\\\[10pt]\n             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\log P(X \\vert \\theta) + \\log P(\\theta) \\\\[10pt]\n             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\log \\prod_i P(x_i \\vert \\theta) + \\log P(\\theta) \\\\[10pt]\n             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\sum_i \\log P(x_i \\vert \\theta) + \\log P(\\theta)\n\n\\end{align}\n$$\n\nComparing both MLE and MAP equation, the only thing differs is the inclusion of prior $P(\\theta)$ in MAP, otherwise they are identical. What it means is that, the likelihood is now weighted with some weight coming from the prior.\n\nLet's consider what if we use the simplest prior in our MAP estimation, i.e. uniform prior. This means, we assign equal weights everywhere, on all possible values of the $\\theta$. The implication is that the likelihood equivalently weighted by some constants. Being constant, we could be ignored from our MAP equation, as it will not contribute to the maximization.\n\nLet's be more concrete, let's say we could assign six possible values into $\\theta$. Now, our prior $P(\\theta)$ is $\\frac{1}{6}$ everywhere in the distribution. And consequently, we could ignore that constant in our MAP estimation.\n\n$$\n\\begin{align}\n\n\\theta_{MAP} &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\sum_i \\log P(x_i \\vert \\theta) + \\log P(\\theta) \\\\[10pt]\n             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\sum_i \\log P(x_i \\vert \\theta) + const \\\\[10pt]\n             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\sum_i \\log P(x_i \\vert \\theta) \\\\[10pt]\n             &= \\theta_{MLE}\n\n\\end{align}\n$$\n\nWe are back at MLE equation again!\n\nIf we use different prior, say, a Gaussian, then our prior is not constant anymore, as depending on the region of the distribution, the probability is high or low, never always the same.\n\nWhat we could conclude then, is that MLE is a special case of MAP, where the prior is uniform!","src/content/post/mle-vs-map.mdx","fe9df83a091a72de","mle-vs-map.mdx","natural-gradient",{"id":479,"data":481,"body":486,"filePath":487,"digest":488,"legacyId":489,"deferredRender":23},{"title":482,"description":483,"publishDate":484,"draft":15,"tags":485},"Natural Gradient Descent","Intuition and derivation of natural gradient descent.",["Date","2018-03-14T11:00:00.000Z"],[17],"import BlogImage from \"@/components/BlogImage.astro\";\n\nPreviously, we looked at the Fisher Information Matrix. We saw that it is equal to the negative expected Hessian of log likelihood. Thus, the immediate application of Fisher Information Matrix is as drop-in replacement of Hessian in second order optimization algorithm. In this article, we will look deeper at the intuition on what excatly is the Fisher Information Matrix represents and what is the interpretation of it.\n\n## Distribution Space\n\nAs per previous article, we have a probabilistic model represented by its likelihood $p(x \\vert \\theta)$. We want to maximize this likelihood function to find the most likely parameter $\\theta$. Equivalent formulation would be to minimize the loss function $\\mathcal{L}(\\theta)$, which is the negative log likelihood.\n\nUsual way to solve this optimization is to use gradient descent. In this case, we are taking step which direction is given by $-\\nabla_\\theta \\mathcal{L}(\\theta)$. This is the steepest descent direction around the local neighbourhood of the current value of $\\theta$ in the parameter space. Formally, we have\n\n$$\n\\frac{-\\nabla_\\theta \\mathcal{L}(\\theta)}{\\lVert \\nabla_\\theta \\mathcal{L}(\\theta) \\rVert} = \\lim_{\\epsilon \\to 0} \\frac{1}{\\epsilon} \\mathop{\\text{arg min}}_{d \\text{ s.t. } \\lVert d \\rVert \\leq \\epsilon} \\mathcal{L}(\\theta + d) \\, .\n$$\n\nThe above expression is saying that the steepest descent direction in parameter space is to pick a vector $d$, such that the new parameter $\\theta + d$ is within the $\\epsilon$-neighbourhood of the current parameter $\\theta$, and we pick $d$ that minimize the loss. Notice the way we express this neighbourhood is by the means of Euclidean norm. Thus, the optimization in gradient descent is dependent to the Euclidean geometry of the parameter space.\n\nMeanwhile, if our objective is to minimize the loss function (maximizing the likelihood), then it is natural that we taking step in the space of all possible likelihood, realizable by parameter $\\theta$. As the likelihood function itself is a probability distribution, we call this space distribution space. Thus it makes sense to take the steepest descent direction in this distribution space instead of parameter space.\n\nWhich metric/distance then do we need to use in this space? A popular choice would be KL-divergence. KL-divergence measure the \"closeness\" of two distributions. Although as KL-divergence is non-symmetric and thus not a true metric, we can use it anyway. This is because as $d$ goes to zero, KL-divergence is asymptotically symmetric. So, within a local neighbourhood, KL-divergence is approximately symmetric [1].\n\nWe can see the problem when using only Euclidean metric in parameter space from the illustrations below. Consider a Gaussian parameterized by only its mean and keep the variance fixed to 2 and 0.5 for the first and second image respectively:\n\n\u003CBlogImage\n  imagePath='/img/natural-gradient/param_space_dist.png'\n  altText='A parametrization of Gaussians.'\n  fullWidth\n/>\n\n\u003CBlogImage\n  imagePath='/img/natural-gradient/param_space_dist2.png'\n  altText='Another parametrization of Gaussians.'\n  fullWidth\n/>\n\nIn both images, the distance of those Gaussians are the same, i.e. 4, according to Euclidean metric (red line). However, clearly in distribution space, i.e. when we are taking into account the shape of the Gaussians, the distance is different in the first and second image. In the first image, the KL-divergence should be lower as there is more overlap between those Gaussians. Therefore, if we only work in parameter space, we cannot take into account this information about the distribution realized by the parameter.\n\n## Fisher Information and KL-divergence\n\nOne question still needs to be answered is what exactly is the connection between Fisher Information Matrix and KL-divergence? It turns out, Fisher Information Matrix defines the local curvature in distribution space for which KL-divergence is the metric.\n\n**Claim:**\nFisher Information Matrix $\\text{F}$ is the Hessian of KL-divergence between two distributions $p(x \\vert \\theta)$ and $p(x \\vert \\theta')$, with respect to $\\theta'$, evaluated at $\\theta' = \\theta$.\n\n_Proof._ &nbsp;&nbsp; KL-divergence can be decomposed into entropy and cross-entropy term, i.e.:\n\n$$\n\\text{KL} [p(x \\vert \\theta) \\, \\Vert \\, p(x \\vert \\theta')] = \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} [ \\log p(x \\vert \\theta) ] - \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} [ \\log p(x \\vert \\theta') ] \\, .\n$$\n\nThe first derivative wrt. $\\theta'$ is:\n\n$$\n\\begin{align}\n    \\nabla_{\\theta'} \\text{KL}[p(x \\vert \\theta) \\, \\Vert \\, p(x \\vert  \\theta')] &= \\nabla_{\\theta'} \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} [ \\log p(x \\vert \\theta) ] - \\nabla_{\\theta'} \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} [ \\log p(x \\vert \\theta') ] \\\\[5pt]\n        &= - \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} [ \\nabla_{\\theta'} \\log p(x \\vert \\theta') ] \\\\[5pt]\n        &= - \\int p(x \\vert \\theta) \\nabla_{\\theta'} \\log p(x \\vert \\theta') \\, \\text{d}x \\, .\n\\end{align}\n$$\n\nThe second derivative is:\n\n$$\n\\begin{align}\n    \\nabla_{\\theta'}^2 \\, \\text{KL}[p(x \\vert \\theta) \\, \\Vert \\, p(x \\vert \\theta')] &= - \\int p(x \\vert \\theta) \\, \\nabla_{\\theta'}^2 \\log p(x \\vert \\theta') \\, \\text{d}x \\\\[5pt]\n\\end{align}\n$$\n\nThus, the Hessian wrt. $\\theta'$ evaluated at $\\theta' = \\theta$ is:\n\n$$\n\\begin{align}\n    \\text{H}_{\\text{KL}[p(x \\vert \\theta) \\, \\Vert \\, p(x \\vert \\theta')]} &= - \\int p(x \\vert \\theta) \\, \\left. \\nabla_{\\theta'}^2 \\log p(x \\vert \\theta') \\right\\vert_{\\theta' = \\theta} \\, \\text{d}x \\\\[5pt]\n        &= - \\int p(x \\vert \\theta) \\, \\text{H}_{\\log p(x \\vert \\theta)} \\, \\text{d}x \\\\[5pt]\n        &= - \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} [\\text{H}_{\\log p(x \\vert \\theta)}] \\\\[5pt]\n        &= \\text{F} \\, .\n\\end{align}\n$$\n\nThe last line follows from the previous post about Fisher Information Matrix, in which we showed that the negative expected Hessian of log likelihood is the Fisher Information Matrix.\n\n$$\n\\qed\n$$\n\n## Steepest Descent in Distribution Space\n\nNow we are ready to use the Fisher Information Matrix to enhance the gradient descent. But first, we need to derive the Taylor series expansion for KL-divergence around $\\theta$.\n\n**Claim:**\nLet $d \\to 0$. The second order Taylor series expansion of KL-divergence is $\\text{KL}[p(x \\vert \\theta) \\, \\Vert \\, p(x \\vert \\theta + d)] \\approx \\frac{1}{2} d^\\text{T} \\text{F} d$.\n\n_Proof._ &nbsp;&nbsp; We will use $p_{\\theta}$ as a notational shortcut for $p(x \\vert \\theta)$. By definition, the second order Taylor series expansion of KL-divergence is:\n\n$$\n\\begin{align}\n    \\text{KL}[p_{\\theta} \\, \\Vert \\, p_{\\theta + d}] &\\approx \\text{KL}[p_{\\theta} \\, \\Vert \\, p_{\\theta}] + (\\left. \\nabla_{\\theta'} \\text{KL}[p_{\\theta} \\, \\Vert \\, p_{\\theta'}] \\right\\vert_{\\theta' = \\theta})^\\text{T} d + \\frac{1}{2} d^\\text{T} \\text{F} d \\\\[5pt]\n        &= \\text{KL}[p_{\\theta} \\, \\Vert \\, p_{\\theta}] - \\mathop{\\mathbb{E}}_{p(x \\vert \\theta)} [ \\nabla_\\theta \\log p(x \\vert \\theta) ]^\\text{T} d + \\frac{1}{2} d^\\text{T} \\text{F} d \\\\[5pt]\n\\end{align}\n$$\n\nNotice that the first term is zero as it is the same distribution. Furthermore, from the previous post, we saw that the expected value of the gradient of log likelihood, which is exactly the gradient of KL-divergence as shown in the previous proof, is also zero. Thus the only thing left is:\n\n$$\n\\begin{align}\n    \\text{KL}[p(x \\vert \\theta) \\, \\Vert \\, p(x \\vert \\theta + d)] &\\approx \\frac{1}{2} d^\\text{T} \\text{F} d \\, .\n\\end{align}\n$$\n\n$$\n\\qed\n$$\n\nNow, we would like to know what is update vector $d$ that minimizes the loss function $\\mathcal{L} (\\theta)$ in distribution space, so that we know in which direction decreases the KL-divergence the most. This is analogous to the method of steepest descent, but in distribution space with KL-divergence as metric, instead of the usual parameter space with Euclidean metric. For that, we do this minimization:\n\n$$\nd^* = \\mathop{\\text{arg min}}_{d \\text{ s.t. } \\text{KL}[p_\\theta \\Vert p_{\\theta + d}] = c} \\mathcal{L} (\\theta + d) \\, ,\n$$\n\nwhere $c$ is some constant. The purpose of fixing the KL-divergence to some constant is to make sure that we move along the space with constant speed, regardless the curvature. Further benefit is that this makes the algorithm more robust to the reparametrization of the model, i.e. the algorithm does not care how the model is parametrized, it only cares about the distribution induced by the parameter [3].\n\nIf we write the above minimization in Lagrangian form, with constraint KL-divergence approximated by its second order Taylor series expansion and approximate $\\mathcal{L}(\\theta + d)$ with its first order Taylor series expansion, we get:\n\n$$\n\\begin{align}\nd^* &= \\mathop{\\text{arg min}}_d \\, \\mathcal{L} (\\theta + d) + \\lambda \\, (\\text{KL}[p_\\theta \\Vert p_{\\theta + d}] - c) \\\\\n    &\\approx \\mathop{\\text{arg min}}_d \\, \\mathcal{L}(\\theta) + \\nabla_\\theta \\mathcal{L}(\\theta)^\\text{T} d + \\frac{1}{2} \\lambda \\, d^\\text{T} \\text{F} d - \\lambda c \\, .\n\\end{align}\n$$\n\nTo solve this minimization, we set its derivative wrt. $d$ to zero:\n\n$$\n\\begin{align}\n0 &= \\frac{\\partial}{\\partial d} \\mathcal{L}(\\theta) + \\nabla_\\theta \\mathcal{L}(\\theta)^\\text{T} d + \\frac{1}{2} \\lambda \\, d^\\text{T} \\text{F} d - \\lambda c \\\\[5pt]\n    &= \\nabla_\\theta \\mathcal{L}(\\theta) + \\lambda \\, \\text{F} d \\\\[5pt]\n    \\lambda \\, \\text{F} d &= -\\nabla_\\theta \\mathcal{L}(\\theta) \\\\[5pt]\n    d &= -\\frac{1}{\\lambda} \\text{F}^{-1} \\nabla_\\theta \\mathcal{L}(\\theta) \\\\[5pt]\n\\end{align}\n$$\n\nUp to constant factor of $\\frac{1}{\\lambda}$, we get the optimal descent direction, i.e. the opposite direction of gradient while taking into account the local curvature in distribution space defined by $\\text{F}^{-1}$. We can absorb this constant factor into the learning rate.\n\n**Definition:**\nNatural gradient is defined as\n\n$$\n\\tilde{\\nabla}_\\theta \\mathcal{L}(\\theta) = \\text{F}^{-1} \\nabla_\\theta \\mathcal{L}(\\theta) \\, .\n$$\n\n$$\n\\qed\n$$\n\nAs corollary, we have the following algorithm:\n\n**Algorithm: Natural Gradient Descent**\n\n1. Repeat:\n   1. Do forward pass on our model and compute loss $\\mathcal{L}(\\theta)$.\n   2. Compute the gradient $\\nabla_\\theta \\mathcal{L}(\\theta)$.\n   3. Compute the Fisher Information Matrix $\\text{F}$, or its empirical version (wrt. our training data).\n   4. Compute the natural gradient $\\tilde{\\nabla}_\\theta \\mathcal{L}(\\theta) = \\text{F}^{-1} \\nabla_\\theta \\mathcal{L}(\\theta)$.\n   5. Update the parameter: $\\theta = \\theta - \\alpha \\, \\tilde{\\nabla}_\\theta \\mathcal{L}(\\theta)$, where $\\alpha$ is the learning rate.\n2. Until convergence.\n\n## Discussion\n\nIn the above very simple model with low amount of data, we saw that we can implement natural gradient descent easily. But how easy is it to do this in the real world? As we know, the number of parameters in deep learning models is very large, within millions of parameters. The Fisher Information Matrix for these kind of models is then infeasible to compute, store, or invert. This is the same problem as why second order optimization methods are not popular in deep learning.\n\nOne way to get around this problem is to approximate the Fisher/Hessian instead. Method like ADAM [4] computes the running average of first and second moment of the gradient. First moment can be seen as momentum which is not our interest in this article. The second moment is approximating the Fisher Information Matrix, but constrainting it to be diagonal matrix. Thus in ADAM, we only need $O(n)$ space to store (the approximation of) $\\text{F}$ instead of $O(n^2)$ and the inversion can be done in $O(n)$ instead of $O(n^3)$. In practice ADAM works really well and is currently the _de facto_ standard for optimizing deep neural networks.\n\n## References\n\n1. Martens, James. \"New insights and perspectives on the natural gradient method.\" arXiv preprint arXiv:1412.1193 (2014).\n2. Ly, Alexander, et al. \"A tutorial on Fisher information.\" Journal of Mathematical Psychology 80 (2017): 40-55.\n3. Pascanu, Razvan, and Yoshua Bengio. \"Revisiting natural gradient for deep networks.\" arXiv preprint arXiv:1301.3584 (2013).\n4. Kingma, Diederik P., and Jimmy Ba. \"Adam: A method for stochastic optimization.\" arXiv preprint arXiv:1412.6980 (2014).","src/content/post/natural-gradient.mdx","4df920157c79a150","natural-gradient.mdx","nn-optimization",{"id":490,"data":492,"body":498,"filePath":499,"digest":500,"legacyId":501,"deferredRender":23},{"title":493,"description":494,"publishDate":495,"draft":15,"tags":496},"Beyond SGD: Gradient Descent with Momentum and Adaptive Learning Rate","There are many attempts to improve Gradient Descent: some add momentum, some add adaptive learning rate. Let's see what's out there in the realm of neural nets optimization.",["Date","2016-06-22T08:55:00.000Z"],[17,42,43,44,497],"optimization","Last time, we implemented Minibatch Gradient Descent to train our neural nets model. Using that post as the base, we will look into another optimization algorithms that are popular out there for training neural nets.\n\nI've since made an update to the last post's SGD codes. Mainly, making the algorithms to use random batch in each iteration, not the whole dataset. However, the problem set and the neural nets model are still the same. Let's refresh the code:\n\n```python\ndef get_minibatch(X, y, minibatch_size):\n    minibatches = []\n\n    X, y = shuffle(X, y)\n\n    for i in range(0, X.shape[0], minibatch_size):\n        X_mini = X[i:i + minibatch_size]\n        y_mini = y[i:i + minibatch_size]\n\n        minibatches.append((X_mini, y_mini))\n\n    return minibatches\n\n\ndef sgd(model, X_train, y_train, minibatch_size):\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        grad = get_minibatch_grad(model, X_mini, y_mini)\n\n        for layer in grad:\n            model[layer] += alpha * grad[layer]\n\n    return model\n```\n\n## SGD + Momentum\n\nImagine a car. The car is going through a mountain range. Being a mountain range, naturally the terrain is hilly. Up and down, up and down. But we, the driver of that car, only want to see the deepest valley of the mountain. So, we want to stop at the part of the road that has the lowest elevation.\n\nOnly, there's a problem: the car is just a box with wheels! So, we can't accelerate and brake at our will, we're at the mercy of the nature! So, we decided to start from the very top of the mountain road and pray that Netwon blesses our journey.\n\nWe're moving now! As our \"car\" moving downhill, it's gaining more and more speed. We find that we're going to get through a small hill. Will this hill stop us? Not quite! Because we have been gaining a lot of momentum! So, we pass that small hill. And another small hill after that. And another. And another...\n\nFinally, after seems like forever, we find ourselves facing very tall hill. Maybe it's tall because it's at the bottom of the mountain range? Nevertheless, the hill is just too much for our \"car\". Finally it stops. And it's true! We could already see the beautiful deepest valley of the mountain!\n\nThat's exactly how momentum plays part in SGD. It uses physical law of motion to go pass through local optima (small hills). Intuitively, adding momentum will also make the convergence faster, as we're accumulating speed, so our Gradient Descent step could be larger, compared to SGD's constant step.\n\nNow the code!\n\n```python\ndef momentum(model, X_train, y_train, minibatch_size):\n    velocity = {k: np.zeros_like(v) for k, v in model.items()}\n    gamma = .9\n\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        grad = get_minibatch_grad(model, X_mini, y_mini)\n\n        for layer in grad:\n            velocity[layer] = gamma * velocity[layer] + alpha * grad[layer]\n            model[layer] += velocity[layer]\n\n    return model\n```\n\nWhat we do is to create a new velocity variable to store our momentum for every parameter. The update of the velocity is given the old velocity value and new Gradient Descent step `alpha * grad`. We also decay our past velocity so that we only consider the most recent velocities with `gamma = .9`.\n\n## Nesterov Momentum\n\nVery similar with momentum method above, Nesterov Momentum add one little different bit to the momentum calculation. Instead of calculating gradient of the current position, it calculates the gradient at the approximated new position.\n\nIntuitively, because we have some momentum applied to our \"car\", then at the current position, we know where will our \"car\" end up one more minute from the current time, ignoring any other variables.\n\nSo, Nesterov Momentum exploits that knowledge, and instead of using the current position's gradient, it uses the next approximated position's gradient with the hope that it will give us better information when we're taking the next step.\n\n```python\ndef nesterov(model, X_train, y_train, minibatch_size):\n    velocity = {k: np.zeros_like(v) for k, v in model.items()}\n    gamma = .9\n\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        model_ahead = {k: v + gamma * velocity[k] for k, v in model.items()}\n        grad = get_minibatch_grad(model_ahead, X_mini, y_mini)\n\n        for layer in grad:\n            velocity[layer] = gamma * velocity[layer] + alpha * grad[layer]\n            model[layer] += velocity[layer]\n\n    return model\n```\n\nLooking at the code, the only difference is that now we're computing the gradient using `model_ahead`: approximated next state of our model parameters that we calculated by adding the momentum to the current parameters.\n\n## Adagrad\n\nNow, we're entering a different realm. Let's forget about our disfunctional \"car\"! We're going to approach the Gradient Descent from different angle that we've been ignoring so far: the learning rate `alpha`.\n\nThe problem with learning rate in Gradient Descent is that it's constant and affecting all of our parameters. What happen if we know that we should slow down or speed up? What happen if we know that we should accelerate more in this direction and decelerate in that direction? Using our standard SGD, we're out of luck.\n\nThat's why Adagrad was invented. It's trying to solve that very problem.\n\n```python\ndef adagrad(model, X_train, y_train, minibatch_size):\n    cache = {k: np.zeros_like(v) for k, v in model.items()}\n\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        grad = get_minibatch_grad(model, X_mini, y_mini)\n\n        for k in grad:\n            cache[k] += grad[k]**2\n            model[k] += alpha * grad[k] / (np.sqrt(cache[k]) + eps)\n\n    return model\n```\n\nNote that the parameters update is pointwise operation, hence the learning rate is adaptive **per-parameter**.\n\nWhat we do is to accumulate the sum of squared of all of our parameters' gradient, and use that to normalize the learning rate `alpha`, so that now our alpha could be smaller or larger depending on how the past gradients behaved: parameters that updated a lot will be slowed down while parameters that received little updates will be have bigger learning rate to accelerate the learning process.\n\nOne note to the implementation, the `eps` there is useful to combat the division by zero, so that our optimization becomes numerically stable. Usually it's set with considerably small value, like `1e-8`.\n\n## RMSprop\n\nIf you notice, at the gradient accumulation part in Adagrad `cache[k] += grad[k]**2`, it's monotonically increasing (hint: sum and squared). This could be problematic as the learning rate will be monotonically decreasing to the point that the learning stops altogether because of the very tiny learning rate.\n\nTo combat that problem, RMSprop decay the past accumulated gradient, so only a portion of past gradients are considered. Now, instead of considering all of the past gradients, RMSprop behaves like moving average.\n\n```python\ndef rmsprop(model, X_train, y_train, minibatch_size):\n    cache = {k: np.zeros_like(v) for k, v in model.items()}\n    gamma = .9\n\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        grad = get_minibatch_grad(model, X_mini, y_mini)\n\n        for k in grad:\n            cache[k] = gamma * cache[k] + (1 - gamma) * (grad[k]**2)\n            model[k] += alpha * grad[k] / (np.sqrt(cache[k]) + eps)\n\n    return model\n```\n\nThe only difference compared to Adagrad is how we calculate the cache. Here, we're take `gamma` portion of past accumulated sum of squared gradient, and take `1 - gamma` portion of the current squared gradient. By doing this, the accumulated gradient won't be aggresively monotonically increasing, depending on the gradients in the moving average window.\n\n## Adam\n\nAdam is the latest state of the art of first order optimization method that's widely used in the real world. It's a modification of RMSprop. Loosely speaking, Adam is RMSprop with momentum. So, Adam tries to combine the best of both world of momentum and adaptive learning rate.\n\n```python\ndef adam(model, X_train, y_train, minibatch_size):\n    M = {k: np.zeros_like(v) for k, v in model.items()}\n    R = {k: np.zeros_like(v) for k, v in model.items()}\n    beta1 = .9\n    beta2 = .999\n\n    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n\n    for iter in range(1, n_iter + 1):\n        t = iter\n        idx = np.random.randint(0, len(minibatches))\n        X_mini, y_mini = minibatches[idx]\n\n        grad = get_minibatch_grad(model, X_mini, y_mini)\n\n        for k in grad:\n            M[k] = beta1 * M[k] + (1. - beta1) * grad[k]\n            R[k] = beta2 * R[k] + (1. - beta2) * grad[k]**2\n\n            m_k_hat = M[k] / (1. - beta1**(t))\n            r_k_hat = R[k] / (1. - beta2**(t))\n\n            model[k] += alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n\n    return model\n```\n\nNotice in the code, we still retain some RMSprop's codes, namely when we calculate `R`. We also add some codes that are similar to how we compute momentum in the form of `M`. Then, for the parameters update, it's the combination of momentum method and adaptive learning rate method: add the momentum, and normalize the learning rate using the moving average squared gradient.\n\nAdam also has a bias correction mechanism, it's calculated in `m_k_hat` and `r_k_hat`. It's useful to make the convergence faster, at several first iterations. The reason is that we initialized `M` and `R` with zero, hence it will be biased toward zero in several first iterations, until they're fully _warmed up_. The solution is to correct the bias and get the unbiased estimate of `M` and `R`. Please refer to the original paper, section 3: https://arxiv.org/pdf/1412.6980.\n\nAs for the recommended value for the hyperparameter: `beta1 = 0.9`, `beta2 = 0.999`, `alpha = 1e-3`, and `eps = 1e-8`.\n\n## Test and Comparison\n\nWith our bag full of those algorithms, let's compare them using our previous problem in the last post. Here's the setup:\n\n```python\nn_iter = 100\neps = 1e-8 # Smoothing to avoid division by zero\nminibatch_size = 50\nn_experiment = 10\n```\n\nWe will run the algorithms to optimize our neural nets for 100 epochs each, and we repeat them 3 times and average the accuracy score.\n\n```\nalpha = 0.5\n\nsgd => mean accuracy: 0.4061333333333333, std: 0.15987773105998498\nadam => mean accuracy: 0.8607999999999999, std: 0.015892975387468082\nnesterov => mean accuracy: 0.47680000000000006, std: 5.551115123125783e-17\nrmsprop => mean accuracy: 0.8506666666666667, std: 0.007224649164876814\nadagrad => mean accuracy: 0.8754666666666667, std: 0.002639865316429748\nmomentum => mean accuracy: 0.3152, std: 0.11427592339012915\n\n========================================================================\n\nalpha = 1e-2\n\nnesterov => mean accuracy: 0.8621333333333334, std: 0.024721021194297126\nrmsprop => mean accuracy: 0.8727999999999999, std: 0.010182337649086262\nsgd => mean accuracy: 0.8784000000000001, std: 0.0026127890589687525\nadam => mean accuracy: 0.8709333333333333, std: 0.01112993960251158\nmomentum => mean accuracy: 0.8554666666666666, std: 0.016657597532524156\nadagrad => mean accuracy: 0.8786666666666667, std: 0.001359738536958064\n\n========================================================================\n\nalpha = 1e-5\n\nadagrad => mean accuracy: 0.504, std: 0.2635737973825673\nsgd => mean accuracy: 0.6509333333333334, std: 0.1101414040626362\nnesterov => mean accuracy: 0.8666666666666667, std: 0.016110727964792775\nrmsprop => mean accuracy: 0.30693333333333334, std: 0.028898596659507347\nmomentum => mean accuracy: 0.8613333333333334, std: 0.02526728231439929\nadam => mean accuracy: 0.43039999999999995, std: 0.0842928229447798\n\n```\n\nUsing large value for the learning rate, the adaptive learning rate methods are the winner here.\n\nHowever, the opposite happens when we're using small learning rate value e.g. `1e-5`. It's small enough for vanilla SGD and momentum based methods to perform well. On the other hand, as the learning rate is already very small, and we normalizes it in the adaptive learning rate methods, it becomes even smaller, which impacting the convergence rate. It makes the learning becomes really slow and they perform worse than the vanilla SGD with the same number of iteration.\n\n## Conclusion\n\nIn this post we looked at the optimization algorithms for neural nets beyond SGD. We looked at two classes of algorithms: momentum based and adaptive learning rate methods.\n\nWe also implement all of those methods in Python and Numpy with the use case of our neural nets stated in the last post.\n\nMost of those methods above are currently implemented in the popular Deep Learning libraries like Tensorflow, Keras, and Caffe. However, Adam is currently the default recommended algorithm to be used.\n\nYou could find the full code used in this post here: https://gist.github.com/wiseodd/85ad008aef5585cec017f4f1e6d67a02.\n\n## References\n\n- http://cs231n.github.io/neural-networks-3/\n- http://sebastianruder.com/optimizing-gradient-descent/\n- https://arxiv.org/pdf/1412.6980.pdf","src/content/post/nn-optimization.mdx","6a2465f80314841e","nn-optimization.mdx","nn-sgd",{"id":502,"data":504,"body":509,"filePath":510,"digest":511,"legacyId":512,"deferredRender":23},{"title":505,"description":506,"publishDate":507,"draft":15,"tags":508},"Implementing Minibatch Gradient Descent for Neural Networks","Let's use Python and Numpy to implement Minibatch Gradient Descent algorithm for a simple 3-layers Neural Networks.",["Date","2016-06-21T06:54:00.000Z"],[17,42,43,44,497],"import BlogImage from \"@/components/BlogImage.astro\";\n\nIt's a public knowledge that Python is the de facto language of Machine Learning. It's not without reason: Python has a very healthy and active libraries that are very useful for numerical computing. Libraries like Numpy, SciPy, Pandas, Matplotlib, etc are very important building blocks for implementing Machine Learning algorithms.\n\nSo, let's back to basic and learn how to create a simple 3-layers Neural Networks, and implement Minibatch Gradient Descent using Numpy.\n\n## Defining the Networks\n\nFirst, the dataset. We're going to create toy dataset by using Scikit-Learn `make_moons` function:\n\n```python\nfrom sklearn.datasets import make_moons\nfrom sklearn.cross_validation import train_test_split\n\nX, y = make_moons(n_samples=5000, random_state=42, noise=0.1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n```\n\nTo get clearer picture about the dataset, here's the visualization:\n\n\u003CBlogImage imagePath='/img/nn-sgd/dataset.png' />\n\nNext, we define our NN model. It will be a three layers networks (single hidden layer):\n\n```python\nimport numpy as np\n\nn_feature = 2\nn_class = 2\nn_iter = 10\n\ndef make_network(n_hidden=100):\n    # Initialize weights with Standard Normal random variables\n    model = dict(\n        W1=np.random.randn(n_feature, n_hidden),\n        W2=np.random.randn(n_hidden, n_class)\n    )\n\n    return model\n```\n\nWe will also define two operations: forward propagation and the backpropagation. Let's digest them one by one. We're going to define the forward propagation first.\n\n```python\ndef softmax(x):\n    return np.exp(x) / np.exp(x).sum()\n\ndef forward(x, model): # Input to hidden\n    h = x @ model['W1'] # ReLU non-linearity\n    h[h \u003C 0] = 0\n\n    # Hidden to output\n    prob = softmax(h @ model['W2'])\n\n    return h, prob\n```\n\nThe basic scheme is to do series of dot product from input to hidden layer to output layer. In the hidden layer, we also apply some kind of non-linearity so that our NN could predict non-linear decision boundary. The hottest non-linearity function right now is ReLU, and we're going to follow that trend.\n\nNow, ReLU is defined by `f(x) = max(0, x)`. But, rather than doing `np.max(0, x)`, there's a neat implementation trick: `x[x \u003C 0] = 0`. It's subjectively more readable.\n\nOnce we reach the output layer, we need to make the output to be a probability distribution, Bernoulli to be exact, so we squash the output with Softmax function to get our label distribution.\n\n```python\ndef backward(model, xs, hs, errs):\n    \"\"\"xs, hs, errs contain all informations (input, hidden state, error) of all data in the minibatch\"\"\"\n    # errs is the gradients of output layer for the minibatch\n    dW2 = hs.T @ errs\n\n    # Get gradient of hidden layer\n    dh = errs @ model['W2'].T\n    dh[hs \u003C= 0] = 0\n\n    dW1 = xs.T @ dh\n\n    return dict(W1=dW1, W2=dW2)\n```\n\nFor the backprop, we start from the top (output layer) down to the input layer. We start by propagating the error to get the gradient of weight between hidden layer and output layer, then we compute the gradient of hidden layer using the error gradient. Using hidden layer gradient, we then compute the gradient of weight between input and hidden layer.\n\n## Gradient Descent\n\nWe could use the model already, but it's meaningless as it will yield random result (because of random initialization). So, what we need to do is to train the networks with our dataset.\n\nThe most popular optimization algorithm in Machine Learning is Gradient Descent. It's a first order method, meaning that we only need to compute the gradient of the objective function to use it. Compared to the second order method like Netwon's Method or BFGS, where we have to derive the partial second order derivative matrix (Hessian), Gradient Descent is simpler, albeit we have more to fiddle about.\n\nNow, there are three variants of Gradient Descent: Batch, Stochastic, and Minibatch:\n\n- **Batch** will use full training data at each iteration, with could be very expensive if our dataset is large and our networks have a lot of parameters.\n- **Stochastic** uses only single data point to propagate the error, which would make the convergence slow, as the variance is big (because Law of Large Numbers doesn't apply).\n- **Minibatch** is combining the best of both worlds. We don't use full dataset but we also don't use single data point either. For example, we're using 50, 100, or 200 random subset of our dataset each time we train the networks. This way, we lower the computation cost, and yet we're still get lower variance than by using the Stochastic version.\n\n## Implementing the Minibatch Gradient Descent\n\nLet's apply our algorithm of choice Minibatch Gradient Descent:\n\n```python\ndef sgd(model, X_train, y_train, minibatch_size):\n    for iter in range(n_iter):\n        print('Iteration {}'.format(iter))\n\n        # Randomize data point\n        X_train, y_train = shuffle(X_train, y_train)\n\n        for i in range(0, X_train.shape[0], minibatch_size):\n            # Get pair of (X, y) of the current minibatch/chunk\n            X_train_mini = X_train[i:i + minibatch_size]\n            y_train_mini = y_train[i:i + minibatch_size]\n\n            model = sgd_step(model, X_train_mini, y_train_mini)\n\n    return model\n```\n\nThis is the skeleton of our training algorithm. Notice how we first shuffle the training data first then get chunks from it. That way we will get the random subset with size of `minibatch_size` of our full training data. Each of those random subset will be fed to the networks, and then the gradients of that minibatch will be propagate back to update the parameters/weights of the networks.\n\nThe `sgd_step` function is like this:\n\n```python\ndef sgd_step(model, X_train, y_train):\n    grad = get_minibatch_grad(model, X_train, y_train)\n    model = model.copy()\n\n    # Update every parameters in our networks (W1 and W2) using their gradients\n    for layer in grad:\n        # Learning rate: 1e-4\n        model[layer] += 1e-4 * grad[layer]\n\n    return model\n```\n\nFirst, we get the gradients of each layer of the networks for the current minibatch. Then we use that gradients to update the weights of each layers of the networks. The update operation is very easy. We just add the gradient of particular weight matrix to our existing weight matrix.\n\nTo make the learning better, we scale the gradients with a _learning rate_ hyperparameter. If we think visually, the gradient of a function is the direction of our step, whereas the learning rate is how far we take the step.\n\nLastly, we look at our `get_minibatch_grad` function:\n\n```python\ndef get_minibatch_grad(model, X_train, y_train):\n    xs, hs, errs = [], [], []\n\n    for x, cls_idx in zip(X_train, y_train):\n        h, y_pred = forward(x, model)\n\n        # Create probability distribution of true label\n        y_true = np.zeros(n_class)\n        y_true[int(cls_idx)] = 1.\n\n        # Compute the gradient of output layer\n        err = y_true - y_pred\n\n        # Accumulate the informations of minibatch\n        # x: input\n        # h: hidden state\n        # err: gradient of output layer\n        xs.append(x)\n        hs.append(h)\n        errs.append(err)\n\n    # Backprop using the informations we get from the current minibatch\n    return backward(model, np.array(xs), np.array(hs), np.array(errs))\n```\n\nWhat we do here is to iterate every data point in our minibatch, then feed it to the network and compare the output with the true label from the training data. The error is neatly defined by the difference of the probability of true label with the probability of our prediction. This is so, because we're implicitly using the Softmax output with Cross Entropy cost function. See here for full derivation: http://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function.\n\nBecause this is a Minibatch Gradient Descent algorithm, we then accumulate all the informations of the current minibatch. We use all those informations of the current minibatch to do the backprop, which will yield gradients of the networks' parameters (`W1` and `W2`).\n\n## Testing\n\nLet's test our model and algorithm. We're going to repeat the train-test procedure for 100 times then report the average of the accuracy.\n\n```python\nminibatch_size = 50\nn_experiment = 100\n\n# Create placeholder to accumulate prediction accuracy\naccs = np.zeros(n_experiment)\n\nfor k in range(n_experiment): # Reset model\n    model = make_network()\n\n    # Train the model\n    model = sgd(model, X_train, y_train, minibatch_size)\n\n    y_pred = np.zeros_like(y_test)\n\n    for i, x in enumerate(X_test):\n        # Predict the distribution of label\n        _, prob = forward(x, model)\n        # Get label by picking the most probable one\n        y = np.argmax(prob)\n        y_pred[i] = y\n\n    # Compare the predictions with the true labels and take the percentage\n    accs[k] = (y_pred == y_test).sum() / y_test.size\n\nprint('Mean accuracy: {}, std: {}'.format(accs.mean(), accs.std()))\n```\n\nWith 10 iterations and minibatch size of 50, here's what we get:\n\n```\nMean accuracy: 0.8765040000000001, std: 0.0048433442991387705\n```\n\nSo, we get about 87% accuracy on average. Not bad at all!\n\n## Conclusion and Future Work\n\nHere, we showed how to define a simple Neural Networks model from scratch using Numpy. We also defined the forward and backpropagation operations of the networks. We then wrote an algorithm to train the network: Minibatch Gradient Descent. Finally we tested the trained networks using test dataset.\n\nThis model obviously a very simple Neural Networks model. If you noticed, it doesn't even have bias! Nevertheless it served our purpose: implementing the Minibatch Gradient Descent algorithm.\n\nThink of this post and codes as the foundation of the next posts about the more sophisticated and better optimization algorithms.","src/content/post/nn-sgd.mdx","40062dbb526da374","nn-sgd.mdx","parallel-monte-carlo",{"id":513,"data":515,"body":520,"filePath":521,"digest":522,"legacyId":523,"deferredRender":23},{"title":516,"description":517,"publishDate":518,"draft":15,"tags":519},"Paralellizing Monte Carlo Simulation in Python","Monte Carlo simulation is all about quantity. It can take a long time to complete. Here's how to speed it up with the amazing Python multiprocessing module!",["Date","2016-06-13T06:42:00.000Z"],[17,42,43],"Let's talk about Monte Carlo.\n\nIt's a method to infer an unknown distribution using stochastic simulation. If that unknown distribution is in a nice form, e.g. Gaussian, Beta, etc, by all means, we could just infer it analytically. However things get more complicated and quickly become untractable when we're dealing with some unknown, complicated distribution. And this is where Monte Carlo method shines.\n\nBy exploiting the [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers), if we sample a lot of data point from that unknown distribution, eventually, we could approximate that distribution. That's the key of Monte Carlo method: quantity of sample, the more the better.\n\nSo, really Monte Carlo method is a general framework. What we do is just plug our unknown distribution, and by drawing a HUGE amount of samples from it, we could infer the true distribution.\n\nBetter yet, because the Monte Carlo draws an i.i.d random variables from the target distribution, it has a very nice property. You bet, Monte Carlo is embarassingly parallel. So, it's a very low hanging fruit. And with Python in our arsenal, we could parallelize Monte Carlo in just several lines!\n\n## Naive Implementation of Monte Carlo Simulation\n\nSuppose we have this `sample()` method that sample from an unknown target distribution that we want to infer. For example purpose, we will use a `x ~ Categorical(p)`. But it can be anything.\n\nWhat we want is to infer the distribution of `p`. For this example we will use `p = (0.3, 0.2, 0.1, 0.1, 0.3)`. Remember, we don't know the true `p`! So pretend that our program don't know that!\n\n```python\nimport numpy as np\nimport time\n\np = (0.3, 0.2, 0.1, 0.1, 0.3)\n\ndef sample():\n    # Just to make it slow! # Pretend that this is a complicated distribution!\n    time.sleep(0.1)\n\n    # Return item index\n    return np.argmax(np.random.multinomial(1, p))\n```\n\nSo, we're wondering what's our distribution looks like. We could draw a huge amount of samples and normalize them to get the distribution.\n\n```python\nfrom collections import Counter\n\ndef monte_carlo(iter=1000):\n    samples = []\n\n    for _ in xrange(iter):\n        x = sample()\n        samples.append(x)\n\n    # Count each item in our samples\n    p = Counter(samples)\n    # Convert to dict so we could use update() method properly\n    p = dict(p)\n    # Normalize them to get the distribution\n    p.update([(item, prob / float(iter)) for item, prob in p.items()])\n\n    return p\n```\n\nLet's try running our simulation!\n\n```bash\nIn [0]: %time monte_carlo(iter=1000)\n\nCPU times: user 91 ms, sys: 28.5 ms, total: 120 ms\nWall time: 1min 42s\n\nOut[0]: {0: 0.29, 1: 0.198, 2: 0.103, 3: 0.101, 4: 0.308}\n\n```\n\nTry to compare the result of our Monte Carlo simulation with the true distribution of `p`! It's very close to our true `p`!\n\nWe finished our simulation in 100s. Now imagine if our unknown target distribution is not trivial and takes a lot of computational time to get sample from. The execution time of our Monte Carlo simulation will quickly get out of hand!\n\n## Parallel Monte Carlo Simulation\n\nLet's try to speed that up by parallelizing it. But first we need to modify our `sample()` method so that it won't use the same random seed across all of the processes, as we will get the same \"random\" results, which would be pointless. Calling `np.random.seed()` would do the trick.\n\n```python\nimport multiprocessing as mp\n\ndef sample():\n    # Just to make it slow! # Pretend that this is a complicated distribution!\n    time.sleep(0.1)\n\n    # Re-seed the random number generator\n    np.random.seed()\n\n    # Return item index\n    return np.argmax(np.random.multinomial(1, p))\n\ndef _parallel_mc(iter=1000):\n    pool = mp.Pool(4)\n\n    future_res = [pool.apply_async(sample) for _ in range(iter)]\n    res = [f.get() for f in future_res]\n\n    return res\n```\n\nHere, we're using Python's `multiprocessing.Pool` module, which we define that we use 4 workers for our Monte Carlo simulation. `Pool` class has some very useful methods like `map` and `apply_async`. Because our simulation doesn't take any argument, we will use `apply_asnyc`.\n\nLet's use that method for our new parallel Monte Carlo!\n\n```python\ndef parallel_monte_carlo(iter=1000):\n    samples = _parallel_mc(iter)\n\n    # Count each item in our samples\n    p = Counter(samples)\n    # Convert to dict so we could use update() method properly\n    p = dict(p)\n    # Normalize them to get the distribution\n    p.update([(item, prob / float(iter)) for item, prob in p.items()])\n\n    return p\n```\n\nWe just swapped our `for` loop in `monte_carlo()` method with the new `_parallel_mc()` method. The rest is the same as before, we were only changing one line of code.\n\nHere's the result:\n\n```bash\nIn [1]: %time parallel_monte_carlo(iter=1000)\n\nCPU times: user 289 ms, sys: 258 ms, total: 547 ms\nWall time: 28.3 s\n\nOut[1]: {0: 0.31, 1: 0.185, 2: 0.099, 3: 0.105, 4: 0.301}\n\n```\n\n100s down to 28s? ~4x of speedup! It's scaled linearly with the number of processes we stated in `Pool` because practically the `sample()` method run in uniform time: ~0.1s. In the real world problem though, it will wildly vary. But linear growth of speedup compared to the number of processes would be the baseline.\n\nAlso, be careful with the number of processes. As a rule of thumb, number of processes should be equal to the number of CPU cores available, so that excessive context switching could be avoided.\n\nLastly, this parallelization scheme works best if each individual simulation takes considerably long time. This won't be effective if each simulation is really quick and we do huge amount of simulations as the overhead will be greater than the benefit of parallelization.\n\n## Conclusion\n\nIn this post, we look at Monte Carlo method and how to speed it up using Python's `multiprocessing` module. We show that parallelizing Monte Carlo in Python is very easy, and it should be the default way to do Monte Carlo simulation.","src/content/post/parallel-monte-carlo.mdx","21ef334c5f3957b2","parallel-monte-carlo.mdx","plotting",{"id":524,"data":526,"body":532,"filePath":533,"digest":534,"legacyId":535,"deferredRender":23},{"title":527,"description":528,"publishDate":529,"draft":15,"tags":530},"The Last Mile of Creating Publication-Ready Plots","In machine learning papers, plots are often treated as afterthought---authors often simply use the default Matplotlib style, resulting in an out-of-place look when the paper is viewed as a whole. In this post, I'm sharing how I make my publication-ready plots using TikZ.",["Date","2022-05-01T04:00:00.000Z"],[531],"tikz","import BlogImage from \"@/components/BlogImage.astro\";\n\nLet's start with a side-by-side comparison. Which one of the following two plots is more aesthetically pleasing?\nLeft or right?\n(Taken from one of my papers [1]. The code for generating it is in [2])\n\n\u003CBlogImage imagePath='/img/plotting/mpl_vs_tikz.png' fullWidth />\n\nHopefully, you agree with me that the answer is the one on the right.\nIn that case, we can start our journey in transforming the l.h.s. figure to the r.h.s. one.\n\n## Elements of publication-ready plots\n\nOver the year of writing papers, I've come to realize some patterns in creating publication-ready plots.\nNote, I'm not even talking about the _content_ of the plot itself---this is more about how to make your plots _fit_ your paper _well_.\nThis is essentially the \"last mile\" of making publication-ready plots, which, sadly, is something that many people ignore.\n\nAnyway, those elements are:\n\n- Must be a vector graphic (pdf, svg, etc.).\n- Should fill the entire `\\linewidth` (or `\\textwidth`) of the page.\n- Must not be stretched disproportionally.\n- The font face must be the same as the text's font face.\n- The font size can be smaller than the text's font size, but must still be legible and consistent.\n\nOne way to tell that one's plot is not publication-ready is if one uses Matplotlib without further touching its [rcParams](https://matplotlib.org/stable/tutorials/introductory/customizing.html), and simply \"pastes\" it to the paper's `.tex` file with `\\includegraphics`.\n\nBelow, I show how to ensure the elements above by leveraging the powerful TikZ.\nNote that one can also do this by modifying the `rcParams` of Matplotlib, but I only do this in a pinch---I will talk about this in a future post.\n\n## TikZ-ing your Matplotlib plots: A basic workflow\n\nTikZ is great because it's tightly coupled to LaTeX, which we already use for writing the paper.\nSo, TikZ plots will respect the styling of the paper by default, making them aesthetically pleasing out of the box.\nHowever, TikZ is notoriously difficult to learn.\n\nBut, what if I told you that you _don't_ need to understand TikZ to use it for making publication-ready plots?\nThe [Tikzplotlib](https://github.com/texworld/tikzplotlib) library will do all the hard work for you, and all you need is to customize the styling _once_.\nThen the resulting plot can be reused over and over again e.g. slides and posters without modification.\n\nSo, here's my workflow for creating a publication-ready plot, from start to finish.\n\n1. Create a Matplotlib plot as usual.\n2. Instead of `plt.savefig(figname)`, do:\n\n   ```python\n   import tikzplotlib as tpl\n\n   # Create a matplotlib plot\n\n   ...\n\n   # Save as TikZ plot\n\n   tpl.save('figname.tex', axis_width=r'\\figwidth', axis_height=r'\\figheight')\n   ```\n\n   Here's an example file you can use to follow this tutorial along:\n\n   \u003Ca href='/files/figname.tex'>download\u003C/a>.\n\n3. Copy `figname.tex` to the `figs` directory in your paper's LaTeX project.\n4. In the preamble of your paper's LaTeX file, add:\n\n   ```tex\n   \\usepackage{pgfplots}\n   \\pgfplotsset{compat=newest}\n   \\pgfplotsset{scaled y ticks=false}\n   \\usepgfplotslibrary{groupplots}\n   \\usepgfplotslibrary{dateplot}\n\n   \\usepackage{tikz}\n   ```\n\n5. In your `.tex` file, do the following to add the figure:\n\n   ```tex\n   \\begin{figure}\n       \\def\\figwidth{\\linewidth}\n       \\def\\figheight{0.15\\textheight} % Feel free to change\n\n       \\input{figs/figname}\n   \\end{figure}\n   ```\n\n   Note that `\\figwidth` and `\\figheight` are local variables, so their values will only be used for `figname`.\n\n6. At this point, you will already have a quite aesthetically pleasing figure, cf. below. Notice that the font face and size are consistent with the paper's text. However, notice that we need to improve the plot further, e.g. by unhiding the x- and y-tick labels.\n   \u003CBlogImage imagePath='/img/plotting/tikzplotlib_raw.png' />\n7. Open `figname.tex`. You will see the following code:\n\n   ```tex\n   \\begin{axis}[\n       width=\\figwidth,\n       height=\\figheight,\n       axis line style={lightgray204},\n       tick align=outside,\n       unbounded coords=jump,\n       x grid style={lightgray204},\n       xlabel=\\textcolor{darkslategray38}{Dataset},\n       xmajorticks=false,\n       xmin=-0.5, xmax=3.5,\n       xtick style={color=darkslategray38},\n       xtick={0,1,2,3},\n       xticklabels={MNIST,SVHN,CIFAR-10,CIFAR-100},\n       y grid style={lightgray204},\n       ylabel=\\textcolor{darkslategray38}{Mean Confidence},\n       ymajorgrids,\n       ymajorticks=false,\n       ymin=0, ymax=102.714733242989,\n       ytick style={color=darkslategray38}\n   ]\n   ```\n\n   You can think of this as the \"CSS\" of your plot.\n\n8. First, add the line `\\tikzstyle{every node}=[font=\\scriptsize]` before `\\begin{axis}`. This will scale all the font in the plot to `\\scriptsize`, which I think is more pleasing, while still legible.\n9. To unhide the x- and y-tick labels, simply change `xmajorticks` and `ymajorticks` to `true`.\n10. Moreover, notice that we don't have much space for the legend. So, we need to customize it. Change `xmax` to `4.1` and add the following option:\n\n    ```tex\n    \\begin{axis}[\n    ...\n        legend style={nodes={scale=0.75, transform shape}, at={(1,0)}, anchor=south east, draw=black},\n    ...\n    ]\n    ```\n\n    The change in `xmax` will make some room to the right of the plot, while the `legend style` option will scale down the legend and move it to the lower-right portion of the plot.\n\nHere's the final result:\n\n\u003CBlogImage imagePath='/img/plotting/tikzplotlib_almostfinal.png' />\n\nLooks much more pleasing than the standard Matplotlib output, isn't it?\nNote that we didn't change many things other than refining the styling options of the TikZ axis---we didn't even touch the plot content itself!\n\nIf you noticed, at this point, we already pretty much fulfilled of the elements of the publication-ready plots we discussed previously.\nI personally think that this level of aesthetic is more than acceptable for publication.\n\nBut, to me, the plot can still be refined further.\n\n- First, notice that the plot still doesn't fill the whole text/column's width.\n  This can be fixed by increasing `\\figwidth` to e.g. `1.04\\linewidth`.\n- Second, the y-axis is too tall: it exceeds the maximum value of the data (100).\n  To fix this, simply set `ymax=100` in the axis option in `figname.tex`.\n- Furthermore, the ticks on the axes (not to be confused with the tick labels) are unecessary. We can hide them setting `xtick style={draw=none}` and `ytick style={draw=none}`.\n- Last, the legend looks ugly to me: for some reason by default TikZ uses two bars in the legend.\n  The fix is to add the following before `\\begin{axis}` or in the preamble of `main.tex` to make it global:\n\n```tex\n\\pgfplotsset{compat=1.11,\n /pgfplots/ybar legend/.style={\n /pgfplots/legend image code/.code={\n \\draw[##1,/tikz/.cd,yshift=-0.25em]\n (0cm,0cm) rectangle (3pt,0.8em);},\n },\n}\n```\n\nPutting everything together, here's the final result:\n\n\u003CBlogImage imagePath='/img/plotting/tikzplotlib_final.png' />\n\nLooks great to me!\nAs a bonus, this plot (i.e. the `figname.tex`) is highly portable.\nFor example, when you want to reuse this plot in a Beamer presentation or poster, you can simply copy-and-paste `figname.tex` and include it in your presentation's `.tex` file as above---you only need to change the values of `figwidth` and `figwidth`.\nAll the refinement done previously will carry over and the plot's style will automatically adapt to the style (e.g. font face and size) of your presentation!\n\n## Miscellaneous Tips\n\nSuppose you have two plots that you want to show side-by-side in a figure:\n\n```tex\n\\begin{figure}[t]\n    \\def\\figwidth{0.5\\linewidth}\n    \\def\\figheight{0.15\\textheight}\n\n    \\centering\n\n    \\subfloat{\\input{figs/fig1a}}\n    \\hfill\n    \\subfloat{\\input{figs/fig1b}}\n\\end{figure}\n```\n\nHow do you make sure that they are perfectly aligned?\nEasy: simply add `baseline` option at the `\\begin{tikzpicture}` line of both `fig1a.tex` and `fig1b.tex`, i.e.,\n\n```tex\n\\begin{tikzpicture}[baseline]\n\n...\n\n\\begin{axis}[\n...\n```\n\nThere are also `trim axis left` and `trim axis right` options for `tikzpicture`.\nAs the names suggest, they can be used to tell LaTeX to ignore the left and right axes of the plot when computing the plot's width.\nThey might be useful in some niche situations.\n\n## Faster compilation\n\nIf your paper has many complex TikZ plots, it can happen that your LaTeX compilation is slow.\nTo mitigate this, we can \"cache\" the compiled TikZ plots.\nTo do so, we can use the `external` package: In your LaTeX preamble, add the following.\n\n```tex\n\\usepackage{tikz}\n\\usetikzlibrary{external}\n\\tikzexternalize[prefix=tikz/, figure name=output-figure]\n```\n\nThen, create a directory called `tikz/` in your main project directory.\nThis will be the directory `external` will cache your compiled TikZ plots.\nNote that this is \"trick\" is fully [compatible with Overleaf](https://www.overleaf.com/learn/latex/Questions/I_have_a_lot_of_tikz%2C_matlab2tikz_or_pgfplots_figures%2C_so_I%27m_getting_a_compilation_timeout._Can_I_externalise_my_figures%3F).\n\nIn case you want to disable `externalize` for one of your plot, e.g. for debugging, you can \"surround\" your TikZ plot with `\\tikzexternaldisable` and `\\tikzexternalenable`.\n\n```tex\n\\begin{figure}[t]\n    \\def\\figwidth{\\linewidth}\n    \\def\\figheight{0.15\\textheight}\n\n    \\centering\n\n    \\tikzexternaldisable\n    \\input{figs/figname}\n    \\tikzexternalenable\n\\end{figure}\n```\n\n## Final remark\n\nLast but not least, my final tips is: utilize Google search and Stackoverflow if you need more advanced styling.\nYou will more often than not find your questions already answered there.\n\n## References\n\n1. Kristiadi, Agustinus, Matthias Hein, and Philipp Hennig. \"Being a Bit Frequentist Improves Bayesian Neural Networks\" AISTATS 2022.\n2. [https://github.com/wiseodd/bayesian_ood_training/blob/master/notebooks/plot_uniform.ipynb](https://github.com/wiseodd/bayesian_ood_training/blob/master/notebooks/plot_uniform.ipynb).","src/content/post/plotting.mdx","9e9f9bd280e1d419","plotting.mdx","optimization-riemannian-manifolds",{"id":536,"data":538,"body":543,"filePath":544,"digest":545,"legacyId":546,"deferredRender":23},{"title":539,"description":540,"publishDate":541,"draft":15,"tags":542},"Optimization and Gradient Descent on Riemannian Manifolds","One of the most ubiquitous applications in the field of differential geometry is the optimization problem. In this article we will discuss the familiar optimization problem on Euclidean spaces by focusing on the gradient descent method, and generalize them on Riemannian manifolds.",["Date","2019-02-22T17:00:00.000Z"],[79],"import BlogImage from \"@/components/BlogImage.astro\";\n\nDifferential geometry can be seen as a generalization of calculus on Riemannian manifolds. Objects in calculus such as gradient, Jacobian, and Hessian on $\\R^n$ are adapted on arbitrary Riemannian manifolds. This fact let us also generalize one of the most ubiquitous problem in calculus: the optimization problem. The implication of this generalization is far-reaching: We can make a more general and thus flexible assumption regarding the domain of our optimization, which might fit real-world problems better or has some desirable properties.\n\nIn this article, we will focus on the most popular optimization there is, esp. in machine learning: the gradient descent method. We will begin with a review on the optimization problem of a real-valued function on $\\R^n$, which we should have been familiar with. Next, we will adapt the gradient descent method to make it work in optimization problem of a real-valued function on an arbitrary Riemannian manifold $(\\M, g)$. Lastly, we will discuss how the natural gradient descent method can be seen from this perspective, instead of purely from the second-order optimization point-of-view.\n\n## Optimization problem and the gradient descent\n\nLet $\\R^n$ be the usual Euclidean space (i.e. a Riemannian manifold $(\\R^n, \\bar{g})$ where $\\bar{g}_{ij} = \\delta_{ij}$) and let $f: \\R^n \\to \\R$ be a real-valued function. An (unconstrained) optimization problem on this space has the form\n\n$$\n    \\min_{x \\in \\R^n} f(x) \\, .\n$$\n\nThat is we would like to find a point $\\hat{x} \\in \\R^n$ such that $f(\\hat{x})$ is the minimum of $f$.\n\nOne of the most popular numerical method for solving this problem is the gradient descent method. Its algorithm is as follows.\n\n**Algorithm 1 (Euclidean gradient descent).**\n\n1. Pick arbitrary $x_{(0)} \\in \\R^n$ and let $\\alpha \\in \\R$ with $\\alpha > 0$\n2. While the stopping criterion is not satisfied:\n   1. Compute the gradient of $f$ at $x_{(t)}$, i.e. $h_{(t)} := \\gradat{f}{x_{(t)}}$\n   2. Move in the direction of $-h_{(t)}$, i.e. $x_{(t+1)} = x_{(t)} - \\alpha h_{(t)}$\n   3. $t = t+1$\n3. Return $x_{(t)}$\n\nThe justification of the gradient descent method is because of the fact that the gradient is the direction in which $f$ is increasing fastest. Its negative therefore points to the direction of steepest descent.\n\n**Proposition 1.** _Let $f: \\R^n \\to \\R$ be a real-valued function on $\\R^n$ and $x \\in \\R^n$. Among all unit vector $v \\in \\R^n$, the gradient $\\grad f \\, \\vert_x$ of $f$ at $x$ is the direction in which the directional derivative $D_v \\, f \\, \\vert_x$ is greatest. Furthermore, $\\norm{\\gradat{f}{x}}$ equals to the value of the directional derivative in that direction._\n\n_Proof._ First, note that, by our assumption, $\\norm{v} = 1$. By definition of the directional derivative and dot product on $\\R^n$,\n\n$$\n\\begin{align}\n    D_v \\, f \\, \\vert_x &= \\grad f \\, \\vert_x \\cdot v \\\\\n                            &= \\norm{\\gradat{f}{x}} \\norm{v} \\cos \\theta \\\\\n                            &= \\norm{\\gradat{f}{x}} \\cos \\theta \\, ,\n\\end{align}\n$$\n\nwhere $\\theta$ is the angle between $\\gradat{f}{x}$ and $v$. As $\\norm{\\cdot} \\geq 0$ and $-1 \\leq \\cos \\theta \\leq 1$, the above expression is maximized whenever $\\cos \\theta = 1$. This implies that the particular vector $\\hat{v}$ that maximizes the directional derivative points in the same direction as $\\gradat{f}{x}$. Furthermore, plugging in $\\hat{v}$ into the above equation, we get\n\n$$\n    D_{\\hat{v}} \\, f \\, \\vert_x = \\norm{\\gradat{f}{x}} \\, .\n$$\n\nThus, the length of $\\gradat{f}{x}$ is equal to the value of $D_{\\hat{v}} \\, f \\, \\vert_x$.\n\n$$\n\\qed\n$$\n\n## Gradient descent on Riemannian manifolds\n\n**Remark.** _We shall use the Einstein summation convention: Repeated indices above and below are implied to be summed, e.g. $v^i w_i \\implies \\sum_i v^i w_i$ and $g_{ij} v^i v^j \\implies \\sum*{ij} g*{ij} v^i v^j$. By convention the index in $\\partder{}{x^i}$ is thought to be a lower index.\\_\n\nWe now want to break the confine of the Euclidean space. We would like to generalize the gradient descent algorithm on a function defined on a Riemannian manifold. Based on Algorithm 1, at least there are two parts of the algorithm that we need to adapt, namely, (i) the gradient of $f$ and (ii) the way we move between points on $\\M$.\n\nSuppose $(\\M, g)$ is a $n$-dimensional Riemannian manifold. Let $f: \\M \\to R$ be a real-valued function (scalar field) defined on $\\M$. Then, the optimization problem on $\\M$ simply has the form\n\n$$\n    \\min_{p \\in \\M} f(p) \\, .\n$$\n\nAlthough it seems innocent enough (we only replace $\\R^n$ with $\\M$ from the Euclidean version), some difficulties exist.\n\nFirst, we shall discuss about the gradient of $f$ on $\\M$. By definition, $\\grad{f}$ is a vector field on $\\M$, i.e. $\\grad{f} \\in \\mathfrak{X}(\\M)$ and at each $p \\in \\M$, $\\gradat{f}{p}$ is a tangent vector in $T_p \\M$. Let the differential $df$ of $f$ be a one one-form, which, in given coordinates $\\vx_p := (x^1(p), \\dots, x^n(p))$, has the form\n\n$$\n    df = \\partder{f}{x^i} dx^i \\, .\n$$\n\nThen, the gradient of $f$ is obtained by raising an index of $df$. That is,\n\n$$\n    \\grad{f} = (df)^\\sharp \\, ,\n$$\n\nand in coordinates, it has the expression\n\n$$\n    \\grad{f} = g^{ij} \\partder{f}{x^i} \\partder{}{x^j} \\, .\n$$\n\nAt any $p \\in \\M$, given $v \\in T_x \\M$, it is characterized by the following equation:\n\n$$\n    \\inner{\\gradat{f}{p}, v}_g = df(v) = vf \\, .\n$$\n\nThat is, pointwise, the inner product of the gradient and any tangent vector is the action of derivation $v$ on $f$. We can think of this action as taking directional derivative of $f$ in the direction $v$. Thus, we have the analogue of Proposition 1 on Riemannian manifolds.\n\n**Proposition 2.** _Let $(\\M, g)$ be a Riemannian manifold and $f: \\M \\to \\R$ be a real-valued function on $\\M$ and $p \\in \\M$. Among all unit vector $v \\in T_p \\M$, the gradient $\\gradat{f}{p}$ of $f$ at $p$ is the direction in which the directional derivative $vf$ is greatest. Furthermore, $\\norm{\\gradat{f}{p}}$ equals to the value of the directional derivative in that direction._\n\n_Proof._ We simply note that by definition of inner product induced by $g$, we have\n\n$$\n    \\inner{u, w}_g = \\norm{u}_g \\norm{w}_g \\cos \\theta \\qquad \\forall \\, u, w \\in T_p \\M \\, ,\n$$\n\nwhere $\\theta$ is again the angle between $u$ and $w$. Using the characteristic of $\\gradat{f}{p}$ we have discussed above and by substituting $vf$ for $D_v \\, f \\, \\vert_p$ in the proof of Proposition 1, we immediately get the desired result.\n\n$$\n\\qed\n$$\n\nProposition 2 therefore provides us with a justification of just simply substituting the Euclidean gradient with the Riemannian gradient in Algorithm 1.\n\nTo make this concrete, we do the computation in coordinates. In coordinates, we can represent $df$ by a row vector $d$ (i.e. a sequence of numbers in the sense of linear algebra) containing all partial derivatives of $f$:\n\n$$\n    d := \\left( \\partder{f}{x^1}, \\dots, \\partder{f}{x^n} \\right) \\, .\n$$\n\nGiven the matrix representation $G$ of the metric tensor $g$ in coordinates, the gradient of $f$ is represented by a column vector $h$, such that\n\n$$\n    h = G^{-1} d^\\T \\, .\n$$\n\n**Example 1. (Euclidean gradient in coordinates).** Notice that in the Euclidean case, $\\bar{g}_{ij} = \\delta_{ij}$, thus it is represented by an identity matrix $I$, in coordinates. Therefore the Euclidean gradient is simply\n\n$$\n    h = I^{-1} d^\\T = d^\\T \\, .\n$$\n\nThe second modification to Algorithm 1 that we need to find the analogue of is the way we move between points on $\\M$. Notice that, at each $x \\in \\R^n$, the way we move between points in the Euclidean gradient descent is by following a straight line in the direction $\\gradat{f}{x}$. We know by triangle inequality that straight line is the path with shortest distance between points in $\\R^n$.\n\nOn Riemannian manifolds, we move between points by the means of curves. There exist a special kind of curve $\\gamma: I \\to \\M$, where $I$ is an interval, that are \"straight\" between two points on $\\M$, in the sense that the covariant derivative $D_t \\gamma'$ of the velocity vector along the curve itself, at any time $t$ is $0$. The intuition is as follows: Although if we look at $\\gamma$ on $\\M$ from the outsider's point-of-view, it is not straight (i.e. it follows the curvature of $\\M$), as far as the inhabitants of $\\M$ are concerned, $\\gamma$ is straight, as its velocity vector (its direction and length) is the same everywhere along $\\gamma$. Thus, geodesics are the generalization of straight lines on Riemannian manifolds.\n\nFor any $p \\in \\M$ and $v \\in T_p \\M$, we can show that there always exists a geodesic starting at $p$ with initial velocity $v$, denoted by $\\gamma_v$. Furthermore, if $c, t \\in \\R$ we can rescale any geodesic $\\gamma_v$ by\n\n$$\n    \\gamma_{cv}(t) = \\gamma_v (ct) \\, ,\n$$\n\nand thus we can define a map $\\exp_p(v): T_p \\M \\to \\M$ by\n\n$$\n    \\exp_p(v) = \\gamma_v(1) \\, ,\n$$\n\ncalled the exponential map. The exponential map is the generalization of \"moving straight in the direction $v$\" on Riemannian manifolds.\n\n**Example 2. (Exponential map on a sphere).** Let $\\mathbb{S}^n(r)$ be a sphere embedded in $\\R^{n+1}$ with radius $r$. The shortest path between any pair of points on the sphere can be found by following the [great circle](https://en.wikipedia.org/wiki/Great_circle) connecting them.\n\nLet $p \\in \\mathbb{S}^n(r)$ and $0 \\neq v \\in T_p \\mathbb{S}^n(r)$ be arbitrary. The curve $\\gamma_v: \\R \\to \\R^{n+1}$ given by\n\n$$\n    \\gamma_v(t) = \\cos \\left( \\frac{t\\norm{v}}{r} \\right) p + \\sin \\left( \\frac{t\\norm{v}}{r} \\right) r \\frac{v}{\\norm{v}} \\, ,\n$$\n\nis a geodesic, as its image is the great circle formed by the intersection of $\\mathbb{S}^n(r)$ with the linear subspace of $\\R^{n+1}$ spanned by $\\left\\{ p, r \\frac{v}{\\norm{v}} \\right\\}$. Therefore the exponential map on $\\mathbb{S}^n(r)$ is given by\n\n$$\n    \\exp_p(v) = \\cos \\left( \\frac{\\norm{v}}{r} \\right) p + \\sin \\left( \\frac{\\norm{v}}{r} \\right) r \\frac{v}{\\norm{v}} \\, .\n$$\n\nGiven the exponential map, our modification to Algorithm 1 is complete, which we show in Algorithm 2. The new modifications from Algorithm 1 are in \u003Cspan style=\"color:blue\">blue\u003C/span>.\n\n**Algorithm 2 (Riemannian gradient descent).**\n\n1. Pick arbitrary \u003Cspan style=\"color:blue\">$p_{(0)} \\in \\M$\u003C/span>. Let $\\alpha \\in \\R$ with $\\alpha > 0$\n2. While the stopping criterion is not satisfied:\n   1. Compute the gradient of $f$ at $p_{(t)}$, i.e. \u003Cspan style=\"color:blue\">$h_{(t)} := \\gradat{f}{p_{(t)}} = (df \\, \\vert_{p_{(t)}})^\\sharp$\u003C/span>\n   2. Move in the direction $-h_{(t)}$, i.e. \u003Cspan style=\"color:blue\">$p_{(t+1)} = \\exp_{p_{(t)}}(-\\alpha h_{(t)})$\u003C/span>\n   3. $t = t+1$\n3. Return $p_{(t)}$\n\n## Approximating the exponential map\n\nIn general, the exponential map is difficult to compute, as to compute a geodesic, we have to solve a system of second-order ODE. Therefore, for a computational reason, we would like to approximate the exponential map with cheaper alternative.\n\nLet $p \\in \\M$ be arbitrary. We define a map $R_p: T\\M \\to \\M$ called the **_retraction_** map, by the following properties:\n\n1. $R_p(0) = p$\n2. $dR_p(0) = \\text{Id}_{T_p \\M}$.\n\nThe second property is called the **_local rigidity_** condition and it preserves gradients at $p$. In particular, the exponential map is a retraction. Furthermore, if $d_g$ denotes the Riemannian distance and $t \\in \\R$, retraction can be seen as a first-order approximation of the exponential map, in the sense that\n\n$$\n    d_g(\\exp_p(tv), R_p(tv)) = O(t^2) \\, .\n$$\n\nOn an arbitrary embedded submanifold $\\S \\in \\R^{n+1}$, if $p \\in \\S$ and $v \\in T_p \\S$, viewing $p$ to be a point on the ambient manifold and $v$ to be a point on the ambient tangent space $T_p \\R^{n+1}$, we can compute $R_p(v)$ by (i) moving along $v$ to get $p + v$ and then (ii) project the point $p+v$back to $\\S$.\n\n**Example 3. (Retraction on a sphere).** Let $\\mathbb{S}^n(r)$ be a sphere embedded in $\\R^{n+1}$ with radius $r$. The retraction on any $p \\in \\mathbb{S}^n(r)$ and $v \\in T_p \\mathbb{S}^n(r)$ is defined by\n\n$$\n    R_p(v) = r \\frac{p + v}{\\norm{p + v}}\n$$\n\nTherefore, the Riemannian gradient descent in Algorithm 2 can be modified to be\n\n**Algorithm 3 (Riemannian gradient descent with retraction).**\n\n1. Pick arbitrary $p_{(0)} \\in \\M$. Let $\\alpha \\in \\R$ with $\\alpha > 0$\n2. While the stopping criterion is not satisfied:\n   1. Compute the gradient of $f$ at $p_{(t)}$, i.e. $h_{(t)} := \\gradat{f}{p_{(t)}} = (df \\, \\vert_{p_{(t)}})^\\sharp$\n   2. Move in the direction $-h_{(t)}$, i.e. \u003Cspan style=\"color:blue\">$p_{(t+1)} = R_{p_{(t)}}(-\\alpha h_{(t)})$\u003C/span>\n   3. $t = t+1$\n3. Return $p_{(t)}$\n\n## Natural gradient descent\n\nOne of the most important applications of the Riemannian gradient descent in machine learning is for doing optimization of statistical manifolds. We define a statistical manifold $(\\R^n, g)$ to be the set $\\R^n$ corresponding to the set of parameter of a statistical model $p_\\theta(z)$, equipped with metric tensor $g$ which is the Fisher information metric, given by\n\n$$\n    g_{ij} = \\E_{z \\sim p_\\theta} \\left[ \\partder{\\log p_\\theta(z)}{\\theta^i} \\partder{\\log p_\\theta(z)}{\\theta^j} \\right] \\, .\n$$\n\nThe most common objective function $f$ in the optimization problem on a statistical manifold is the expected log-likelihood function of our statistical model. That is, given a dataset $\\D = \\\\{ z_i \\\\}$, the objective is given by $f(\\theta) = \\sum_{z \\in \\D} \\log p_\\theta(z)$.\n\nThe metric tensor $g$ is represented by $n \\times n$ matrix $F$, called the _Fisher information matrix_. The Riemannian gradient in this manifold is therefore can be represented by a column vector $h = F^{-1} d^\\T$. Furthermore, as the manifold is $\\R^n$, the construction of the retraction map we have discussed previously tells us that we can simply do addition $p + v$ for any $p \\in \\R^n$ and $v \\in T_p \\R^n$. This is well defined as there is a natural isomorphism between $\\R^n$ and $T_p \\R^n$. All in all, the gradient descent in this manifold is called _natural gradient descent_ and is presented in Algorithm 4 below.\n\n**Algorithm 4 (Natural gradient descent).**\n\n1. Pick arbitrary $\\theta_{(0)} \\in \\R^n$. Let $\\alpha \\in \\R$ with $\\alpha > 0$\n2. While the stopping criterion is not satisfied:\n   1. Compute the gradient of $f$ at $\\theta_{(t)}$, i.e. $h_{(t)} := F^{-1} d^\\T$\n   2. Move in the direction $-h_{(t)}$, i.e. $\\theta_{(t+1)} = \\theta_{(t)} - \\alpha h_{(t)}$\n   3. $t = t+1$\n3. Return $\\theta_{(t)}$\n\n## Conclusion\n\nOptimization in Riemannian manifold is an interesting and important application in the field of geometry. It generalizes the optimization methods from Euclidean spaces onto Riemannian manifolds. Specifically, in the gradient descent method, adapting it to a Riemannian manifold requires us to use the Riemannian gradient as the search direction and the exponential map or retraction to move between points on the manifold.\n\nOne major difficulty exists: Computing and storing the matrix representation $G$ of the metric tensor are very expensive. Suppose the manifold is $n$-dimensional. Then, the size of $G$ is in $O(n^2)$ and the complexity of inverting it is in $O(n^3)$. In machine learning, $n$ could be in the order of million, so a naive implementation is infeasible. Thankfully, many approximations of the metric tensor, especially for the Fisher information metric exist (e.g. [7]). Thus, even with these difficulties, the Riemannian gradient descent or its variants have been successfully applied on many areas, such as in inference problems [8], word or knowledge graph embeddings [9], etc.\n\n## References\n\n1. Lee, John M. \"Smooth manifolds.\" Introduction to Smooth Manifolds. Springer, New York, NY, 2013. 1-31.\n2. Lee, John M. Riemannian manifolds: an introduction to curvature. Vol. 176. Springer Science & Business Media, 2006.\n3. Fels, Mark Eric. \"An Introduction to Differential Geometry through Computation.\" (2016).\n4. Absil, P-A., Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2009.\n5. Boumal, Nicolas. Optimization and estimation on manifolds. Diss. Catholic University of Louvain, Louvain-la-Neuve, Belgium, 2014.\n6. Graphics: [https://tex.stackexchange.com/questions/261408/sphere-tangent-to-plane](https://tex.stackexchange.com/questions/261408/sphere-tangent-to-plane).\n7. Martens, James, and Roger Grosse. \"Optimizing neural networks with kronecker-factored approximate curvature.\" International conference on machine learning. 2015.\n8. Patterson, Sam, and Yee Whye Teh. \"Stochastic gradient Riemannian Langevin dynamics on the probability simplex.\" Advances in neural information processing systems. 2013.\n9. Suzuki, Atsushi, Yosuke Enokida, and Kenji Yamanishi. \"Riemannian TransE: Multi-relational Graph Embedding in Non-Euclidean Space.\" (2018).","src/content/post/optimization-riemannian-manifolds.mdx","213b801359b55b3c","optimization-riemannian-manifolds.mdx","residual-net",{"id":547,"data":549,"body":554,"filePath":555,"digest":556,"legacyId":557,"deferredRender":23},{"title":550,"description":551,"publishDate":552,"draft":15,"tags":553},"Residual Net","In this post, we will look into the record breaking convnet model of 2015: the Residual Net (ResNet).",["Date","2016-10-13T13:19:00.000Z"],[17,42,43,44],"import BlogImage from \"@/components/BlogImage.astro\";\n\nSeptember 2015, at the ImageNet Large Scale Visual Recognition Challenge's (ILSVRC) winners announcement, there was this one net by MSRA that dominated it all: Residual Net (ResNet) (He et al., 2015). The ensemble of ResNets crushing the classification task of other competitiors and almost halving the error rate of the 2014 winner.\n\nAside from winning the ILSVRC 2015 classification, ResNet also won the detection and localization challenge of the said competition. Additionally, it also won the MSCOCO detection and segmentation challenge. Quite a feat!\n\nSo, what makes ResNet so good? What's the difference compared to the previous convnet models?\n\n## ResNet: the intuition behind it\n\nThe authors of ResNet observed, no matter how deep a network is, it should not be any worse than the shallower network. That's because if we argue that neural net could approximate any complicated function, then it could also learn identity function, i.e. input = output, effectively skipping the learning progress on some layers. But, in real world, this is not the case because of the vanishing gradient and curse of dimensionality problems.\n\n\u003CBlogImage imagePath='/img/residual-net/residual_block.png' />\n\nHence, it might be useful to explicitly force the network to learn an identity mapping, by learning the residual of input and output of some layers (or subnetworks). Suppose the input of the subnetwork is $x$, and the **true** output is $H(x)$. The residual is the difference between them: $F(x) = H(x) - x$. As we are interested in finding the true, underlying output of the subnetwork, we then rearrange that equation into $H(x) = F(x) + x$.\n\nSo that's the difference between ResNet and traditional neural nets. Where traditional neural nets will learn $H(x)$ directly, ResNet instead models the layers to learn the residual of input and output of subnetworks. This will give the network an option to just skip subnetworks by making $F(x) = 0$, so that $H(x) = x$. In other words, the output of a particular subnetwork is just the output of the last subnetwork.\n\nDuring backpropagation, learning residual gives us nice property. Because of the formulation, the network could choose to ignore the gradient of some subnetworks, and just forward the gradient from higher layers to lower layers without any modification. As an extreme example, this means that ResNet could just forward gradient from the last layer, e.g. layer 151, directly to the first layer. This gives ResNet additional nice to have option which might be useful, rather than just strictly doing computation in all layers.\n\n## ResNet: implementation detail\n\nHe et al. experimented with 152 layers deep ResNet in their paper. But due to our (my) monitor budget, we will look at the 34 layers version instead. Furthermore, it's easier to understand with not so many layers, isn't it?\n\n\u003CBlogImage imagePath='/img/residual-net/resnet34.png' />\n\nAt the first layer, ResNet use 7x7 convolution with stride 2 to downsample the input by the order of 2, similar to pooling layer. Then it's followed by three identity blocks before downsampling again by 2. The downsampling layer is also a convolution layer, but without the identity connection. It continues like that for several layer deep. The last layer is average pooling which creates 1000 feature maps (for ImageNet data), and average it for each feature map. The result would be 1000 dimensional vector which then fed into Softmax layer directly, so it's fully convolutional.\n\nIn the paper, He et al. use bottleneck architecture for each the residual block. It means that the residual block consists of 3 layers in this order: 1x1 convolution - 3x3 convolution - 1x1 convolution. The first and the last convolution is the bottleneck. It mostly just for practical consideration, as the first 1x1 convolution is being used to reduce the dimensionality, and the last 1x1 convolution is to restore it. So, the same network is now become 50 layers.\n\n\u003CBlogImage imagePath='/img/residual-net/resnet50.png' fullWidth />\n\nNotice, in 50 layers and more ResNet, at each block, there are now two 1x1 convolution layers.\n\n## References\n\n1. [He, Kaiming, et al. \"Deep residual learning for image recognition.\" arXiv preprint arXiv:1512.03385 (2015).](http://arxiv.org/pdf/1512.03385)\n2. [Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" arXiv preprint arXiv:1602.07261 (2016).](http://arxiv.org/pdf/1602.07261)\n3. [Huang, Gao, Zhuang Liu, and Kilian Q. Weinberger. \"Densely Connected Convolutional Networks.\" arXiv preprint arXiv:1608.06993 (2016).](https://arxiv.org/pdf/1608.06993)","src/content/post/residual-net.mdx","c43dcf94b46f4a77","residual-net.mdx","rejection-sampling",{"id":558,"data":560,"body":565,"filePath":566,"digest":567,"legacyId":568,"deferredRender":23},{"title":561,"description":562,"publishDate":563,"draft":15,"tags":564},"Rejection Sampling","Rejection is always painful, but it's for the greater good! You can sample from a complicated distribution by rejecting samples!",["Date","2015-10-21T10:45:00.000Z"],[17,42,43],"import BlogImage from \"@/components/BlogImage.astro\";\n\nRejection Sampling is one of the simplest sampling algorithm. Every introductory text about Monte Carlo method use this algorithm as a first example. The de facto example of this is to approximate pi.\n\nAs with MCMC method like Metropolis-Hastings and Gibbs Sampling, Rejection Sampling is used to draw samples from a complicated target distribution where direct sampling is hard. This can be done by using a proposal distribution `Q(x)` that is easy to sample from. This `Q(x)` has to have an important property, namely, `Q(x)` has to envelope the target distribution `P(x)`. That means, given a scaling factor `k`, it has to be `kQ(x) > P(x)`, for all x. In other word, our target distribution has to be entirely under our scaled proposal distribution.\n\nFor clearer picture, let's dive into the code.\n\n```python\nimport numpy as np\nimport scipy.stats as st\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set()\n\ndef p(x):\n    return st.norm.pdf(x, loc=30, scale=10) + st.norm.pdf(x, loc=80, scale=20)\n\ndef q(x):\n    return st.norm.pdf(x, loc=50, scale=30)\n\nx = np.arange(-50, 151)\nk = max(p(x) / q(x))\n\ndef rejection_sampling(iter=1000):\n    samples = []\n\n    for i in range(iter):\n        z = np.random.normal(50, 30)\n        u = np.random.uniform(0, k*q(z))\n\n        if u \u003C= p(z):\n            samples.append(z)\n\n    return np.array(samples)\n\nif __name__ == '__main__':\n    plt.plot(x, p(x))\n    plt.plot(x, k*q(x))\n    plt.show()\n\n    s = rejection_sampling(iter=100000)\n    sns.distplot(s)\n```\n\nThe target distribution that I want to sample here is a mixture of Gaussian: `N(30, 10) + N(80, 20)`. Take a note that this distribution is not normalized. Here's what it looks like.\n\n\u003CBlogImage imagePath='/img/rejection-sampling/00.png' />\n\nNow, I'll pick `N(50, 30)` as my proposal distribution `Q(x)`.\n\n\u003CBlogImage imagePath='/img/rejection-sampling/01.png' />\n\nIf we do Rejection Sampling on this `Q(x)` naively, it will surely failed, as those high probability areas of `P(x)` aren't covered by `Q(x)`. Intuitively, the acceptance rate will be lower if `Q(x)` isn't enveloping `P(x)`, which resulting in having a lot less samples in those high probability areas than it should.\n\nTo remedy that, we need to find scaling factor `k`, so that `kQ(x)` will envelop `P(x)` entirely. To find it, we need to get the maximum ratio of `P(x)` and `Q(x)`. So it will just be `k = max(P(x) / Q(x)) for all x`.\n\n\u003CBlogImage imagePath='/img/rejection-sampling/02.png' />\n\nThe main sampling algorithm is to produce a lot of samples from proposal distribution `Q(x)`, `z ~ Q(x)`, then uniformly pick the height of the distribution, so that it will be `u ~ Unif(0, kQ(z))`. Now that sample of `(z, u)` will be uniform under the `kQ(x)` curve. Then, we should just need to evaluate the height of our target distribution `P(x)` at point `z`. And then very intuitively, we accept `(z, u)` samples that are under the `P(x)` curve.\n\nGiven a long time to run and generating samples, then it will converge to `P(x)`. Let's see the result.\n\n\u003CBlogImage imagePath='/img/rejection-sampling/03.png' />\n\nRecall that our target distribution that we used in Rejection Sampling is unnormalized. However, given the samples, we could reconstruct the proper distribution. So normalization doesn't really matter. The requirement of the target distribution hence relaxed, we just need a function proportional to our true target distribution.\n\nRejection Sampling sure is simple. But it has some drawback. We have to have pretty good heuristic on choosing the proposal distribution `Q(x)`. It has to envelope the target distribution. Given a complicated target distribution, it's hard to know the shape, and it's hard to choose the proposal distribution.\n\nAlso when computing the scaling factor, we have to be careful to make it \"just enough\" enveloping the target distribution, as the rejection rate will be proportional to the ratio of `P(x)` and `Q(x)`. Having a really big `Q(x)` sure envelop the `P(x)`, but it also means that there are a lot of wasted space that the algorithm will reject, which means a wasted computational time.","src/content/post/rejection-sampling.mdx","042ff93f3b66d8f0","rejection-sampling.mdx","slice-sampling",{"id":569,"data":571,"body":576,"filePath":577,"digest":578,"legacyId":579,"deferredRender":23},{"title":572,"description":573,"publishDate":574,"draft":15,"tags":575},"Slice Sampling","An implementation example of Slice Sampling for a special case: unimodal distribution with known inverse PDF",["Date","2015-10-24T12:04:00.000Z"],[17,42,43],"import BlogImage from \"@/components/BlogImage.astro\";\n\nAnother MCMC method beside Metropolis-Hastings and Gibbs Sampling, Slice Sampling generates random samples based on their previous states. Those samples then could be used to compute integrals (mean, median, etc) or just to plot the histogram, like I'll do here.\n\nCompared to M-H and Gibbs Sampling, Slice Sampling doesn't need any helper distribution. Whereas M-H needs us to pick a transition distribution `Q(x -> x')`, and Gibbs Sampling needs us to find full conditionals of the joint distribution analytically, Slice Sampling will only need us to provide the target distribution.\n\nFor a special case of Slice Sampling where target distribution `P(x)` is unimodal, if we could get our hand to the inverse function of the PDF, implementing Slice Sampling will be a breeze. This post will only cover this special case as it's the simplest.\n\nLet's say we have a Standard Normal from which we want to sample with Slice Sampling. Because Gaussian is unimodal, and the inverse PDF could be easily found analytically, it fits this special case. I'm not going to put the step by step derivation of the inverse PDF here as this blog don't have math engine to render formula.\n\n\u003CBlogImage imagePath='/img/slice-sampling/00.png' />\n\nThe PDF and inverse PDF of Standard Normal are as follow in Python code:\n\n```python\ndef p(x):\n    return np.exp(-(x**2) / 2) / (np.sqrt(2*np.pi))\n\ndef p_inv(y):\n    return np.sqrt(-2 * np.log(y _ np.sqrt(2*np.pi)))\n```\n\nHere's the inverse PDF of that Gaussian:\n\n\u003CBlogImage imagePath='/img/slice-sampling/01.png' />\n\nWhy do we need the inverse PDF? Recall that if we have `y = f(x)`, then if we have y, we could find the x using the inverse function: `x = f_inv(y)`. To give you some visualization, consider a Gaussian. The PDF will give you probability `P(x)` when you feed it with some `x`. Now what happen if we want to know, which `x` would yield this probability `P(x)`? That's where the inverse function comes in. Similarly, think about the relation of CDF and Quantile Function of probability distribution. Quantile Function is an inverse of CDF. Given an `x`, CDF will give you cumulative probability up to `x`. Given a cumulative probability `c`, Quantile Function will give you the `x` such that the cumulative probability at point `x` is `c`.\n\nHere's the algorithm for Standard Normal case (mu = 0, sigma = 1):\n\n1. Set a starting point `x`\n2. Sample the height `u ~ Unif(0, p(x))`\n3. Sample the next `x ~ Unif(-z, z)`, where `z` is the inverse PDF evaluated at `u`\n4. Go to step 2\n\nAs you can see right away, this is MCMC indeed, as to get the next sample of `x` depends on auxiliary variable `u`, which in turn depends on the current sample of `x`.\n\nIntuitively, let's say our starting point is `x = 0`. Then the Slice Sampler will sample a height at that point uniformly from 0 up to the height of the true distribution, or a function that's proportional to the true distribution. That means the sampler will sample `u ~ Unif(0, 0.4)`. Now let's say `u` is 0.25, we then use our inverse PDF to find z, which is x at the right boundary of the curve, which is 1. The inverse function of Gaussian is the right half of that Gaussian. If we're sampling uniformly from `[-z, z]: x ~ Unif(-z, z)`, because Gaussian is symmetric, then it means that we're sampling the line segment that slicing the area under the curve at height `u`. Using the new `x`, we proceed to find the new u, and so on, rinse and repeat until we get the number of samples we want.\n\nThe example above is for Standard Normal, the special case of Gaussian. If we want to use generalize the Slice Sampler, then we need to do some modification. First, we need to derive the generalized inverse Gaussian PDF, which means we need to take parameter mu and sigma into account when inversing the PDF. Second, we need to take into account the mu when finding the line segment. mu in Gaussian is used to shift the center of the distribution. Because in Standard Normal the mu is 0, we could leave that out. But for general Gaussian, the range of the line segment under the curve at height u would be `[mu-z, mu+z]`.\n\nLet's see that in the code. Here, in the code, I picked an arbitrary parameter for the Gaussian, `mu=65` and `sigma=32`.\n\n```python\nimport numpy as np\nimport scipy.stats as st\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set()\nmu = 65\nsigma = 32\n\ndef p(x):\n    return st.norm.pdf(x, loc=mu, scale=sigma)\n\ndef p*inv(y):\n    x = np.sqrt(-2*sigma**2 * np.log(y * sigma _ np.sqrt(2*np.pi)))\n    return mu-x, mu+x\n\ndef slice_sampling(iter=1000):\n    samples = np.zeros(iter)\n    x = 0\n\n    for i in range(iter):\n        u = np.random.uniform(0, p(x))\n        x_lo, x_hi = p_inv(u)\n        x = np.random.uniform(x_lo, x_hi)\n        samples[i] = x\n\n    return samples\n\nif __name__ == '__main__':\n    samples = slice_sampling(iter=10000)\n    sns.distplot(samples, kde=False, norm_hist=True)\n    x = np.arange(-100, 250)\n    plt.plot(x, p(x))\n    plt.show()\n```\n\nHere's the sampling result. The green curve is the real PDF of `Normal(65, 32)`.\n\n\u003CBlogImage imagePath='/img/slice-sampling/02.png' />\n\nSlice Sampling, being MCMC method, obviously shares some characteristic of the other MCMC methods like Metropolis-Hastings or Gibbs Sampling. Because it's Markovian, then there is a correlation between samples, which makes the samples not i.i.d. To remedy that, we could use technique called thinning, to only collect sample after some iteration to reduce the correlation. Another thing is that we should consider some burn-in period before collecting sample, to make sure we get only samples we care.\n\nThe advantage of Slice Sampling compared to M-H and Gibbs Sampling of course is that we don't need to derive or pick anything other than a function that is proportional to the true distribution. However, the implementation of generalized Slice Sampling is rather complicated because we have to consider a lot more cases like when the distribution is multimodal, etc. The hardest part would be to find the line segment under the curve at height u. The special case on multimodal distribution makes finding the line segment really easy.\n\nIn this post, we only evaluated Slice Sampling for one dimensional distribution. To sample from multidimensional distribution, we could use Slice Sampling like Gibbs Sampling: run the sampler in turn for each dimension.","src/content/post/slice-sampling.mdx","f0dd54b6957cbe32","slice-sampling.mdx","scrapy-long-running",{"id":580,"data":582,"body":588,"filePath":589,"digest":590,"legacyId":591,"deferredRender":23},{"title":583,"description":584,"publishDate":585,"draft":15,"tags":586},"Scrapy as a Library in Long Running Process","Scrapy is a great web crawler framework, but it's tricky to make it runs as a library in a long-running process. Here's how!",["Date","2016-06-10T07:37:00.000Z"],[587,43,42],"scrapy","There's no denying that Scrapy is one of the best crawler frameworks for Python, if not the best. Scrapy's developers designed Scrapy as a standalone process, i.e. in a different instance from our main program, and using database for communication between them.\n\nSometimes, we want to integrate Scrapy in our own framework. It's a little bit tricky because it's not what Scrapy designed for. In the documentation page, there are some information on how to run Scrapy from our own script: http://doc.scrapy.org/en/latest/topics/practices.html.\n\nIt's sufficient if our goal is to use Scrapy from our own script for one time crawling, i.e. execute the script, retrieve the result, then terminate it. However, it won't work if our goal is to make Scrapy run from our own long-running worker instance. After the first execution, it will raise `twisted.internet.error.ReactorNotRestartable` error.\n\nThere's a workaround for this, and in my experience, this solution is quite robust: I haven't encountered a single failure since I implemented this solution. The workaround is to use `multiprocessing` module in Python's standard library.\n\nLet's define our crawler module. I'll assume that we've already had our spiders defined.\n\n```python\nfrom scrapy import signals\nfrom scrapy.crawler import CrawlerProcess, Crawler\nfrom scrapy.settings import Settings\n\nclass CustomCrawler(object):\n\n    def crawl(self, spider):\n        crawled_items = []\n\n    def add_item(item):\n        crawled_items.append(item)\n\n        process = CrawlerProcess()\n\n        crawler = Crawler(spider, self.settings)\n        crawler.signals.connect(add_item, signals.item_scraped)\n        process.crawl(crawler)\n\n        process.start()\n\n        return crawled_items\n```\n\nHere, we're creating the `CustomCrawler` just as advised by the official Scrapy documentation. We use Scrapy's `signals` to get the crawled item, and add it to a list that stores our crawled items. So, each time an item is crawled, it will send a signal. If we caught such signal, then we add the said crawled item to the list. In the end of this method, we'll have a list that contains all of the items we've crawled.\n\nThis solution alone will suffice if our goal is to create a script for one-time-and-terminate crawler job. To make it works as long-running crawler job, we have to wrap the `CustomCrawler` class with Python's `multiprocessing` library.\n\n```python\nimport multiprocessing as mp\n\ndef crawl():\n    def _crawl(queue):\n        crawler = CustomCrawler() # Assume we have a spider class called: WebSpider\n        res = crawler.crawl(WebSpider)\n        queue.put(res)\n\n    q = mp.Queue()\n    p = mp.Process(target=_crawl, args=(q))\n    p.start()\n    res = q.get()\n    p.join()\n\n    return res\n```\n\nOne thing to note though, we have to execute this line: `res = q.get()` **before** `p.join()`. This is a known issue for this implementation. If you know what's up with this, drop a comment or reply in this Stackoverflow thread: http://stackoverflow.com/questions/35810024/python-multiprocessing-queue-behavior.\n\nHaving wrapped our `CustomCrawler` class, we then could use it inside our long-running worker.\n\n```python\nimport time\n\nwhile True:\n    items = crawl() # do something with crawled items ...\n    time.sleep(3600)\n```\n\nAnd, that's it! Now our worker will do the crawling job periodically without ever terminating!","src/content/post/scrapy-long-running.mdx","5ce70ed75537d783","scrapy-long-running.mdx","theano-pde",{"id":592,"data":594,"body":601,"filePath":602,"digest":603,"legacyId":604,"deferredRender":23},{"title":595,"description":596,"publishDate":597,"draft":15,"tags":598},"Theano for solving Partial Differential Equation problems","We all know Theano as a forefront library for Deep Learning research. However, it should be noted that Theano is a general purpose numerical computing library, like Numpy. Hence, in this post, we will look at the implementation of PDE simulation in Theano.",["Date","2017-01-08T08:34:00.000Z"],[599,600],"pde","theano","import BlogImage from \"@/components/BlogImage.astro\";\n\nWe all know Theano as a forefront library for Deep Learning research. However, it should be noted that Theano is a general purpose numerical computing library, like Numpy. Hence, in this post, we will look at the implementation of PDE simulation in Theano.\n\n## The Laplace Equation\n\nWe will look at a simple PDE example, the [Laplace Equation](https://en.wikipedia.org/wiki/Laplace's_equation):\n\n$$\n    \\nabla^2 \\phi = 0\n$$\n\nIn other words, this is an second order PDE, as, recall that Laplacian in calculus is the divergence of gradient of a function:\n\n$$\n\\nabla \\cdot \\nabla \\phi = 0\n$$\n\nparticularly, in two dimension:\n\n$$\n\\frac{\\partial^2 \\phi}{\\partial x^2} + \\frac{\\partial^2 \\phi}{\\partial y^2} = 0\n$$\n\nThis simple equation could be solved by using Finite Difference scheme [1].\n\nNote that in this example, we are ignoring the boundary value problem.\n\n## Solving Laplace Equation in Numpy\n\nThe Finite Difference solution of Laplace Equation is to repeatedly averaging the neighbors of a particular point:\n\n$$\n\\phi*{i, j} = \\frac{1}{4} \\left( \\phi*{i+1, j} + \\phi*{i+1, j} + \\phi*{i, j+1} + \\phi\\_{i, j-1} \\right)\n$$\n\nThis iterative solution is very simple to implement in Numpy. But first, let's give this problem an initial condition:\n\n$$\n\\phi = -5 \\, \\exp (x^2 + y^2)\n$$\n\nVisualizing the function:\n\n\u003CBlogImage imagePath='/img/theano-pde/initial.png' altText='Initial function.' />\n\nNow, let's implement this. First, we create the mesh, the solution space:\n\n```python\n# Create 21x21 mesh grid\nm = 21\nmesh_range = np.arange(-1, 1, 2/(m-1))\nx, y = np.meshgrid(mesh_range, mesh_range)\n\n# Initial condition\nU = np.exp(-5 \\* (x**2 + y**2))\n```\n\nWe then create an indexing scheme that select the point north, west, south, and east of any given point. We do this so that we could implement this in vectorized manner.\n\n```python\n# [1, 2, ... , 19, 19]\nn = list(range(1, m-1)) + [m-2]\ne = n\n\n# [0, 0, 1, 2, ... , 18]\ns = [0] + list(range(0, m-2))\nw = s\n```\n\nHaving those indices, we could translate the PDE solution above in the code:\n\n```python\ndef pde_step(U):\n    return (U[n, :]+U[:, e]+U[s, :]+U[:, w])/4.\n```\n\nFinally, we iteratively apply this update function.\n\n```python\nU_step = U\n\nfor it in range(500):\n    U_step = pde_step(U_step)\n```\n\nHere is the result:\n\n\u003CBlogImage imagePath='/img/theano-pde/final.png' altText='Final function.' />\n\nAgain, as we do not consider boundary value problem, the surface is diminishing until it is flat.\n\n## From Numpy to Theano\n\nTranslating the Numpy code to Theano is straightforward with caveat. The only thing different in the initialization is the variables: instead of Numpy array, we are now using Theano tensor.\n\n```python\nimport theano as th\nfrom theano import tensor as T\n\n# Create 21x21 mesh grid\n\nm = 21\nmesh_range = np.arange(-1, 1, 2/(m-1))\nx_arr, y_arr = np.meshgrid(mesh_range, mesh_range)\n\n# Initialize variables\n\nx, y = th.shared(x_arr), th.shared(y_arr)\nU = T.exp(-5 \\* (x**2 + y**2))\n\nn = list(range(1, m-1)) + [m-2]\ne = n\ns = [0] + list(range(0, m-2))\nw = s\n\ndef pde_step(U):\nreturn (U[n, :]+U[:, e]+U[s, :]+U[:, w])/4.\n```\n\nWe are using Theano's shared variables for our mesh variables as they are constants.\n\nIn the iteration part, things get little more interesting. In Theano, we replace loop with `scan` function. It is unintuitive at first, though.\n\n```python\nk = 5\n\n# Batch process the PDE calculation, calculate together k steps\n\nresult, updates = th.scan(fn=pde_step, outputs_info=U, n_steps=k)\nfinal_result = result[-1]\ncalc_pde = th.function(inputs=[U], outputs=final_result, updates=updates)\n```\n\nWhat it does is to apply function `pde_step` repeatedly for `k` steps, and we initialize the tensor we are interested in with the initial state of `U`.\n\nThe output of this `scan` function is the result of each time step, in this case, there are `k` results. Hence, as we are only interested in the latest state of `U` (or \\\\( \\phi \\\\)), we just pluck the last result.\n\nFinally, we wrap this into a Theano's function. The input is the current value of `U` and the output is `U` after applying `pde_step`, `k` times on `U`.\n\n```python\nU_step = U.eval()\n\nfor it in range(100): # Every k steps, draw the graphics\n    U_step = calc_pde(U_step)\n    draw_plot(x_arr, y_arr, U_step)\n```\n\nWe could think of our `calc_pde` as a batch processing of `k` iterations of our PDE. After each batch, we could use the latest `U` for e.g. visualization.\n\nWe might ask ourselves, do we need `k` to be greater than one? Definitely yes, as at every function call, we are sending our matrices from CPU to GPU, and receive them back. Low value of `k` definitely introduces overhead.\n\n## Why not TensorFlow?\n\nTensorFlow is definitely an interesting library, on par with Theano in Deep Learning domain. However, the problem with TensorFlow is that it is still not very mature.\n\nThis piece of codes:\n\n```python\nn = list(range(1, m-1)) + [m-2]\ne = n\ns = [0] + list(range(0, m-2))\nw = s\n\ndef pde_step(U):\n    return (U[n, :]+U[:, e]+U[s, :]+U[:, w])/4.\n```\n\ndoes not have any difference at all in Numpy and Theano version. However, we could not do this (at least easily) in TensorFlow.\n\nFeature and behavior parity with Numpy is definitely a big factor, and it seems Theano is in front of TensorFlow in this regard, at least for now.\n\n## References\n\nIn this post we looked at a very simple Finite Difference solution of Laplace Equation. We implemented it in both Numpy and Theano.\n\nAlthough Theano could be non-intuitive, especially in loop, it has better parity to Numpy compared to TensorFlow.\n\nThe full code, both in Numpy and Theano, could be found here: https://gist.github.com/wiseodd/c08d5a2b02b1957a16f886ab7044032d.\n\n## References\n\nMitra, Ambar K. \"Finite difference method for the solution of Laplace equation.\" preprint.","src/content/post/theano-pde.mdx","ab06d37c8bdde51e","theano-pde.mdx","twitter-auth-flask",{"id":605,"data":607,"body":612,"filePath":613,"digest":614,"legacyId":615,"deferredRender":23},{"title":608,"description":609,"publishDate":610,"draft":15,"tags":611},"Twitter Authentication with Tweepy and Flask","A tutorial on how to do Twitter OAuth authentication in Flask web application.",["Date","2015-08-29T09:46:00.000Z"],[43,42,181],"Recently, I created an app using Flask for the first time. The app uses Twitter API to get the data, where user could log in with her Twitter account, and she will get the report she needs. Here's the app: http://responsetime.thirdworldnomad.com.\n\nI was wondering, how do I do the OAuth authentication flow? Turned out, it's really easy!\n\nFirst, create an Flask route to redirect the user to Twitter login page:\n\n```python\nconsumer_key = ''\nconsumer_secret = ''\ncallback = 'http://yourdoamain.com/callback'\n\n@app.route('/auth')\ndef auth():\n    auth = tweepy.OAuthHandler(consumer_key, consumer_secret, callback)\n    url = auth.get_authorization_url()\n    session['request_token'] = auth.request_token\n    return redirect(url)\n```\n\nWhat it does is to create an authentication handler according to your Twitter app's keys, and your predefined callback. This callback URL should point to your another Flask endpoint that I'll describe after this. Then, you'll need to redirect your user to the authorization URL that Tweepy constructed for you. Here, we save the `request_token` to the session because we need it to get the user's access token in the next endpoint. Session is a great way to handle states between requests.\n\nNext, let's handle the case after user has logged in via Twitter. Twitter will redirect the user to your predefined callback URL.\n\n```python\n@app.route('/callback')\ndef twitter_callback():\n    request_token = session['request_token']\n    del session['request_token']\n\n    auth = tweepy.OAuthHandler(consumer_key, consumer_secret, callback)\n    auth.request_token = request_token\n    verifier = request.args.get('oauth_verifier')\n    auth.get_access_token(verifier)\n    session['token'] = (auth.access_token, auth.access_token_secret)\n\n    return redirect('/app')\n```\n\nHere, we get the `request_token` that we've saved from the previous request on `/auth`. As session is unique per user, it's guaranteed that there's no token mixup between users. Because the `request_token` won't be used anymore, it's recommended to delete it.\n\nNext, we have to reconstruct the Tweepy's authentication handler again, using the same parameters. It will be rejected if the parameters are different. Twitter will append an query param `oauth_verifier` when requesting our callback endpoint. We'll use that token to be exchanged with the real access token.\n\nThe next is simple, just call Tweepy's `get_access_token` function and pass the token we get from the query param. Tweepy will exchange it for user's access token and secret token. You can store it in the database, or just save it in the session. Using session is simpler, but it won't persist, as it could expire.\n\nNow that we have user's access token and secret token, we can use it to call Twitter API.\n\n```python\n@app.route('/app')\ndef request_twitter:\n    token, token_secret = session['token']\n    auth = tweepy.OAuthHandler(consumer_key, consumer_secret, callback)\n    auth.set_access_token(token, token_secret)\n    api = tweepy.API(auth)\n\n    return api.me()\n```\n\nTo use the token, we'll need to get it from the session, then pass it to Tweepy `OAuthHandler` object, and create an API object based on that. Then, we could use the API object to request the Twitter REST and Streaming API.\n\nSo, that's it. It's surprising how easy it is to do OAuth authentication using Tweepy. On another web framework, like Django or Bottle, the principle and the flow is similar.","src/content/post/twitter-auth-flask.mdx","76a0819a0cc26851","twitter-auth-flask.mdx","vae-pytorch",{"id":616,"data":618,"body":624,"filePath":625,"digest":626,"legacyId":627,"deferredRender":23},{"title":619,"description":620,"publishDate":621,"draft":15,"tags":622},"Variational Autoencoder (VAE) in Pytorch","With all of those bells and whistles surrounding Pytorch, let's implement Variational Autoencoder (VAE) using it.",["Date","2017-01-24T17:26:00.000Z"],[17,623,238],"vae","This post should be quick as it is just a port of the previous Keras code. For the intuition and derivative of Variational Autoencoder (VAE) plus the Keras implementation, check previou post. The full code is available in my Github repo: https://github.com/wiseodd/generative-models.\n\n## The networks\n\nLet's begin with importing stuffs.\n\n```python\nimport torch\nimport torch.nn.functional as nn\nimport torch.autograd as autograd\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport os\nfrom torch.autograd import Variable\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\nmb_size = 64\nZ_dim = 100\nX_dim = mnist.train.images.shape[1]\ny_dim = mnist.train.labels.shape[1]\nh_dim = 128\nc = 0\nlr = 1e-3\n```\n\nNow, recall in VAE, there are two networks: encoder $Q(z \\vert X)$ and decoder $P(X \\vert z)$. So, let's build our $Q(z \\vert X)$ first:\n\n```python\ndef xavier_init(size):\n    in_dim = size[0]\n    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n    return Variable(torch.randn(size) * xavier_stddev, requires_grad=True)\n\nWxh = xavier_init(size=[X_dim, h_dim])\nbxh = Variable(torch.zeros(h_dim), requires_grad=True)\n\nWhz_mu = xavier_init(size=[h_dim, Z_dim])\nbhz_mu = Variable(torch.zeros(Z_dim), requires_grad=True)\n\nWhz_var = xavier_init(size=[h_dim, Z_dim])\nbhz_var = Variable(torch.zeros(Z_dim), requires_grad=True)\n\ndef Q(X):\n    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1)\n    z_var = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n    return z_mu, z_var\n```\n\nOur $Q(z \\vert X)$ is a two layers net, outputting the $\\mu$ and $\\Sigma$, the parameter of encoded distribution. So, let's create a function to sample from it:\n\n```python\ndef sample_z(mu, log_var):\n    # Using reparameterization trick to sample from a gaussian\n    eps = Variable(torch.randn(mb_size, Z_dim))\n    return mu + torch.exp(log_var / 2) * eps\n```\n\nLet's construct the decoder $P(z \\vert X)$, which is also a two layers net:\n\n```python\nWzh = xavier_init(size=[Z_dim, h_dim])\nbzh = Variable(torch.zeros(h_dim), requires_grad=True)\n\nWhx = xavier_init(size=[h_dim, X_dim])\nbhx = Variable(torch.zeros(X_dim), requires_grad=True)\n\ndef P(z):\n    h = nn.relu(z @ Wzh + bzh.repeat(z.size(0), 1))\n    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n    return X\n```\n\nNote, the use of `b.repeat(X.size(0), 1)` is because this [Pytorch issue](https://github.com/pytorch/pytorch/issues/491).\n\n## Training\n\nNow, the interesting stuff: training the VAE model. First, as always, at each training step we do forward, loss, backward, and update.\n\n```python\nparams = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var,\nWzh, bzh, Whx, bhx]\n\nsolver = optim.Adam(params, lr=lr)\n\nfor it in range(100000):\n    X, _ = mnist.train.next_batch(mb_size)\n    X = Variable(torch.from_numpy(X))\n\n    # Forward\n    # ...\n\n    # Loss\n    # ...\n\n    # Backward\n    # ...\n\n    # Update\n    # ...\n\n    # Housekeeping\n    for p in params:\n        p.grad.data.zero_()\n```\n\nNow, the forward step:\n\n```python\nz_mu, z_var = Q(X)\nz = sample_z(z_mu, z_var)\nX_sample = P(z)\n```\n\nThat is it. We just call the functions we defined before. Let's continue with the loss, which consists of two parts: reconstruction loss and KL-divergence of the encoded distribution:\n\n```python\nrecon_loss = nn.binary_cross_entropy(X_sample, X, size_average=False)\nkl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var)\nloss = recon_loss + kl_loss\n```\n\nBackward and update step is as easy as calling a function, as we use Autograd feature from Pytorch:\n\n```python\n# Backward\nloss.backward()\n\n# Update\nsolver.step()\n```\n\nAfter that, we could inspect the loss, or maybe visualizing $P(X \\vert z)$ to check the progression of the training every now and then.\n\nThe full code could be found here: https://github.com/wiseodd/generative-models.","src/content/post/vae-pytorch.mdx","17eb8813b8648b68","vae-pytorch.mdx","volume-form",{"id":628,"data":630,"body":635,"filePath":636,"digest":637,"legacyId":638,"deferredRender":23},{"title":631,"description":632,"publishDate":633,"draft":15,"tags":634},"Volume Forms and Probability Density Functions Under Change of Variables","From elementary probability theory, it is well known that a probability density function (pdf) is not invariant under an arbitrary change of variables (reparametrization). In this article we'll see that pdf are actually invariant when we see a pdf in its entirety, as a volume form and a Radon-Nikodym derivative in differential geometry.",["Date","2023-12-13T05:00:00.000Z"],[],"import BlogImage from \"@/components/BlogImage.astro\";\n\nSuppose we have $\\R^n$ equipped with the Cartesian **_coordinates_**; the latter represents a point in $\\R^n$ with $x = (x^1, \\dots, x^n)$, an $n$-tuple of numbers via the identity function $\\mathrm{Id}_{\\R^n}$---this is because $\\R^n$ itself is already defined as the space of tuples of $n$ numbers.\n(Note that $v^i$ is not a power, but just indexing; we write e.g. $(v^i)^2$ if we need to take the power.)\n\n\u003CBlogImage\n  imagePath='/img/volume-form/01_coords.jpg'\n  altText='Coordinates of the Euclidean space.'\n/>\n\nHere are some interesting objects to study in this setting.\n\n## Riemannian Metrics\n\nIn $\\R^d$, we usually have the standard Euclidean inner product $\\inner{v, w} = \\sum_{i=1}^n v^i w^i$ where $v = (v^1, \\dots, v^n)$ and $w = (w^1, \\dots, w^n)$ are two vectors.\nWe can write an inner product in terms of an inner product matrix $\\inner{v, w} = v^\\top G w$.\n\n\u003CBlogImage imagePath='/img/volume-form/02_metric.jpeg' altText='Riemannian metric.' />\n\nThe matrix $G$, which is symmetric positive definite, is called (the matrix representation of) a **_Riemannian metric_**.\nIn the case of the Euclidean inner product, we have $G = I$, the identity $n \\times n$ matrix.\n\n## Volume Forms\n\nAnother interesting object is the **_volume form_** $dx$.\nThis is a differential form of degree $n$, meaning that it takes $n$ vectors as arguments and returns a number.\nThere is a deeper meaning in the notation, but for the purpose of this post, it suffices to say that $dx$ measures the volume of a parallelepiped spanned by $n$ vectors.\nIndeed, the evaluation $dx(v_1, \\dots, v_n)$ on vectors $v_1, \\dots, v_n$ is obtained by computing the determinant of the matrix resulting from stacking the tuples $v_1, \\dots, v_n$.\nAn important fact is that if $f: \\R^n \\to \\R$ is any continuous function on $\\R^n$, then $f \\, dx$ is also a volume form.\n\n\u003CBlogImage imagePath='/img/volume-form/03_dx.jpeg' altText='Volume form.' />\n\nThe Riemannian metric $G$ and the volume form $dx$ can be combined to obtain a special volume form\n\n$$\n  dV_G = \\sqrt{\\abs{\\det{G}}} \\, dx\n$$\n\ncalled the **_Riemannian volume form_**.\nIn the case of $\\R^n$ with the Cartesian coordinates and the standard dot product, $G = I$, so, $dV_G = dx$ is a special case.\nThe idea here is that non-identity $G$'s \"distort\" the Cartesian grids and thus the volume changes proportionally to the distortion.\nFor this reason, $dV_G$ is _the_ natural volume form for any choice of metric and any manifold in general.\nIndeed, technically speaking, it is the unique volume form that evaluates to one on parallelepipeds spanned by orthonormal basis vectors.\n\n## Volume Forms and Measures\n\nA non-negative volume form $f \\, dx$ induces a measure via $\\mu(A) = \\int_A f \\, dx$ for $A$ Borel measurable subset of $\\R^n$.\nOne can then see that $dx$ is the volume form corresponding to the Lebesgue measure $\\mu(A) = \\int_A dx$.\n\nSuppose we have a probability measure (with support $\\R^n$) and assume that it can be expressed as $P(A) = \\int_A p \\, dx$.\nThen, $p$ is the probability density function (pdf) of $P$ under the reference measure $dx$, i.e., it is positive everywhere $p > 0$ and it integrates to one under $dx$, that is, $\\int_{\\R^n} p \\, dx = 1$.\n\nAnother way to define $p$ as a pdf is via the Radon-Nikodym derivative\n\n$$\n  p = \\frac{p \\, dx}{dx} .\n$$\n\nThen it's clear that we can take any volume form as the reference measure, not just $dx$.\nE.g., we can take\n\n$$\n  p_G := \\frac{p \\, dx}{dV_G} = \\frac{p \\, dx}{\\sqrt{\\abs{\\det{G}}} \\, dx} = p \\, \\abs{\\det{G}}^{-\\frac{1}{2}} ,\n$$\n\nwhich is a pdf under $dV_G$ since it's still positive (note that $G$ is positive-definite) and\n\n$$\n  \\int_{\\R^n} p \\, \\abs{\\det{G}}^{-\\frac{1}{2}} \\, dV_G = \\int_{\\R^n} p \\, \\cancel{\\abs{\\det{G}}^{-\\frac{1}{2}}} \\, \\cancel{\\abs{\\det{G}}^{\\frac{1}{2}}} \\, dx = 1 ,\n$$\n\ni.e., it integrates to one under $dV_G$.\n\n## Change of Variables\n\nNow, assume that we have another coordinates for $\\R^n$, say, representing each element of $\\R^n$ with $y = (y^1, \\dots, y^n)$ instead.\nThe change of coordinates function, mapping $x \\mapsto y$ is a diffeomorphism---a differentiable function with a differentiable inverse.\nLet's call it $\\varphi$; and call its $n \\times n$ Jacobian matrix $J = [\\partial y^i / \\partial x^j]\\_{i,j=1}^n$ with inverse $J^{-1} = [\\partial x^i / \\partial y^j]_{ij=1}^n$.\n\n\u003CBlogImage imagePath='/img/volume-form/04_cov.jpeg' altText='Change of coordinates.' />\n\nHere are some rules for transforming a metric and a volume form.\n\nIf $G$ is a matrix representation of a Riemannian metric in $x$-coordinates, then\n\n$$\n  \\widehat{G} = (J^{-1})^\\top G J^{-1}\n$$\n\nis the matrix representation of the same metric in $y$-coordinates.\nConsequently, the determinant of the metric $\\abs{\\det G}$ transforms into $\\abs{\\det G} \\, \\abs{J^{-1}}^2$.\nThis transformation rule is to ensure that if $\\hat{v}, \\hat{w}$ are the representations of $v, w$ in $y$-coordinates, then $\\hat{v}^\\top \\widehat{G} \\hat{w} = v^\\top G w$.\nThat is, the value of the inner product is independent of the choice of coordinates.\nIn other words, this rule is to make sure we are referring to the same abstract object (in this case inner product, which is an abstract function) even when we use a different representation.\n\nNow, if $f \\, dx$ is a volume form in $x$-coordinates, then\n\n$$\n  (f \\circ \\varphi^{-1}) \\, \\abs{\\det J^{-1}} \\, dy\n$$\n\nis the same volume form in $y$-coordinates.\nIn particular, we have the relation $dx = \\abs{\\det J^{-1}} \\, dy$ [2, Corollary 14.21].\nAgain, this rule is to ensure coordinate independence.\n\n\u003CBlogImage\n  imagePath='/img/volume-form/05_dx-cov.jpeg'\n  altText='Volume forms under change of coordinates.'\n/>\n\nAs a consequence, integrals are also invariant under a change of coordinates:\n\n$$\n  \\int_{\\varphi(A)} (f \\circ \\varphi) \\, \\underbrace{\\abs{\\det{J^{-1}}} \\, dy}_{=dx} = \\int_A f \\, dx \\,,\n$$\n\nwhere $A \\subseteq \\R^n$.\nNotice that this is just the standard change-of-variable rule in calculus.\nBut one thing to keep in mind is that the Jacobian-determinant term is part of the transformation of $dx$, not the function $f$ itself.\n\n## Pdfs Under Change of Variables\n\nFrom elementary probability theory, we have the transformation of a pdf $p_x$ (defined w.r.t. $dx$):\n\n$$\n  p_y = (p_x \\circ \\varphi^{-1}) \\, \\abs{\\det{J^{-1}}} \\, ,\n$$\n\nand this is known to be problematic because of the additional Jacobian-determinant term.\n\n\u003CBlogImage\n  imagePath='/img/volume-form/06_density-cov.jpeg'\n  altText='Probability densities under change of coordinates.'\n/>\n\nFor instance, the mode $\\argmax p_y$ of $p_y$ doesn't correspond to the mode $\\argmax p_x$ of $p_x$.\nThat is, modes of pdfs are not coordinate-independent.\n_Maximum a posterior_ (MAP) estimation, which is the standard estimation method for neural networks is thus pathological since an arbitrary reparametrization/change of variables will yield a different MAP estimate, see e.g. [1, Sec. 5.2.1.4]\nOr are they?\n\nThe reason for the above transformation rule between $p_x$ and $p_y$ is to ensure invariance in the integration, to ensure that $p_y$ is a valid pdf w.r.t. $dy$:\n\n$$\n\\begin{align}\n  \\int_{\\varphi(\\R^n)} p_y \\, dy &= \\int_{\\varphi(\\R^n)} (p_x \\circ \\varphi^\\inv) \\, \\underbrace{\\abs{\\det J^{-1}} \\, dy}_{= dx} \\\\\n    %\n    &= \\int_{\\R^n} p_x \\, dx \\\\[5pt]\n    %\n    &= 1 .\n\\end{align}\n$$\n\nHowever, as we have seen before, $\\abs{\\det J^{-1}}$ is part of the transformation of $dx$, i.e. $dx = \\abs{\\det J^{-1}} dy$!\nSo, the problem in pdf maximization is actually because we attribute the Jacobian-determinant to the wrong part of the volume measure $p_x \\, dx$.\nThis can only be detected if we see things holistically as the transformation of the whole volume form, and not just view it as the transformation of the function $p_x$ independently.\n\n\u003CBlogImage\n  imagePath='/img/volume-form/07_density-cov-correct.jpeg'\n  altText='Pdfs. under proper change of coordinates.'\n/>\n\nThis leads to a very straightforward solution to the non-invariance problem.\nSimply transform $p_x$ into $p_y = (p_x \\circ \\varphi^{-1})$.\nThis is just the transformation rule of standard function, so its extrema will always be coordinate-independent.\nIt is still a pdf w.r.t. $dy$, just don't forget to add a Jacobian-determinant term as part of the transformation from $dx$ to $dy$.\n\n## Riemannian Pdfs Under Change of Variables\n\nWhat about a Riemannian pdf $p_G = p_x \\, \\abs{\\det{G}}^{-\\frac{1}{2}}$ under the Riemannian volume form $dV_G$?\nFirst, recall that $\\abs{\\det{\\widehat{G}}} = \\abs{\\det{G}} \\, \\abs{\\det J^{-1}}^2$.\nSo,\n\n$$\n  p_{\\widehat{G}} = (p_x \\circ \\varphi^{-1}) \\, \\abs{\\det{G}}^{-\\frac{1}{2}} \\, \\abs{\\det J^{-1}}^{-1} .\n$$\n\nThis seems problematic since now we have the Jacobian determinant term again, just like the \"incorrect\" transformation of pdf in the previous section.\nIt actually is!\nJust look at the following integral that attempts to show that $p_{\\widehat{G}}$ integrates to one under $dV_{\\widehat{G}}$.\n\n$$\n\\begin{align}\n  \\int_{\\varphi(\\R^n)} p_{\\widehat{G}} \\, dV_{\\widehat{G}} &= \\int_{\\varphi(\\R^n)} (p_x \\circ \\varphi^{-1}) \\, \\cancel{\\abs{\\det{G}}^{-\\frac{1}{2}}} \\, \\cancel{\\abs{\\det J^{-1}}^{-1}} \\, \\cancel{\\abs{\\det J^{-1}}} \\, \\cancel{\\abs{\\det G}^{\\frac{1}{2}}} \\, dy \\\\\n    %\n    &= \\int_{\\varphi(\\R^n)} (p_x \\circ \\varphi^{-1}) \\, dy .\n\\end{align}\n$$\n\nWe now don't have the $\\abs{\\det{J^{-1}}}$ term anymore.\nSo we can't apply the relation $dx = \\abs{\\det{J^{-1}}} \\, dy$ to complete the steps.\nWhat gives?\n\nThis is actually because there is a Jacobian-determinant term that we forget about because we don't see things as a whole.\nThe complete way to see a pdf is in terms of the Radon-Nikodym derivative.\nSo, let's see, in $x$-coordinates, we have:\n\n$$\n  p_G = \\frac{p_x \\, dx}{\\abs{\\det{G}}^{\\frac{1}{2}} \\, dx} .\n$$\n\nNow in $y$-coordinates, we have the following by transforming both the volume forms in the numerator and the denominator:\n\n$$\n  p_{\\widehat{G}} = \\frac{(p_x \\circ \\varphi^{-1}) \\, \\cancel{\\abs{\\det{J^{-1}}} \\, dy}}{(\\abs{\\det{G}}^{\\frac{1}{2}} \\circ \\varphi^{-1}) \\, \\cancel{\\abs{\\det{J^{-1}}} \\, dy}} = (p_x \\circ \\varphi^{-1}) \\, \\abs{\\det{G}}^{-\\frac{1}{2}} .\n$$\n\nThe key is to view $\\abs{\\det{G}}^{\\frac{1}{2}}$ as a function in front of $dx$, which, by the transformation rule discussed previously, transforms into $\\abs{\\det{G}}^{\\frac{1}{2}} \\circ \\varphi^{-1}$.\nFor brevity, we might as well write it down as $\\abs{\\det{G}}^{\\frac{1}{2}}$, just remember that the domain of this function is the $y$-coordinates.\n\nCompare this to before: we now don't have the Jacobian-determinant term!\nPerforming the integration as before:\n\n$$\n\\begin{align}\n  \\int_{\\varphi(\\R^n)} p_{\\widehat{G}} \\, dV_{\\widehat{G}} &= \\int_{\\varphi(\\R^n)} (p_x \\circ \\varphi^{-1}) \\, \\cancel{\\abs{\\det{G}}^{-\\frac{1}{2}}} \\, \\cancel{\\abs{\\det G}^{\\frac{1}{2}}} \\, \\underbrace{\\abs{\\det J^{-1}} \\, dy}_{=dx} \\\\\n    %\n    &= \\int_{\\R^n} p_x \\, dx \\\\[5pt]\n    %\n    &= 1 .\n\\end{align}\n$$\n\nAnd therefore, we have shown that $(p_x \\circ \\varphi^{-1}) \\, \\abs{\\det{G}}^{-\\frac{1}{2}}$ is the correct transformation of $p_G$.\nNotice that this is again just a transformation of standard function and so the modes are coordinate-independent.\n\n## Conclusion\n\nTwo take-aways from this post.\nFirst, be aware of the correct transformation of objects.\nIn particular, for a volume form $f \\, dx$, the Jacobian-determinant is part of the transformation of $dx$, not the function $f$.\nThis way, we don't have any problem with MAP estimation.\n\nSecond, it's best to see things as a whole to avoid confusion.\nFor pdfs, write them holistically as Radon-Nikodym derivatives.\nThen, the correct transformations can easily be applied without confusion.\n\n## References\n\n1. Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT Press, 2012.\n2. Lee, John M. Introduction to Smooth Manifolds. 2003.","src/content/post/volume-form.mdx","0c15d18236f1bde5","volume-form.mdx","variational-autoencoder",{"id":639,"data":641,"body":646,"filePath":647,"digest":648,"legacyId":649,"deferredRender":23},{"title":642,"description":643,"publishDate":644,"draft":15,"tags":645},"Variational Autoencoder: Intuition and Implementation","Variational Autoencoder (VAE) (Kingma et al., 2013) is a new perspective in the autoencoding business. It views Autoencoder as a bayesian inference problem: modeling the underlying probability distribution of data.",["Date","2016-12-10T05:30:00.000Z"],[42,43,102],"import BlogImage from \"@/components/BlogImage.astro\";\n\nThere are two generative models facing neck to neck in the data generation business right now: Generative Adversarial Nets (GAN) and Variational Autoencoder (VAE). These two models have different take on how the models are trained. GAN is rooted in game theory, its objective is to find the Nash Equilibrium between discriminator net and generator net. On the other hand, VAE is rooted in bayesian inference, i.e. it wants to model the underlying probability distribution of data so that it could sample new data from that distribution.\n\nIn this post, we will look at the intuition of VAE model and its implementation in Keras.\n\n## VAE: Formulation and Intuition\n\nSuppose we want to generate a data. Good way to do it is first to decide what kind of data we want to generate, then actually generate the data. For example, say, we want to generate an animal. First, we imagine the animal: it must have four legs, and it must be able to swim. Having those criteria, we could then actually generate the animal by sampling from the animal kingdom. Lo and behold, we get Platypus!\n\nFrom the story above, our imagination is analogous to **latent variable**. It is often useful to decide the latent variable first in generative models, as latent variable could describe our data. Without latent variable, it is as if we just generate data blindly. And this is the difference between GAN and VAE: VAE uses latent variable, hence it's an expressive model.\n\nAlright, that fable is great and all, but how do we model that? Well, let's talk about probability distribution.\n\nLet's define some notions:\n\n1. $X$: data that we want to model a.k.a the animal\n2. $z$: latent variable a.k.a our imagination\n3. $P(X)$: probability distribution of the data, i.e. that animal kingdom\n4. $P(z)$: probability distribution of latent variable, i.e. our brain, the source of our imagination\n5. $P(X \\vert z)$: distribution of generating data given latent variable, e.g. turning imagination into real animal\n\nOur objective here is to model the data, hence we want to find $P(X)$. Using the law of probability, we could find it in relation with $z$ as follows:\n\n$$\n    P(X) = \\int P(X \\vert z) P(z) dz\n$$\n\nthat is, we marginalize out $z$ from the joint probability distribution $P(X, z)$.\n\nNow if only we know $P(X, z)$, or equivalently, $P(X \\vert z)$ and $P(z)$...\n\nThe idea of VAE is to infer $P(z)$ using $P(z \\vert X)$. This is make a lot of sense if we think about it: we want to make our latent variable likely under our data. Talking in term of our fable example, we want to limit our imagination only on animal kingdom domain, so we shouldn't imagine about things like root, leaf, tyre, glass, GPU, refrigerator, doormat, ... as it's unlikely that those things have anything to do with things that come from the animal kingdom. Right?\n\nBut the problem is, we have to infer that distribution $P(z \\vert X)$, as we don't know it yet. In VAE, as it name suggests, we infer $P(z \\vert X)$ using a method called Variational Inference (VI). VI is one of the popular choice of method in bayesian inference, the other one being MCMC method. The main idea of VI is to pose the inference by approach it as an optimization problem. How? By modeling the true distribution $P(z \\vert X)$ using simpler distribution that is easy to evaluate, e.g. Gaussian, and minimize the difference between those two distribution using [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) metric, which tells us how difference it is $P$ and $Q$.\n\nAlright, now let's say we want to infer $P(z \\vert X)$ using $Q(z \\vert X)$. The KL divergence then formulated as follows:\n\n$$\n\\begin{align}\n\nD_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)] &= \\sum_z Q(z \\vert X) \\, \\log \\frac{Q(z \\vert X)}{P(z \\vert X)} \\\\[10pt]\n                            &= E \\left[ \\log \\frac{Q(z \\vert X)}{P(z \\vert X)} \\right] \\\\[10pt]\n                            &= E[\\log Q(z \\vert X) - \\log P(z \\vert X)]\n\n\\end{align}\n$$\n\nRecall the notations above, there are two things that we haven't use, namely $P(X)$, $P(X \\vert z)$, and $P(z)$. But, with Bayes' rule, we could make it appear in the equation:\n\n$$\n\\begin{align}\nD_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)] &= E \\left[ \\log Q(z \\vert X) - \\log \\frac{P(X \\vert z) P(z)}{P(X)} \\right] \\\\[10pt]\n                                        &= E[\\log Q(z \\vert X) - (\\log P(X \\vert z) + \\log P(z) - \\log P(X))] \\\\[10pt]\n                                        &= E[\\log Q(z \\vert X) - \\log P(X \\vert z) - \\log P(z) + \\log P(X)]\n\\end{align}\n$$\n\nNotice that the expectation is over $z$ and $P(X)$ doesn't depend on $z$, so we could move it outside of the expectation.\n\n$$\n\\begin{align}\nD_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)] &= E[\\log Q(z \\vert X) - \\log P(X \\vert z) - \\log P(z)] + \\log P(X) \\\\[10pt]\nD_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)] - \\log P(X) &= E[\\log Q(z \\vert X) - \\log P(X \\vert z) - \\log P(z)]\n\\end{align}\n$$\n\nIf we look carefully at the right hand side of the equation, we would notice that it could be rewritten as another KL divergence. So let's do that by first rearranging the sign.\n\n$$\n\\begin{align}\nD_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)] - \\log P(X) &= E[\\log Q(z \\vert X) - \\log P(X \\vert z) - \\log P(z)] \\\\[10pt]\n\\log P(X) - D_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)] &= E[\\log P(X \\vert z) - (\\log Q(z \\vert X) - \\log P(z))] \\\\[10pt]\n                                       &= E[\\log P(X \\vert z)] - E[\\log Q(z \\vert X) - \\log P(z)] \\\\[10pt]\n                                       &= E[\\log P(X \\vert z)] - D_{KL}[Q(z \\vert X) \\Vert P(z)]\n\\end{align}\n$$\n\nAnd this is it, the VAE objective function:\n\n$$\n    \\log P(X) - D_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)] = E[\\log P(X \\vert z)] - D_{KL}[Q(z \\vert X) \\Vert P(z)]\n$$\n\nAt this point, what do we have? Let's enumerate:\n\n1. $Q(z \\vert X)$ that project our data $X$ into latent variable space\n2. $z$, the latent variable\n3. $P(X \\vert z)$ that generate data given latent variable\n\nWe might feel familiar with this kind of structure. And guess what, it's the same structure as seen in Autoencoder! That is, $Q(z \\vert X)$ is the encoder net, $z$ is the encoded representation, and $P(X \\vert z)$ is the decoder net! Well, well, no wonder the name of this model is Variational Autoencoder!\n\n## VAE: Dissecting the Objective\n\nIt turns out, VAE objective function has a very nice interpretation. That is, we want to model our data, which described by $\\log P(X)$, under some error $D_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)]$. In other words, VAE tries to find the lower bound of $\\log P(X)$, which in practice is good enough as trying to find the exact distribution is often untractable.\n\nThat model then could be found by maximazing over some mapping from latent variable to data $\\log P(X \\vert z)$ and minimizing the difference between our simple distribution $Q(z \\vert X)$ and the true latent distribution $P(z)$.\n\nAs we might already know, maximizing $E[\\log P(X \\vert z)]$ is a maximum likelihood estimation. We basically see it all the time in discriminative supervised model, for example Logistic Regression, SVM, or Linear Regression. In the other words, given an input $z$ and an output $X$, we want to maximize the conditional distribution $P(X \\vert z)$ under some model parameters. So we could implement it by using any classifier with input $z$ and output $X$, then optimize the objective function by using for example log loss or regression loss.\n\nWhat about $D_{KL}[Q(z \\vert X) \\Vert P(z)]$? Here, $P(z)$ is the latent variable distribution. We might want to sample $P(z)$ later, so the easiest choice is $N(0, 1)$. Hence, we want to make $Q(z \\vert X)$ to be as close as possible to $N(0, 1)$ so that we could sample it easily.\n\nHaving $P(z) = N(0, 1)$ also add another benefit. Let's say we also want $Q(z \\vert X)$ to be Gaussian with parameters $\\mu(X)$ and $\\Sigma(X)$, i.e. the mean and variance **given** X. Then, the KL divergence between those two distribution could be computed in closed form!\n\n$$\nD_{KL}[N(\\mu(X), \\Sigma(X)) \\Vert N(0, 1)] = \\frac{1}{2} \\, \\left( \\textrm{tr}(\\Sigma(X)) + \\mu(X)^T\\mu(X) - k - \\log \\, \\det(\\Sigma(X)) \\right)\n$$\n\nAbove, $k$ is the dimension of our Gaussian. $\\textrm{tr}(X)$ is trace function, i.e. sum of the diagonal of matrix $X$. The determinant of a diagonal matrix could be computed as product of its diagonal. So really, we could implement $\\Sigma(X)$ as just a vector as it's a diagonal matrix:\n\n$$\n\\begin{align}\nD_{KL}[N(\\mu(X), \\Sigma(X)) \\Vert N(0, 1)] &= \\frac{1}{2} \\, \\left( \\sum_k \\Sigma(X) + \\sum_k \\mu^2(X) - \\sum_k 1 - \\log \\, \\prod_k \\Sigma(X) \\right) \\\\[10pt]\n                                      &= \\frac{1}{2} \\, \\left( \\sum_k \\Sigma(X) + \\sum_k \\mu^2(X) - \\sum_k 1 - \\sum_k \\log \\Sigma(X) \\right) \\\\[10pt]\n                                      &= \\frac{1}{2} \\, \\sum_k \\left( \\Sigma(X) + \\mu^2(X) - 1 - \\log \\Sigma(X) \\right)\n\\end{align}\n$$\n\nIn practice, however, it's better to model $\\Sigma(X)$ as $\\log \\Sigma(X)$, as it is more numerically stable to take exponent compared to computing log. Hence, our final KL divergence term is:\n\n$$\nD_{KL}[N(\\mu(X), \\Sigma(X)) \\Vert N(0, 1)] = \\frac{1}{2} \\sum_k \\left( \\exp(\\Sigma(X)) + \\mu^2(X) - 1 - \\Sigma(X) \\right)\n$$\n\n## Implementation in Keras\n\nFirst, let's implement the encoder net $Q(z \\vert X)$, which takes input $X$ and outputting two things: $\\mu(X)$ and $\\Sigma(X)$, the parameters of the Gaussian.\n\n```python\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom keras.layers import Input, Dense, Lambda\nfrom keras.models import Model\nfrom keras.objectives import binary_crossentropy\nfrom keras.callbacks import LearningRateScheduler\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras.backend as K\nimport tensorflow as tf\n\nm = 50\nn_z = 2\nn_epoch = 10\n\n# Q(z|X) -- encoder\n\ninputs = Input(shape=(784,))\nh_q = Dense(512, activation='relu')(inputs)\nmu = Dense(n_z, activation='linear')(h_q)\nlog_sigma = Dense(n_z, activation='linear')(h_q)\n```\n\nThat is, our $Q(z \\vert X)$ is a neural net with one hidden layer. In this implementation, our latent variable is two dimensional, so that we could easily visualize it. In practice though, more dimension in latent variable should be better.\n\nHowever, we are now facing a problem. How do we get $z$ from the encoder outputs? Obviously we could sample $z$ from a Gaussian which parameters are the outputs of the encoder. Alas, sampling directly won't do, if we want to train VAE with gradient descent as the sampling operation doesn't have gradient!\n\nThere is, however a trick called reparameterization trick, which makes the network differentiable. Reparameterization trick basically divert the non-differentiable operation out of the network, so that, even though we still involve a thing that is non-differentiable, at least it is out of the network, hence the network could still be trained.\n\nThe reparameterization trick is as follows. Recall, if we have $x \\sim N(\\mu, \\Sigma)$ and then standardize it so that $\\mu = 0, \\Sigma = 1$, we could revert it back to the original distribution by reverting the standardization process. Hence, we have this equation:\n\n$$\nx = \\mu + \\Sigma^{\\frac{1}{2}} x_{std}\n$$\n\nWith that in mind, we could extend it. If we sample from a standard normal distribution, we could convert it to any Gaussian we want if we know the mean and the variance. Hence we could implement our sampling operation of $z$ by:\n\n$$\nz = \\mu(X) + \\Sigma^{\\frac{1}{2}}(X) \\, \\epsilon\n\n\n$$\n\nwhere $\\epsilon \\sim N(0, 1)$.\n\nNow, during backpropagation, we don't care anymore with the sampling process, as it is now outside of the network, i.e. doesn't depend on anything in the net, hence the gradient won't flow through it.\n\n```python\ndef sample_z(args):\n    mu, log_sigma = args\n    eps = K.random_normal(shape=(m, n_z), mean=0., std=1.)\n    return mu + K.exp(log_sigma / 2) \\* eps\n\n# Sample z ~ Q(z|X)\nz = Lambda(sample_z)([mu, log_sigma])\n```\n\nNow we create the decoder net $P(X \\vert z)$:\n\n```python\n# P(X|z) -- decoder\ndecoder_hidden = Dense(512, activation='relu')\ndecoder_out = Dense(784, activation='sigmoid')\n\nh_p = decoder_hidden(z)\noutputs = decoder_out(h_p)\n```\n\nLastly, from this model, we can do three things: reconstruct inputs, encode inputs into latent variables, and generate data from latent variable. So, we have three Keras models:\n\n```python\n# Overall VAE model, for reconstruction and training\nvae = Model(inputs, outputs)\n\n# Encoder model, to encode input into latent variable\n# We use the mean as the output as it is the center point, the representative of the gaussian\nencoder = Model(inputs, mu)\n\n# Generator model, generate new data given latent variable z\nd_in = Input(shape=(n_z,))\nd_h = decoder_hidden(d_in)\nd_out = decoder_out(d_h)\ndecoder = Model(d_in, d_out)\n```\n\nThen, we need to translate our loss into Keras code:\n\n```python\ndef vae_loss(y_true, y_pred):\n    \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n    # E[log P(X|z)]\n    recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)\n    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n    kl = 0.5 * K.sum(K.exp(log_sigma) + K.square(mu) - 1. - log_sigma, axis=1)\n\n    return recon + kl\n\n```\n\nand then train it:\n\n```python\nvae.compile(optimizer='adam', loss=vae_loss)\nvae.fit(X_train, X_train, batch_size=m, nb_epoch=n_epoch)\n```\n\nAnd that's it, the implementation of VAE in Keras!\n\n## Implementation on MNIST Data\n\nWe could use any dataset really, but like always, we will use MNIST as an example.\n\nAfter we trained our VAE model, we then could visualize the latent variable space $Q(z \\vert X)$:\n\n\u003CBlogImage imagePath='/img/variational-autoencoder/z_dist.png' />\n\nAs we could see, in the latent space, the representation of our data that have the same characteristic, e.g. same label, are close to each other. Notice that in the training phase, we never provide any information regarding the data.\n\nWe could also look at the data reconstruction by running through the data into overall VAE net:\n\n\u003CBlogImage imagePath='/img/variational-autoencoder/reconstruction.png' fullWidth />\n\nLastly, we could generate new sample by first sample $z \\sim N(0, 1)$ and feed it into our decoder net:\n\n\u003CBlogImage imagePath='/img/variational-autoencoder/generation.png' fullWidth />\n\nIf we look closely on the reconstructed and generated data, we would notice that some of the data are ambiguous. For example the digit 5 looks like 3 or 8. That's because our latent variable space is a continous distribution (i.e. $N(0, 1)$), hence there bound to be some smooth transition on the edge of the clusters. And also, the cluster of digits are close to each other if they are somewhat similar. That's why in the latent space, 5 is close to 3.\n\n## Conclusion\n\nIn this post we looked at the intuition behind Variational Autoencoder (VAE), its formulation, and its implementation in Keras.\n\nWe also saw the difference between VAE and GAN, the two most popular generative models nowadays.\n\nFor more math on VAE, be sure to hit the original paper by Kingma et al., 2014. There is also an excellent tutorial on VAE by Carl Doersch. Check out the references section below.\n\nThe full code is available in my repo: https://github.com/wiseodd/generative-models\n\n## References\n\n1. Doersch, Carl. \"Tutorial on variational autoencoders.\" arXiv preprint arXiv:1606.05908 (2016).\n2. Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" arXiv preprint arXiv:1312.6114 (2013).\n3. https://blog.keras.io/building-autoencoders-in-keras.html","src/content/post/variational-autoencoder.mdx","21637d0b8bd676c8","variational-autoencoder.mdx","wagtail-dev-env",{"id":650,"data":652,"body":657,"filePath":658,"digest":659,"legacyId":660,"deferredRender":23},{"title":653,"description":654,"publishDate":655,"draft":15,"tags":656},"Setting Up Wagtail Development Environment","My experience on building a blog using Wagtail CMS, with zero Django knowledge. I’ll walk you through from scratch up until the blog is live",["Date","2015-06-21T06:52:00.000Z"],[43,42,181],"Wagtail is a CMS, made on top of Django framework. Why Wagtail, you ask. Well first thing first, Python is my favorite programming language, and naturally I’ll look building a website using Django. I’ve researched a bit about what CMS to use with Django, and I found there are three CMS-es out there that are stand out from the rest: Django-CMS, Mezzanine, and Wagtail. I chose Wagtail simply because I think it has the most modern look, easy to customize, and has this killer feature, StreamField.\n\nFirst thing first, this post is from a web developer who had essentially ZERO knowledge about Django and Wagtail. So if you’re an seasoned Django dev, please bear with me! Also, I’m mainly using OSX Yosemite and Ubuntu 14.04 for development, so, sadly I can’t comment about Windows.\n\nFirst let’s install the dependencies: Python, pip, and PostgreSQL\n\n```bash\nUbuntu:\nsudo apt-get install python python-dev postgresql-9.3 postgresql-server-dev-9.3\nsudo easy_install pip\n\nOSX:\nbrew install python pip postgres\n```\n\nNow we’ll install virtualenv. This is not necessary, but will improve our development greatly as virtualenv will let us have clean and reproducible Python development environment.\n\n```bash\nsudo pip install virtualenv virtualenvwrapper\n```\n\nAnd then, register virtualenvwrapper to our shell. Add these lines to your `.bashrc` (Ubuntu) or `.zshrc` (OSX).\n\n```bash\nexport WORKON_HOME=~/.virtualenv\nsource /usr/local/bin/virtualenvwrapper.sh\n```\n\nThen, you’ll need to log out to apply the change.\n\nNow, we’ll create new virtualenv for our blog, then install Wagtail.\n\n```bash\nmkvirtualenv yourblogname\npip install wagtail==1.0b2\nwagtail start yourblogname\n```\n\nNow we’ve scaffolded our blog project structure! We can start our development now. However, by default, Wagtail uses SQLite for its database, which is alright for development purpose. For production environment, I strongly recommend to use more heavy duty database, namely PostgreSQL. So let’s make Wagtail uses PostgreSQL.\n\nOpen up the requirements.txt on your project directory, then uncomment this line that says `psycopg2==2.5.2`. This will tell pip to install psycopg2 dependency which is needed for Python to interface with PostgreSQL. After that run:\n\n```bash\npip install -r requirements.txt\n```\n\nThere are some hurdles though, when using PostgreSQL for the first time. I’m experienced with database, but mainly with Oracle and MySQL. PostgreSQL is a bit different and coming from that MySQL it’s hard at first.\n\nWhat we want to do is to:\n\n1. Create a user, matching our blog name\n2. Create a database, also matching our blog name\n3. Give that user access to the database\n\nOf course, you can use whatever name you want, but same name will simplify our live, so hey why not. So, run this:\n\n```bash\nsudo su - postgres\ncreatedb yourblogname\npsql yourblogname\n```\n\nNow we’re inside PostgreSQL shell. We’ll create the user, then grant the permission to yourblogname database.\n\n```sql\nCREATE ROLE yourblogname WITH PASSWORD ‘yourpassword’\nGRANT ALL PRIVILEGES ON DATABASE yourblogname TO yourblogname\n```\n\nThen use `CTRL+D` to exit psql and type `exit` to exit user postgres’ shell.\n\nNow we have to tell our blog to use PostgreSQL instead of SQLite. In your `yourblogname/settings/base.py`, comment out SQLite database entry:\n\n```python\n# # SQLite (simplest install)\n# DATABASES = {\n# 'default': {\n# 'ENGINE': 'django.db.backends.sqlite3',\n# 'NAME': join(PROJECT_ROOT, 'db.sqlite3'),\n# }\n# }\n```\n\nThen, uncomment PostgreSQL entry, and fill it with our database information:\n\n```python\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'NAME': 'yourblogname',\n        'USER': 'yourblogname',\n        'PASSWORD': ''yourpassword,\n        'HOST': ‘127.0.0.1’, # Set to empty string for localhost.\n        'PORT': '', # Set to empty string for default.\n        'CONN_MAX_AGE': 600, # number of seconds database connections should persist for\n    }\n}\n```\n\nOne of the problem I encountered was because I set `HOST` to be empty, which in turn will use socket connection, and gave me this error:\n\n```\nFATAL: Peer authentication failed for user “yourblogname\"\n```\n\nSo, make sure to fill `HOST` with `localhost` or `127.0.0.1`\n\nAfter that, we’ll write our initial database to PostgreSQL:\n\n```bash\n./manage.py makemigrations\n./manage.py migrate\n```\n\nAlso, don't forget to create an admin account so that we could get inside the admin panel.\n\n```bash\n./manage.py createsuperuser\n```\n\nIf everything is good, then we have our database ready, and we’re ready to actually code our blog!\n\nIn the next post, I’ll guide you to create our blog post model, home page, and the templates.","src/content/post/wagtail-dev-env.mdx","71b1aa2a32a651d8","wagtail-dev-env.mdx","wasserstein-gan",{"id":661,"data":663,"body":668,"filePath":669,"digest":670,"legacyId":671,"deferredRender":23},{"title":664,"description":665,"publishDate":666,"draft":15,"tags":667},"Wasserstein GAN implementation in TensorFlow and Pytorch","Wasserstein GAN comes with promise to stabilize GAN training and abolish mode collapse problem in GAN.",["Date","2017-02-04T08:10:00.000Z"],[17,56],"import BlogImage from \"@/components/BlogImage.astro\";\n\nGAN is very popular research topic in Machine Learning right now. There are two types of GAN researches, one that applies GAN in interesting problems and one that attempts to stabilize the training.\n\nIndeed, stabilizing GAN training is a very big deal in the field. The original GAN suffers from several difficulties, e.g. mode collapse, where generator collapse into very narrow distribution that only covers a single mode in data distribution. The implication of mode collapse is that generator can only generate very similar samples (e.g. a single digit in MNIST), i.e. the samples generated are not diverse. This problem of course violates the spirit of GAN.\n\nAnother problem in GAN is that there is no metric that tells us about the convergence. The generator and discriminator loss do not tell us anything about this. Of course we could monitor the training progress by looking at the data generated from generator every now and then. However, it is a strictly manual process. So, it would be great to have an interpretable metric that tells us about the training progress.\n\nNote, code could be found here: https://github.com/wiseodd/generative-models.\n\n## Wasserstein GAN\n\nWasserstein GAN (WGAN) is a newly proposed GAN algorithm that promises to remedy those two problems above.\n\nFor the intuition and theoritical background behind WGAN, redirect to [this excellent summary](https://paper.dropbox.com/doc/Wasserstein-GAN-GvU0p2V9ThzdwY3BbhoP7) (credits to the author).\n\nThe overall algorithm is shown below:\n\n\u003CBlogImage imagePath='/img/wasserstein-gan/00.png' altText='WGAN' fullWidth />\n\nWe could see that the algorithm is quite similar to the original GAN. However, to implement WGAN, we should notice few things from the above:\n\n1. No $\\log$ in the loss. The output of $D$ is no longer a probability, hence we do not apply sigmoid at the output of $D$\n2. Clip the weight of $D$\n3. Train $D$ more than $G$\n4. Use RMSProp instead of ADAM\n5. Lower learning rate, the paper uses $\\alpha = 0.00005$\n\n## WGAN TensorFlow implementation\n\nThe base implementation of GAN could be found in the past post. We need only to modify traditional GAN with respect to those items above. So first, let's update our $D$:\n\n```python\n\"\"\" Vanilla GAN \"\"\"\ndef discriminator(x):\n    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n    out = tf.matmul(D_h1, D_W2) + D_b2\n    return tf.nn.sigmoid(out)\n\n\"\"\" WGAN \"\"\"\ndef discriminator(x):\n    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n    out = tf.matmul(D_h1, D_W2) + D_b2\n    return out\n```\n\nNext, we modify our loss by simply removing the $\\log$:\n\n```python\n\"\"\" Vanilla GAN \"\"\"\nD_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\nG_loss = -tf.reduce_mean(tf.log(D_fake))\n\n\"\"\" WGAN \"\"\"\nD_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)\nG_loss = -tf.reduce_mean(D_fake)\n```\n\nWe then clip the weight of $D$ after each gradient descent update:\n\n```python\n# theta_D is list of D's params\nclip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D]\n```\n\nLastly, we train $D$ more:\n\n```python\nD_solver = (tf.train.RMSPropOptimizer(learning_rate=5e-5)\n            .minimize(-D_loss, var_list=theta_D))\nG_solver = (tf.train.RMSPropOptimizer(learning_rate=5e-5)\n            .minimize(G_loss, var_list=theta_G))\n\nfor it in range(1000000):\n    for _ in range(5):\n        X_mb, _ = mnist.train.next_batch(mb_size)\n\n        _, D_loss_curr, _ = sess.run(\n            [D_solver, D_loss, clip_D],\n            feed_dict={X: X_mb, z: sample_z(mb_size, z_dim)}\n        )\n\n    _, G_loss_curr = sess.run(\n        [G_solver, G_loss],\n        feed_dict={z: sample_z(mb_size, z_dim)}\n    )\n```\n\nAnd that is it.\n\n## WGAN Pytorch implementation\n\nThe base implementation of original GAN could be found in the past post.\nSimilar to the TensorFlow version, the modifications are quite straight forward. Note the codes below are inside each training iteration.\n\nFirst, update $D$:\n\n```python\n\"\"\" Vanilla GAN \"\"\"\nD = torch.nn.Sequential(\n    torch.nn.Linear(X_dim, h_dim),\n    torch.nn.ReLU(),\n    torch.nn.Linear(h_dim, 1),\n    torch.nn.Sigmoid()\n)\n\n\"\"\" WGAN \"\"\"\nD = torch.nn.Sequential(\n    torch.nn.Linear(X_dim, h_dim),\n    torch.nn.ReLU(),\n    torch.nn.Linear(h_dim, 1),\n)\n```\n\nModifying loss:\n\n```python\n\"\"\" Vanilla GAN \"\"\"\n# During discriminator forward-backward-update\nD_loss = torch.mean(torch.log(D_real) - torch.log(1- D_fake))\n# During generator forward-backward-update\nG_loss = -torch.mean(torch.log(D_fake))\n\n\"\"\" WGAN \"\"\"\n# During discriminator forward-backward-update\nD_loss = -(torch.mean(D_real) - torch.mean(D_fake))\n# During generator forward-backward-update\nG_loss = -torch.mean(D_fake)\n```\n\nWeight clipping:\n\n```python\nD_loss.backward()\nD_solver.step()\n\nfor p in D.parameters():\n    p.data.clamp_(-0.01, 0.01)\n```\n\nTrain $D$ more:\n\n```python\nG_solver = optim.RMSprop(G.parameters(), lr=5e-5)\nD_solver = optim.RMSprop(D.parameters(), lr=5e-5)\n\nfor it in range(1000000):\n    for _ in range(5):\n        \"\"\" Dicriminator forward-loss-backward-update \"\"\"\n\n    \"\"\" Generator forward-loss-backward-update \"\"\"\n```\n\n## References\n\n1. https://arxiv.org/abs/1701.07875\n2. https://paper.dropbox.com/doc/Wasserstein-GAN-GvU0p2V9ThzdwY3BbhoP7","src/content/post/wasserstein-gan.mdx","3f272177635691d7","wasserstein-gan.mdx","writing-advice",{"id":672,"data":674,"body":679,"filePath":680,"digest":681,"legacyId":682,"deferredRender":23},{"title":675,"description":676,"publishDate":677,"draft":15,"tags":678},"Writing Advice for Fledging Machine Learning Researchers","Some bullet-point advice regarding paper-writing I would give to my younger self.",["Date","2024-08-30T04:00:00.000Z"],[],"import BlogImage from \"@/components/BlogImage.astro\";\n\n## General writing protips\n\n- **DO:** Write before you even do any experiment.\n  - Your initial draft = hypothesis\n  - Experiments = observations\n  - Update your draft based on your experiments = posterior\n  - Rinse lather repeat\n- **_DON’T_:** Leave everything to the last minute, esp. writing.\n- **_DON'T_:** Wait until the last minute to submit your paper.\n  - Server overload is a thing.\n- **DO:** Read _widely_ and _a lot_ (in English!): novels, nonfictions,\n  popular science, etc.\n- **DO:** Take pride of your work and paper. It's your life's work.\n  - Haphazard, low-effort, inconsistent-styling paper signals to the reader that you\n    yourself don't care.\n  - Why should they care about your work then?\n- **DO:** See paper-writing as a **_craft_**. Always hone your **_craftmanship_**.\n- **DO:** _Obsess_ over styling (see below).\n- **DO:** Read math books and appreciate the typography & styling.\n- **DO:** Have a blog site. Publish several in-depth blog posts per year.\n- **_DON'T:_** Be afraid of people think you're dumb based on your blog posts.\n  - See my blog post archive from 2016 --- they're so simple and embarassing.\n\n## LaTeX protips\n\n### General\n\n- **One line = one sentence**\n  - This will make debugging easier, due to how LaTeX shows errors\n- **Quotation marks:** Instead of `\"something\"`, write ` ``something'' `\n  (2 backtics & 2 standard ticks)\n\n### Styles\n\n#### Figures\n\n- Must always fill the full paper width (or column width).\n  - Use [my library](https://github.com/wiseodd/pub-ready-plots).\n  - For figures with multiple subplots, avoid creating a single pdf file\n    for each subplot. Instead, [follow this](https://github.com/wiseodd/pub-ready-plots?tab=readme-ov-file#creating-a-figure-with-multiple-subplots)\n- If you think they are not appropriate for full width, use [wrapfig](https://www.overleaf.com/learn/latex/Positioning_images_and_tables).\n  - See also [how to do this with my plotting library](https://github.com/wiseodd/pub-ready-plots?tab=readme-ov-file#creating-plots-for-wrapfigure).\n\n#### Tables\n\n- Always use booktabs instead of standard table. [See this](https://nhigham.com/2019/11/19/better-latex-tables-with-booktabs/).\n- Like figures, must also always fill the full width.\n  [See how](https://tex.stackexchange.com/questions/240154/setting-table-width-exactly-to-linewidth).\n- Also, like figures, use [wraptable](https://www.overleaf.com/learn/latex/Positioning_images_and_tables)\n  if your table is not appropriate for full width.\n\n### Maths\n\n- **Math macros:** [Use this](https://github.com/goodfeli/dlbook_notation/blob/master/math_commands.tex)\n  and add your own often-used math definitions\n  - So, gone are the days where you need to write `\\mathbf{x}` again and again.\n    Instead you could just write `\\vx`.\n\n### Bibliography\n\n#### Citations\n\n- Always use `natbib`! Two ways of citing:\n  - \"... has been done before [4].\" --- in this case, you use `\\citep`\n    and write `... has been done before \\citep{someone2024}`.\n  - \"Someone et al, 2024 has done ...\" --- in this case, you use `\\citet`\n    and write `\\citet{someone2024} has done ...`\n\n#### References / bibtex\n\n- **Rule to live by:** Dirty, inconsistent, low-effort reference list signals\n  to the reader that you don't care. Why should they care about your work?\n- Don't just copy-paste from Google Scholar! Always recheck & edit!\n- E.g. make sure the proper capitalization:\n  - Instead of: `title={Introduction to Bayesian optimization}`\n  - Write this: `title={Introduction to {B}ayesian optimization}`\n  - I.e., always surround the character that needs to be capitalized\n    with `{ }`\n- Venue precedence if a publication appears in multiple venues\n  (top = most prioritized):\n  1. Journal\n  2. Conference\n  3. Workshop/symposium\n  4. ArXiv/preprint\n- For well-known ML conferences, simply use their abbreviations.\n  - E.g. \"Advances in Neural Information Processing Systems 35\" -> \"NeurIPS\".\n- For conference, use `@inproceedings`.\n  - The only fields needed are `title`, `author`, `booktitle`, and `year`.\n  - No need for other things like `page`, `editor`, etc.\n- For journal/ArXiv, use `@article`.\n  - Use `journal` instead of `booktitle`.\n  - Additionally, `volume` and `number` must be included.\n- **Example:**\n\n```bibtex\n@inproceedings{kristiadi2024sober,\n  title={\n    A Sober Look at {LLMs} for Material Discovery:\n    {A}re They Actually Good for {B}ayesian Optimization Over Molecules?\n  },\n  author={\n    Kristiadi, Agustinus and Strieth-Kalthoff, Felix and Skreta, Marta\n    and Poupart, Pascal and Aspuru-Guzik, Al\\'{a}n and Pleiss, Geoff\n  },\n  booktitle={ICML},\n  year={2024}\n}\n```","src/content/post/writing-advice.mdx","163feb0a45531d5a","writing-advice.mdx"]